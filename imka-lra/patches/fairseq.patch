diff --git a/fairseq/checkpoint_utils.py b/fairseq/checkpoint_utils.py
index fb9a6679..8f504a93 100644
--- a/fairseq/checkpoint_utils.py
+++ b/fairseq/checkpoint_utils.py
@@ -27,8 +27,13 @@ from fairseq.dataclass.utils import (
 from fairseq.distributed.fully_sharded_data_parallel import FSDP, has_FSDP
 from fairseq.file_io import PathManager
 from fairseq.models import FairseqDecoder, FairseqEncoder
+from fairseq.utils import analog2model
 from omegaconf import DictConfig, OmegaConf, open_dict
 
+from aihwkit.nn import AnalogWrapper
+from aihwkit.nn.conversion import convert_to_analog, convert_to_digital
+from copy import deepcopy
+
 logger = logging.getLogger(__name__)
 
 
@@ -406,6 +411,7 @@ def load_model_ensemble_and_task(
     suffix="",
     num_shards=1,
     state=None,
+    new_data_dir=None
 ):
     assert state is None or len(filenames) == 1
 
@@ -440,7 +446,7 @@ def load_model_ensemble_and_task(
                 )
 
             if task is None:
-                task = tasks.setup_task(cfg.task, from_checkpoint=True)
+                task = tasks.setup_task(cfg.task, from_checkpoint=True, new_data_dir=new_data_dir)
 
             if "task_state" in state:
                 task.load_state_dict(state["task_state"])
@@ -489,10 +495,18 @@ def load_model_ensemble_and_task(
                     and len(state["optimizer_history"]) > 0
                     and "num_updates" in state["optimizer_history"][-1]
                 ):
-                    model.set_num_updates(state["optimizer_history"][-1]["num_updates"])
-                model.load_state_dict(
-                    state["model"], strict=strict, model_cfg=cfg.model
-                )
+                    analog2model(model).set_num_updates(state["optimizer_history"][-1]["num_updates"])
+                if isinstance(model, AnalogWrapper):
+                    digital_model = convert_to_digital(model)
+                    # - Need to build rpu_config from cfg.model
+                    rpu_config = deepcopy(next(model.analog_tiles()).rpu_config)
+                    model = convert_to_analog(digital_model, rpu_config)
+                    model.load_state_dict(state["model"], strict=strict)
+                    del digital_model
+                else:
+                    model.load_state_dict(
+                        state["model"], strict=strict, model_cfg=cfg.model
+                    )
 
             # reset state so it gets loaded for the next model in ensemble
             state = None
diff --git a/fairseq/criterions/lra_cross_entropy.py b/fairseq/criterions/lra_cross_entropy.py
new file mode 100644
index 00000000..96941d95
--- /dev/null
+++ b/fairseq/criterions/lra_cross_entropy.py
@@ -0,0 +1,121 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from omegaconf import II
+from dataclasses import dataclass
+import torch
+import torch.nn.functional as F
+
+from fairseq import metrics, utils
+from fairseq.criterions import FairseqCriterion, register_criterion
+from fairseq.dataclass.configs import FairseqDataclass
+from fairseq.utils import analog2model
+
+
+@dataclass
+class CrossEntropyCriterionConfig(FairseqDataclass):
+    sentence_avg: bool = II("optimization.sentence_avg")
+
+
+@register_criterion("lra_cross_entropy", dataclass=CrossEntropyCriterionConfig)
+class LRACrossEntropyCriterion(FairseqCriterion):
+
+    def __init__(self, task, sentence_avg):
+        super().__init__(task)
+        self.sentence_avg = sentence_avg
+
+    def forward(self, model, sample, reduce=True):
+        """Compute the loss for the given sample.
+
+        Returns a tuple with three elements:
+        1) the loss
+        2) the sample size, which is used as the denominator for the gradient
+        3) logging outputs to display while training
+        """
+        # from aihwkit.nn.conversion import convert_to_digital
+        # from aihwkit.nn import AnalogLinear
+        # dig_model = convert_to_digital(model)
+        # dig_model = dig_model.cuda()
+        # torch.manual_seed(0)
+        # dig_net_output = dig_model(sample)
+        # dig_loss, _ = self.compute_loss(dig_model, dig_net_output, sample, reduce=reduce)
+        # dig_loss.backward()
+        # dig_l = []; dig_l_names = []
+        # for n,m in dig_model.named_modules():
+        #     if isinstance(m, torch.nn.Linear):
+        #         dig_l_names.append(n)
+        #         dig_l.append(m.weight)
+        # torch.manual_seed(0)
+        # net_output = model(sample)
+        # loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)
+        # loss.backward()
+        # analog_l = []; analog_l_names = []
+        # for n,m in model.named_modules():
+        #     if isinstance(m, AnalogLinear):
+        #         analog_l_names.append(n)
+        #         analog_l.append(m.analog_module.shared_weights)
+
+        # for i in range(len(dig_l)):
+        #         if dig_l[i].shape != analog_l[i].T.shape:
+        #             print(f"{dig_l_names[i]} and {analog_l_names[i]} shapes dont match. got {dig_l[i].shape} and {analog_l[i].T.shape}")
+        #         else:
+        #             if dig_l[i].grad is None:
+        #                 print(dig_l_names[i], "grad is none")
+        #             if analog_l[i].grad is None:
+        #                 print(analog_l_names[i], "grad is none")
+        #             if dig_l[i].grad is not None and analog_l[i].grad is not None:
+        #                 print("Dig.", dig_l[i].grad[0,:5])
+        #                 print("Analog", analog_l[i].grad.T[0,:5])
+        #                 print("Scale", dig_l[i].grad[0,:5]  / analog_l[i].grad.T[0,:5])
+        #                 print(torch.allclose(dig_l[i].grad, analog_l[i].grad.T))
+        #                 print(torch.allclose(dig_l[i], analog_l[i].T))
+
+        net_output = model(sample)
+        loss, correct = self.compute_loss(model, net_output, sample, reduce=reduce)
+        sample_size = sample['target'].size(0) if self.sentence_avg else sample['ntokens']
+        logging_output = {
+            'loss': loss.data,
+            'ncorrects': correct.data,
+            'ntokens': sample['ntokens'],
+            'nsentences': sample['target'].size(0),
+            'sample_size': sample_size,
+        }
+        return loss, sample_size, logging_output
+
+    def compute_loss(self, model, net_output, sample, reduce=True):
+        lprobs = analog2model(model).get_normalized_probs(net_output, log_probs=True)
+        lprobs = lprobs.view(-1, lprobs.size(-1))
+        targets = analog2model(model).get_targets(sample, net_output).view(-1)
+        loss = F.nll_loss(
+            lprobs,
+            targets,
+            reduction="sum" if reduce else "none"
+        )
+        preds = torch.argmax(lprobs, 1)
+        correct = (preds == targets).sum()
+        return loss, correct
+
+    @staticmethod
+    def reduce_metrics(logging_outputs) -> None:
+        """Aggregate logging outputs from data parallel training."""
+        loss_sum = sum(log.get('loss', 0) for log in logging_outputs)
+        ntokens = sum(log.get('ntokens', 0) for log in logging_outputs)
+        nsentences = sum(log.get('nsentences', 0) for log in logging_outputs)
+        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)
+
+        metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=5)
+        if len(logging_outputs) > 0 and 'ncorrects' in logging_outputs[0]:
+            ncorrects = sum(log.get('ncorrects', 0) for log in logging_outputs)
+            metrics.log_scalar('accuracy', 100.0 * ncorrects / nsentences, nsentences, round=3)
+
+    @staticmethod
+    def logging_outputs_can_be_summed() -> bool:
+        """
+        Whether the logging outputs returned by `forward` can be summed
+        across workers prior to calling `reduce_metrics`. Setting this
+        to True will improves distributed training speed.
+        """
+        return True
\ No newline at end of file
diff --git a/fairseq/criterions/masked_lm.py b/fairseq/criterions/masked_lm.py
index 09ddd9f3..ba56e162 100644
--- a/fairseq/criterions/masked_lm.py
+++ b/fairseq/criterions/masked_lm.py
@@ -57,7 +57,7 @@ class MaskedLmLoss(FairseqCriterion):
             )
 
         logits = model(**sample["net_input"], masked_tokens=masked_tokens)[0]
-        targets = model.get_targets(sample, [logits])
+        targets = utils.analog2model(model).get_targets(sample, [logits])
         if masked_tokens is not None:
             targets = targets[masked_tokens]
 
diff --git a/fairseq/criterions/rxn_cross_entropy.py b/fairseq/criterions/rxn_cross_entropy.py
new file mode 100644
index 00000000..a684f3e1
--- /dev/null
+++ b/fairseq/criterions/rxn_cross_entropy.py
@@ -0,0 +1,69 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+import torch.nn.functional as F
+
+from fairseq import metrics
+from fairseq.criterions import FairseqCriterion, register_criterion
+from fairseq.utils import log_softmax
+
+@register_criterion("rxn_cross_entropy")
+class RXNCrossEntropyCriterion(FairseqCriterion):
+
+    def __init__(self, task):
+        super().__init__(task)
+        self.ignore_index = 1
+
+    def forward(self, model, sample, reduce=True):
+        """Compute the loss for the given sample.
+
+        Returns a tuple with three elements:
+        1) the loss
+        2) the sample size, which is used as the denominator for the gradient
+        3) logging outputs to display while training
+        """
+        src, tgt = sample
+        tgt = tgt.long()
+        net_output = model(src, tgt)
+        loss = self.compute_loss(net_output, tgt, reduce=reduce)
+        sample_size = sample[0].size(1)
+        logging_output = {
+            'loss': loss.data,
+            'ntokens': sample[0].size(0),
+            'nsentences': sample[0].size(1),
+            'sample_size': 1,
+        }
+        return loss, sample_size, logging_output
+
+    def compute_loss(self, net_output, target, reduce=True):
+        target = target[1:, :]
+        # net_output needs to be [target-T, B, len(vocab)] -> [T 256 296]
+        # lprobs = analog2model(model).transformer.get_normalized_probs(net_output, log_probs=True)
+        lprobs = log_softmax(net_output, dim=-1)
+        lprobs = lprobs.view(-1, lprobs.size(-1))
+        loss = F.nll_loss(
+            lprobs,
+            target.reshape(-1),
+            ignore_index=self.ignore_index,
+            reduction="mean" if reduce else "sum"
+        )
+        return loss
+
+    @staticmethod
+    def reduce_metrics(logging_outputs) -> None:
+        """Aggregate logging outputs from data parallel training."""
+        loss_sum = sum(log.get('loss', 0) for log in logging_outputs)
+        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)
+        metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=5)
+
+    @staticmethod
+    def logging_outputs_can_be_summed() -> bool:
+        """
+        Whether the logging outputs returned by `forward` can be summed
+        across workers prior to calling `reduce_metrics`. Setting this
+        to True will improves distributed training speed.
+        """
+        return True
\ No newline at end of file
diff --git a/fairseq/criterions/rxn_entropy_reg.py b/fairseq/criterions/rxn_entropy_reg.py
new file mode 100644
index 00000000..eee9eec3
--- /dev/null
+++ b/fairseq/criterions/rxn_entropy_reg.py
@@ -0,0 +1,191 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+import logging
+from functools import partial
+import torch
+import torch.nn.functional as F
+from aihwkit.nn.modules.container import AnalogWrapper
+
+from fairseq import metrics
+from fairseq.criterions import FairseqCriterion, register_criterion
+from fairseq.utils import log_softmax
+
+PI = 3.14159265
+
+
+def normpdf(x, mean, sd):
+    var = sd**2
+    denom = (2* PI * var)**.5
+    num = torch.exp(-(x-mean)**2 / (2*var))
+    return num / denom
+
+
+def _entropy_reg_pre_forward(
+    mod: torch.nn.Module,
+    input_args: tuple,
+    cache_key: str,
+    global_cache: dict[str, torch.Tensor],
+    max_samples: int,
+) -> None:
+    """Caches inputs for calibrating the input ranges.
+
+    Args:
+        input_args: Forward inputs.
+        calibration_type: type used for calibration
+        cache_key: key of global cache
+        max_samples: Maximal number of cache samples
+    """
+    x_input = input_args[0]
+    r_perm = torch.randperm(torch.tensor(x_input.shape[:-1]).prod())[:max_samples]
+    if hasattr(mod, "input_range") and mod.input_range is not None:
+        cache = x_input.reshape(-1, x_input.size(-1))[r_perm].clone().clamp(-mod.input_range, mod.input_range) / mod.input_range
+    else:
+        abs_max = x_input.abs().max()
+        cache = x_input.reshape(-1, x_input.size(-1))[r_perm].clone()
+        cache = cache.clamp(-abs_max, abs_max) / abs_max
+    global_cache[cache_key] = cache
+
+
+@register_criterion("rxn_entropy_reg")
+class RXNEntropyRegCriterion(FairseqCriterion):
+
+    def __init__(self, task, beta: float, max_samples: int, sigma: float, n_bins: int):
+        super().__init__(task)
+        self.ignore_index = 1
+        self.beta = beta
+        self.max_samples = max_samples
+        self.sigma = sigma
+        self.n_bins = n_bins
+        self.eps = 1e-6
+        assert beta != None and max_samples != None and sigma != None and n_bins != None, "Need to configure beta and max_samples in `criterion` in yaml."
+
+    def forward(self, model, sample, reduce=True):
+        """Compute the loss for the given sample.
+
+        Returns a tuple with three elements:
+        1) the loss
+        2) the sample size, which is used as the denominator for the gradient
+        3) logging outputs to display while training
+        """
+
+        # - Install the pre-forward hook
+        handles = []
+        cache = {}
+        if isinstance(model, AnalogWrapper):
+            for tile_name, tile in model.named_analog_tiles():
+                cache[tile_name] = torch.tensor([])
+                hook = partial(
+                    _entropy_reg_pre_forward,
+                    cache_key=tile_name,
+                    global_cache=cache,
+                    max_samples=self.max_samples,
+                )
+                handles.append(tile.register_forward_pre_hook(hook, prepend=True))
+        else:
+            raise NotImplementedError("Still needs to be implemented for normal Linear layers.")
+
+
+        src, tgt = sample
+        tgt = tgt.long()
+        net_output = model(src, tgt)
+        natural_loss = self.compute_loss(net_output, tgt, reduce=reduce)
+        entropy_loss = 0.
+        count_thresh = 0
+        for k, a in cache.items():
+            if a.numel() == 0.:
+                continue
+            assert a.abs().max() <= 1., "Activations must be in [-1,1]"
+            sparsity = 1. - (a * 127).int().nonzero().size(0) / a.numel()
+            if sparsity >= 0.8:
+                if self.beta > 0.:
+                    count_thresh += 1
+                    entropy_loss += self.compute_entropy(a, name=k)
+        
+        if count_thresh > 0:
+            logging.warn(f"{count_thresh} many layers over 80% zero-percentage")
+
+        if count_thresh > 0:
+            loss = natural_loss - self.beta * entropy_loss / count_thresh # average across num layers where applied
+        else:
+            loss = natural_loss - self.beta * entropy_loss
+        sample_size = sample[0].size(1)
+        logging_output = {
+            'loss': loss.data,
+            'natural_loss': natural_loss.data,
+            'entropy_loss': 0.0 if entropy_loss == 0.0 else self.beta * entropy_loss.data,
+            'ntokens': sample[0].size(0),
+            'nsentences': sample[0].size(1),
+            'sample_size': 1,
+        }
+
+        # - Remove handles
+        for handle in handles:
+            handle.remove()
+
+        return loss, sample_size, logging_output
+
+    def compute_entropy(self, activations, name):
+        q = 1.0
+        d = 2 * q
+        
+        if isinstance(self.sigma, (float,int)):
+            scaled_sigma = self.sigma*d
+        else:
+            raise Exception("Unknown sigma, specify either float or \"cv\"")
+
+        bins = torch.linspace(-q, q, self.n_bins, device=activations.device)
+        px = normpdf(bins.reshape((-1,1,1)), activations.reshape((1,-1,activations.size(-1))), scaled_sigma).sum(-1) # - Sum over the activations
+        # print(px.shape)
+        # assert px.size(0)==self.n_bins
+        px = px / px.sum(0) # - Normalize across activations
+        s = - px * (px + self.eps).log()
+        e = s.mean() # - Average over the bins and activation vectors
+
+        # import os
+        # import matplotlib.pyplot as plt
+        # plt.clf()
+        # plt.subplot(211)
+        # plt.hist(activations.flatten().clamp(-q,q).cpu().detach().numpy(), bins=50)
+        # plt.subplot(212)
+        # plt.plot(px[:,0].cpu().detach().numpy())
+        # plt.savefig(os.path.join(os.path.expanduser("~/RXN-Demo/resources"), f"{name}.png"))
+
+        return e
+
+    def compute_loss(self, net_output, target, reduce=True):
+        target = target[1:, :]
+        # net_output needs to be [target-T, B, len(vocab)] -> [T 256 296]
+        # lprobs = analog2model(model).transformer.get_normalized_probs(net_output, log_probs=True)
+        lprobs = log_softmax(net_output, dim=-1)
+        lprobs = lprobs.view(-1, lprobs.size(-1))
+        loss = F.nll_loss(
+            lprobs,
+            target.reshape(-1),
+            ignore_index=self.ignore_index,
+            reduction="mean" if reduce else "sum"
+        )
+        return loss
+
+    @staticmethod
+    def reduce_metrics(logging_outputs) -> None:
+        """Aggregate logging outputs from data parallel training."""
+        loss_sum = sum(log.get('loss', 0) for log in logging_outputs)
+        nat_sum = sum(log.get('natural_loss', 0) for log in logging_outputs)
+        entropy_sum = sum(log.get('entropy_loss', 0) for log in logging_outputs)
+        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)
+        metrics.log_scalar('cce', nat_sum / sample_size / math.log(2), sample_size, round=5)
+        metrics.log_scalar('loss', loss_sum / sample_size / math.log(2), sample_size, round=5)
+        metrics.log_scalar('beta*entropy', entropy_sum / sample_size / math.log(2), sample_size, round=5)
+
+    @staticmethod
+    def logging_outputs_can_be_summed() -> bool:
+        """
+        Whether the logging outputs returned by `forward` can be summed
+        across workers prior to calling `reduce_metrics`. Setting this
+        to True will improves distributed training speed.
+        """
+        return True
\ No newline at end of file
diff --git a/fairseq/criterions/sentence_prediction.py b/fairseq/criterions/sentence_prediction.py
index 298b8057..a983ccc1 100644
--- a/fairseq/criterions/sentence_prediction.py
+++ b/fairseq/criterions/sentence_prediction.py
@@ -18,7 +18,7 @@ from fairseq.logging import metrics
 from fairseq.criterions import FairseqCriterion, register_criterion
 from fairseq.dataclass import FairseqDataclass
 from fairseq.logging.meters import safe_round
-
+from fairseq.utils import analog2model
 
 def simple_accuracy(preds, labels):
     return (preds == labels).mean()
@@ -87,8 +87,8 @@ class SentencePredictionCriterion(FairseqCriterion):
         3) logging outputs to display while training
         """
         assert (
-            hasattr(model, "classification_heads")
-            and self.classification_head_name in model.classification_heads
+            hasattr(analog2model(model), "classification_heads")
+            and self.classification_head_name in analog2model(model).classification_heads
         ), "model must provide sentence classification head for --criterion=sentence_prediction"
 
         logits, _ = model(
@@ -96,7 +96,7 @@ class SentencePredictionCriterion(FairseqCriterion):
             features_only=True,
             classification_head_name=self.classification_head_name,
         )
-        targets = model.get_targets(sample, [logits]).view(-1)
+        targets = analog2model(model).get_targets(sample, [logits]).view(-1)
         sample_size = targets.numel()
 
         if not self.regression_target:
@@ -111,19 +111,19 @@ class SentencePredictionCriterion(FairseqCriterion):
         loss = task_loss
         # mha & ffn regularization update
         if (
-            hasattr(model, "args")
-            and hasattr(model.args, "mha_reg_scale_factor")
-            and model.args.mha_reg_scale_factor != 0.0
+            hasattr(analog2model(model), "args")
+            and hasattr(analog2model(model).args, "mha_reg_scale_factor")
+            and analog2model(model).args.mha_reg_scale_factor != 0.0
         ):
-            mha_reg_loss = model._get_adaptive_head_loss()
+            mha_reg_loss = analog2model(model)._get_adaptive_head_loss()
             loss += mha_reg_loss
             logging_output.update({"mha_reg_loss": mha_reg_loss})
         if (
-            hasattr(model, "args")
-            and hasattr(model.args, "ffn_reg_scale_factor")
-            and model.args.ffn_reg_scale_factor != 0.0
+            hasattr(analog2model(model), "args")
+            and hasattr(analog2model(model).args, "ffn_reg_scale_factor")
+            and analog2model(model).args.ffn_reg_scale_factor != 0.0
         ):
-            ffn_reg_loss = model._get_adaptive_ffn_loss()
+            ffn_reg_loss = analog2model(model)._get_adaptive_ffn_loss()
             loss += ffn_reg_loss
             logging_output.update({"ffn_reg_loss": ffn_reg_loss})
 
diff --git a/fairseq/data/__init__.py b/fairseq/data/__init__.py
index eeaae2b2..618762d6 100644
--- a/fairseq/data/__init__.py
+++ b/fairseq/data/__init__.py
@@ -52,6 +52,7 @@ from .replace_dataset import ReplaceDataset
 from .resampling_dataset import ResamplingDataset
 from .roll_dataset import RollDataset
 from .round_robin_zip_datasets import RoundRobinZipDatasets
+from .pixel_sequence_dataset import PixelSequenceDataset
 from .sort_dataset import SortDataset
 from .speech_dlm_dataset import SpeechDLMDataset
 from .strip_token_dataset import StripTokenDataset
diff --git a/fairseq/data/pad_dataset.py b/fairseq/data/pad_dataset.py
index b512d370..2848d425 100644
--- a/fairseq/data/pad_dataset.py
+++ b/fairseq/data/pad_dataset.py
@@ -27,5 +27,5 @@ class LeftPadDataset(PadDataset):
 
 
 class RightPadDataset(PadDataset):
-    def __init__(self, dataset, pad_idx):
-        super().__init__(dataset, pad_idx, left_pad=False)
+    def __init__(self, dataset, pad_idx, pad_length):
+        super().__init__(dataset, pad_idx, left_pad=False, pad_length=pad_length)
diff --git a/fairseq/data/pixel_sequence_dataset.py b/fairseq/data/pixel_sequence_dataset.py
new file mode 100644
index 00000000..a6e61bb7
--- /dev/null
+++ b/fairseq/data/pixel_sequence_dataset.py
@@ -0,0 +1,74 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from functools import lru_cache
+import os
+
+import numpy as np
+import torch
+
+from . import FairseqDataset
+from fairseq.tokenizer import tokenize_line
+
+
+class PixelSequenceDataset(FairseqDataset):
+
+    def __init__(self, path, normalization, reverse_order=False):
+        self.mean = normalization[0]
+        self.std = normalization[1]
+        self.tokens_list = []
+        self.lines = []
+        self.sizes = []
+        self.reverse_order = reverse_order
+        self.read_data(path)
+        self.size = len(self.tokens_list)
+
+    def read_data(self, path):
+        with open(path, 'r', encoding='utf-8') as f:
+            for line in f:
+                self.lines.append(line.strip('\n'))
+                tokens = self.encode_line(line, reverse_order=self.reverse_order)
+                self.tokens_list.append(tokens)
+                self.sizes.append(len(tokens))
+        self.sizes = np.array(self.sizes)
+
+    def encode_line(self, line, line_tokenizer=tokenize_line, reverse_order=False):
+        words = line_tokenizer(line)
+        if reverse_order:
+            words = list(reversed(words))
+        pixels = [int(w) for w in words]
+
+        default_float_dtype = torch.get_default_dtype()
+        pixels = torch.tensor(pixels, dtype=default_float_dtype).div(255.)
+        return pixels.sub(self.mean).div(self.std)
+
+    def check_index(self, i):
+        if i < 0 or i >= self.size:
+            raise IndexError('index out of range')
+
+    @lru_cache(maxsize=8)
+    def __getitem__(self, i):
+        self.check_index(i)
+        return self.tokens_list[i]
+
+    def get_original_text(self, i):
+        self.check_index(i)
+        return self.lines[i]
+
+    def __del__(self):
+        pass
+
+    def __len__(self):
+        return self.size
+
+    def num_tokens(self, index):
+        return self.sizes[index]
+
+    def size(self, index):
+        return self.sizes[index]
+
+    @staticmethod
+    def exists(path):
+        return os.path.exists(path)
\ No newline at end of file
diff --git a/fairseq/data/sliced_dataset.py b/fairseq/data/sliced_dataset.py
new file mode 100644
index 00000000..261f06ce
--- /dev/null
+++ b/fairseq/data/sliced_dataset.py
@@ -0,0 +1,58 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+import numpy as np
+from . import BaseWrapperDataset
+
+
+logger = logging.getLogger(__name__)
+
+
+class SlicedDataset(BaseWrapperDataset):
+    """Slice a dataset within two bounds, a and b.
+    """
+
+    def __init__(self, dataset, a, b):
+        super().__init__(dataset)
+        assert a > 0 and b < len(self.dataset)
+        self.actual_size = b - a
+        self.indices = list(range(len(a, b)))
+        logger.info(
+            "sliced dataset from {} to {}".format(a, b)
+        )
+
+    def __getitem__(self, index):
+        return self.dataset[self.indices[index]]
+
+    def __len__(self):
+        return self.actual_size
+
+    def collater(self, samples):
+        return self.dataset.collater(samples)
+
+    @property
+    def sizes(self):
+        return self.dataset.sizes[self.indices]
+
+    @property
+    def name(self):
+        return self.dataset.name
+
+    def num_tokens(self, index):
+        return self.dataset.num_tokens(self.indices[index])
+
+    def size(self, index):
+        return self.dataset.size(self.indices[index])
+
+    def ordered_indices(self):
+        """Return an ordered list of indices. Batches will be constructed based
+        on this order."""
+        order = [np.arange(len(self))]
+        order.append(self.sizes)
+        return np.lexsort(order)
+
+    def prefetch(self, indices):
+        self.dataset.prefetch(self.indices[indices])
diff --git a/fairseq/model_parallel/megatron b/fairseq/model_parallel/megatron
deleted file mode 160000
index adb23324..00000000
--- a/fairseq/model_parallel/megatron
+++ /dev/null
@@ -1 +0,0 @@
-Subproject commit adb23324c222aad0aad89308e70302d996a5eaeb
diff --git a/fairseq/models/fairseq_encoder.py b/fairseq/models/fairseq_encoder.py
index 08cbde15..df349298 100644
--- a/fairseq/models/fairseq_encoder.py
+++ b/fairseq/models/fairseq_encoder.py
@@ -29,6 +29,7 @@ class FairseqEncoder(nn.Module):
     def __init__(self, dictionary):
         super().__init__()
         self.dictionary = dictionary
+        self.traceable = False
 
     def forward(self, src_tokens, src_lengths=None, **kwargs):
         """
diff --git a/fairseq/models/fairseq_incremental_decoder.py b/fairseq/models/fairseq_incremental_decoder.py
index cc72a0f8..0d415b78 100644
--- a/fairseq/models/fairseq_incremental_decoder.py
+++ b/fairseq/models/fairseq_incremental_decoder.py
@@ -39,6 +39,7 @@ class FairseqIncrementalDecoder(FairseqDecoder):
 
     def __init__(self, dictionary):
         super().__init__(dictionary)
+        self.traceable = False
 
     def forward(
         self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs
diff --git a/fairseq/models/lra/__init__.py b/fairseq/models/lra/__init__.py
new file mode 100644
index 00000000..fbe90021
--- /dev/null
+++ b/fairseq/models/lra/__init__.py
@@ -0,0 +1,7 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from .model import LRAModel
+from .transformer_lra_encoder import TransformerLRAEncoder
\ No newline at end of file
diff --git a/fairseq/models/lra/lra_config.py b/fairseq/models/lra/lra_config.py
new file mode 100644
index 00000000..e3053952
--- /dev/null
+++ b/fairseq/models/lra/lra_config.py
@@ -0,0 +1,83 @@
+from dataclasses import dataclass, field
+from typing import Optional
+
+from omegaconf import II, MISSING
+from fairseq.models.transformer.transformer_config import TransformerConfig
+
+
+@dataclass
+class LRAConfig(TransformerConfig):
+    layer_type: str = field(
+        default="transformer",
+        metadata={"help": "transformer layer type - from MEGA"},
+    )
+    norm_type: str = field(
+        default="layernorm",
+        metadata={"help": "layer regularization technique"},
+    )
+    sen_rep_type: str = field(
+        default="mp",
+        metadata={
+            "help": "entity used for classification"
+            "use 'cls' to use the CLS token"
+            "'mp' to use an average of all the sentence's tokens"
+        },
+    )
+    max_positions: int = field(
+        default=MISSING,
+    )
+    input_type: str = field(
+        default=MISSING,
+        metadata={"help": "transformer layer type - from MEGA"},
+    )
+    attention_activation_fn: Optional[str] = field(default=None)
+    classifier_in_dim: Optional[int] = field(default=None)
+    classifier_out_dim: Optional[int] = field(default=None)
+    sentence_class_num: Optional[int] = II("task.num_classes")
+
+    # MEGA params
+    n_dim: Optional[int] = field(default=None)
+    chunk_size: Optional[int] = field(default=None)
+    attention_activation_fn: Optional[str] = field(default=None)
+    rel_pos_bias: Optional[str] = field(default="simple")
+    z_dim: Optional[int] = field(default=64)
+
+    # additional parameters
+    bias: Optional[bool] = field(default=True)
+
+    tile_type: Optional[str] = field(default="torch")
+    noise_per_sample: Optional[bool] = field(default=False)
+    modifier_std_dev: Optional[float] = field(default=0.0)
+    modifier_type: Optional[str] = field(default="none")
+    forward_out_noise: Optional[float] = field(default=0.0)
+    mapping_weight_scaling_omega: Optional[float] = field(default=1.0)
+    mapping_learn_out_scaling: Optional[bool] = field(default=True)
+    mapping_weight_scaling_columnwise: Optional[bool] = field(default=False)
+    mapping_out_scaling_columnwise: Optional[bool] = field(default=False)
+    remap_type: Optional[str] = field(default="layer")
+    forward_is_perfect: Optional[bool] = field(default=False)
+    clip_type: Optional[str] = field(default="none")
+    clip_sigma: Optional[float] = field(default=2.5)
+    clip_fixed_value: Optional[float] = field(default=1.0)
+    forward_inp_res: Optional[float] = field(default=1 / (2**8 - 2))
+    forward_out_res: Optional[float] = field(default=1 / (2**8 - 2))
+    forward_out_bound: Optional[float] = field(default=10.0)
+    forward_inp_bound: Optional[float] = field(default=1.0)
+    forward_bound_management: Optional[str] = field(default="none")
+    forward_noise_management: Optional[str] = field(default="abs_max")
+    forward_w_noise_type: Optional[str] = field(default="none")
+    forward_w_noise: Optional[float] = field(default=0.0)
+    input_range_enable: Optional[bool] = field(default=False)
+    input_range_manage_output_clipping: Optional[bool] = field(default=False)
+    input_range_decay: Optional[float] = field(default=0.01)
+    input_range_input_min_percentage: Optional[float] = field(default=0.95)
+    input_range_output_min_percentage: Optional[float] = field(default=0.95)
+    input_range_init_value: Optional[float] = field(default=3.0)
+    input_range_init_std_alpha: Optional[float] = field(default=3.0)
+    input_range_init_from_data: Optional[int] = field(default=0)
+    static_input_clipping: Optional[bool] = field(default=False)
+    static_input_clipping_quantile: Optional[float] = field(default=0.995)
+    split_layers: Optional[bool] = field(default=True)
+    max_input_size: Optional[int] = field(default=0)
+    max_output_size: Optional[int] = field(default=0)
+    eta_eval: Optional[float] = field(default=0.0)
diff --git a/fairseq/models/lra/mega_lra_encoder.py b/fairseq/models/lra/mega_lra_encoder.py
new file mode 100644
index 00000000..c7bcd306
--- /dev/null
+++ b/fairseq/models/lra/mega_lra_encoder.py
@@ -0,0 +1,264 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple, List, Union
+import math
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from fairseq.modules import (
+    SequenceNorm,
+    RealNumberEmbedding,
+    LayerDropModuleList,
+    MegaSentenceEncoderLayer,
+)
+from fairseq.modules.fairseq_dropout import FairseqDropout
+
+
+class MegaLRAEncoder(nn.Module):
+    """
+    Implementation for a Bi-directional FLASH based Sentence Encoder used
+    in masked pre-trained language models.
+
+    This first computes the token embedding using the token embedding matrix,
+    position embeddings (if specified) and segment embeddings
+    (if specified). After applying the specified number of
+    TransformerEncoderLayers, it outputs all the internal states of the
+    encoder as well as the final representation associated with the first
+    token (usually CLS token).
+
+    Input:
+        - tokens: B x T matrix representing sentences
+        - segment_labels: B x T matrix representing segment label for tokens
+
+    Output:
+        - a tuple of the following:
+            - a list of internal model states used to compute the
+              predictions where each tensor has shape T x B x C
+            - sentence representation associated with first input token
+              in format B x C.
+    """
+
+    def __init__(
+        self,
+        padding_idx: int,
+        vocab_size: int,
+        num_encoder_layers: int = 6,
+        embedding_type: str = "sparse",
+        embedding_dim: int = 512,
+        hidden_dim: int = 1024,
+        ffn_hidden_dim: int = 1024,
+        z_dim: int = 128,
+        n_dim: int = 16,
+        activation: str = 'silu',
+        attention_activation: str = 'softmax',
+        dropout: float = 0.0,
+        attention_dropout: float = 0.0,
+        hidden_dropout: float = 0.0,
+        chunk_size: int = -1,
+        norm_type: str = 'layernorm',
+        normalize_before: bool = False,
+        normalize_embedding: bool = False,
+        feature_dropout: bool = False,
+        layerdrop: float = 0.0,
+        truncation: int = None,
+        rel_pos_bias: str = 'simple',
+        max_seq_len: int = 256,
+        export: bool = False,
+        traceable: bool = False,
+        sen_rep_type: str = 'cls',
+    ) -> None:
+
+        super().__init__()
+        self.padding_idx = padding_idx
+        self.vocab_size = vocab_size
+        self.embedding_dropout = FairseqDropout(dropout, module_name=self.__class__.__name__)
+        self.chunk_size = chunk_size
+        self.layerdrop = layerdrop
+        self.max_seq_len = max_seq_len
+        self.embedding_type = embedding_type
+        self.embedding_dim = embedding_dim
+        self.traceable = traceable
+        self.tpu = False  # whether we're on TPU
+        self.sen_rep_type = sen_rep_type
+
+        assert embedding_type in ['sparse', 'linear']
+        self.embed_tokens = self.build_embedding(self.embedding_type, self.embedding_dim,
+                                                 self.vocab_size, self.padding_idx)
+
+        assert not normalize_embedding or not normalize_before
+        self.embed_norm = SequenceNorm(norm_type, embedding_dim, export=export) if normalize_embedding else None
+
+        if self.layerdrop > 0.0:
+            self.layers = LayerDropModuleList(p=self.layerdrop)
+        else:
+            self.layers = nn.ModuleList([])
+        self.num_layers = num_encoder_layers
+
+        self.layers.extend([
+            self.build_mega_sentence_encoder_layer(
+                embedding_dim=self.embedding_dim,
+                hidden_dim=hidden_dim,
+                ffn_hidden_dim=ffn_hidden_dim,
+                z_dim=z_dim,
+                n_dim=n_dim,
+                dropout=dropout,
+                attention_dropout=attention_dropout,
+                hidden_dropout=hidden_dropout,
+                chunk_size=chunk_size,
+                truncation=truncation,
+                rel_pos_bias=rel_pos_bias,
+                max_positions=self.max_seq_len,
+                activation=activation,
+                attention_activation=attention_activation,
+                norm_type=norm_type,
+                prenorm=normalize_before,
+                feature_dropout=feature_dropout,
+                export=export
+            )
+            for _ in range(self.num_layers)
+        ])
+
+        if normalize_before:
+            self.final_norm = SequenceNorm(norm_type, embedding_dim, export=export)
+        else:
+            self.final_norm = None
+
+    def build_embedding(self, embedding_type, embedding_dim, vocab_size, padding_idx):
+        if embedding_type == 'sparse':
+            embed_tokens = Embedding(vocab_size, embedding_dim, padding_idx)
+            return embed_tokens
+        else:
+            embed_tokens = RealNumberEmbedding(embedding_dim)
+            return embed_tokens
+
+    def build_mega_sentence_encoder_layer(
+        self,
+        embedding_dim,
+        hidden_dim,
+        ffn_hidden_dim,
+        z_dim,
+        n_dim,
+        dropout,
+        attention_dropout,
+        hidden_dropout,
+        chunk_size,
+        truncation,
+        rel_pos_bias,
+        max_positions,
+        activation,
+        attention_activation,
+        norm_type,
+        prenorm,
+        feature_dropout,
+        export,
+    ):
+        return MegaSentenceEncoderLayer(
+            embedding_dim=embedding_dim,
+            hidden_dim=hidden_dim,
+            ffn_hidden_dim=ffn_hidden_dim,
+            z_dim=z_dim,
+            n_dim=n_dim,
+            dropout=dropout,
+            attention_dropout=attention_dropout,
+            hidden_dropout=hidden_dropout,
+            chunk_size=chunk_size,
+            truncation=truncation,
+            rel_pos_bias=rel_pos_bias,
+            max_positions=max_positions,
+            activation=activation,
+            attention_activation=attention_activation,
+            norm_type=norm_type,
+            prenorm=prenorm,
+            feature_dropout=feature_dropout,
+            export=export
+        )
+
+    def forward(
+            self,
+            tokens: torch.Tensor,
+            src_lengths: torch.Tensor,
+            last_state_only: bool = False,
+    ) -> Tuple[Union[torch.Tensor, List[torch.Tensor]], torch.Tensor]:
+
+        bsz, seq_len = tokens.size()
+        if self.chunk_size > 0 and seq_len > self.chunk_size and seq_len % self.chunk_size != 0:
+            assert self.embedding_type == 'sparse', 'for image the sequence length {} must be divided by chunk size {}'.format(seq_len, self.chunk_size)
+
+            num_paddings = math.ceil(seq_len / self.chunk_size) * self.chunk_size - seq_len
+            tokens = F.pad(tokens, (0, num_paddings), value=self.padding_idx)
+
+        if self.embedding_type == 'sparse':
+            padding_mask = tokens.eq(self.padding_idx)
+            if not self.traceable and not self.tpu and not padding_mask.any():
+                padding_mask = None
+            # B x T -> B x T x D
+            x = self.embed_tokens(tokens)
+        else:
+            padding_mask = None
+            # B x T -> B x T x D
+            x = self.embed_tokens(tokens)
+
+        if self.embed_norm is not None:
+            x = self.embed_norm(x)
+
+        x = self.embedding_dropout(x)
+
+        # account for padding while computing the representation
+        if padding_mask is not None:
+            # B x N
+            inverse_mask = 1.0 - padding_mask.type_as(x)
+            x = x * inverse_mask.unsqueeze(-1)
+        else:
+            inverse_mask = None
+
+        # B x T x C -> T x B x C
+        x = x.transpose(0, 1)
+
+        inner_states = []
+        if not last_state_only:
+            inner_states.append(x)
+
+        for i in range(self.num_layers):
+            x, _ = self.layers[i](x, x_padding_mask=padding_mask)
+            if not last_state_only:
+                inner_states.append(x)
+
+        if self.final_norm is not None:
+            x = self.final_norm(x)
+
+        if inverse_mask is not None:
+            x = x * inverse_mask.transpose(0, 1).unsqueeze(-1)
+
+        if self.sen_rep_type == 'mp':
+            sentence_rep = x.sum(dim=0) / src_lengths.unsqueeze(1)
+        else:
+            sentence_rep = x[0, :, :]
+
+        if last_state_only:
+            inner_states = [x]
+
+        if self.traceable:
+            return torch.stack(inner_states), sentence_rep
+        else:
+            return inner_states, sentence_rep
+
+
+class Embedding(nn.Module):
+    def __init__(self, vocab_size, embedding_dim, padding_idx):
+        super().__init__()
+        self.embed = nn.Embedding(vocab_size, embedding_dim, padding_idx)
+        self.reset_parameters(embedding_dim)
+
+    def reset_parameters(self, embedding_dim):
+        std = embedding_dim ** -0.5
+        nn.init.normal_(self.embed.weight, mean=0, std=std)
+
+    def forward(self, tokens):
+        x = self.embed(tokens)
+        return x
\ No newline at end of file
diff --git a/fairseq/models/lra/model.py b/fairseq/models/lra/model.py
new file mode 100644
index 00000000..f67531b2
--- /dev/null
+++ b/fairseq/models/lra/model.py
@@ -0,0 +1,442 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn as nn
+
+from fairseq import utils
+from fairseq.models import (
+    FairseqEncoderModel,
+    FairseqEncoder,
+    register_model,
+    register_model_architecture,
+)
+from fairseq.models.lra.lra_config import LRAConfig
+from fairseq.modules import (
+    FairseqDropout,
+)
+from fairseq.models.lra.transformer_lra_encoder import TransformerLRAEncoder
+from fairseq.models.lra.mega_lra_encoder import MegaLRAEncoder
+from fairseq.modules.transformer_sentence_encoder import init_bert_params
+from fairseq.utils import assign_default_rpu_params, safe_getattr, create_rpu_config, convert2analog
+
+from aihwkit.nn.conversion import convert_to_analog
+
+def Linear(in_features, out_features, bias=True):
+    m = nn.Linear(in_features, out_features, bias)
+    nn.init.xavier_uniform_(m.weight)
+    if bias:
+        nn.init.constant_(m.bias, 0.0)
+    return m
+
+
+@register_model("lra", dataclass=LRAConfig)
+class LRAModel(FairseqEncoderModel):
+    """
+    Class for training a transformer for LRA tasks.
+    """
+
+    def __init__(self, args, encoder, task):
+        super().__init__(encoder)
+        self.encoder = encoder
+        self.use_p = safe_getattr(args, "use_p", False)
+        self._max_positions = args.max_positions
+        self.sentence_out_dim = args.sentence_class_num
+        self.lm_output_learned_bias = None
+        self.task = safe_getattr(args, "_name", None).split("_")[-1]
+        self.dropout_module = FairseqDropout(
+            args.dropout, module_name=self.__class__.__name__
+        )
+        self.classifier = nn.ModuleList([])
+        if args.classifier_layers > 0:
+            self.classifier.append(
+                nn.Sequential(
+                    Linear(args.classifier_in_dim, args.classifier_out_dim),
+                    self.dropout_module,
+                )
+            )
+            self.classifier.extend(
+                [
+                    nn.Sequential(
+                        Linear(args.classifier_out_dim, args.classifier_out_dim),
+                        self.dropout_module,
+                    )
+                    for _ in range(args.classifier_layers - 1)
+                ]
+            )
+            self.classifier_activation = utils.get_activation_fn(
+                args.classifier_activation_fn
+            )
+
+        self.sentence_projection_layer = Linear(
+            args.classifier_out_dim, self.sentence_out_dim, bias=False
+        )
+        self.sen_rep_type = safe_getattr(args, "sen_rep_type", "cls")
+        self.layer_type = args.layer_type
+        self.traceable = False
+        # if specified then apply bert initialization on the model. We need
+        # to explictly call this to make sure that the output embeddings
+        # and projection layers are also correctly initialized
+        if safe_getattr(args, "apply_bert_init", False):
+            self.apply(init_bert_params)
+
+
+    def forward(self, sample):
+        src_tokens = sample["net_input"]["src_tokens"]
+        src_lengths = sample["net_input"]["src_lengths"]
+        sentence_rep = self.encoder(src_tokens, src_lengths)
+        if not self.use_p:
+            sentence_rep = sentence_rep[1]
+        else:
+            sentence_rep = sentence_rep[1][1].mean(dim=0)
+        if self.task == "aan":
+            src1_tokens = sample["net_input1"]["src_tokens"]
+            src1_lengths = sample["net_input1"]["src_lengths"]
+            sentence1_rep = self.encoder(src1_tokens, src1_lengths)
+            if not self.use_p:
+                sentence1_rep = sentence1_rep[1]
+            else:
+                sentence1_rep = sentence1_rep[1][1].mean(dim=0)
+            concat_rep = []
+            concat_rep.append(sentence1_rep)
+            concat_rep.append(sentence_rep)
+            sentence_rep = torch.cat(concat_rep, dim=-1)
+        for layer in self.classifier:
+            sentence_rep = self.classifier_activation(layer(sentence_rep))
+        if self.sentence_projection_layer:
+            sentence_logits = self.sentence_projection_layer(sentence_rep)
+        return {"encoder_out": sentence_logits}
+
+    def max_positions(self):
+        """Maximum output length supported by the encoder."""
+        return self._max_positions
+
+    @classmethod
+    def build_model(cls, args, task):
+        """Build a new model instance."""
+        if getattr(args, "analog", False):
+            import ast
+            xformers_args = ast.literal_eval(args.encoder.xformers_att_config)
+            xformers_args["sep_proj"] = True
+            args.encoder.xformers_att_config = str(xformers_args)
+
+        encoder = LRAEncoder(args, task)
+        model = cls(args, encoder, task)
+        if getattr(args, "analog", False):
+            model = convert_to_analog(model, create_rpu_config(args))
+        else:
+            model = convert2analog(model, args.modifier_std_dev, args.modifier_type)
+        return model
+
+
+class LRAEncoder(FairseqEncoder):
+    """LRA encoder."""
+    def __init__(self, args, task):
+        if args.input_type == "text":
+            dictionary = task.dictionary
+            vocab_size = len(dictionary)
+            padding_idx = dictionary.pad_index
+            offset_positions_by_padding = True
+            embedding_type = "sparse"
+        else:
+            assert args.sen_rep_type == "mp" or args.layer_type == "lstm"
+            dictionary = None
+            vocab_size = None
+            padding_idx = None
+            offset_positions_by_padding = False
+            embedding_type = "linear"
+        super().__init__(dictionary)
+        self.args = args
+        if args.layer_type == "transformer":
+            self.encoder = TransformerLRAEncoder(
+                tie_layer_weights=getattr(args, "tie_layer_weights", False),
+                padding_idx=padding_idx,
+                vocab_size=vocab_size,
+                num_encoder_layers=args.encoder.layers,
+                embedding_type=embedding_type,
+                embedding_dim=args.encoder.embed_dim,
+                ffn_embedding_dim=args.encoder.ffn_embed_dim,
+                num_attention_heads=args.encoder.attention_heads,
+                dropout=args.dropout,
+                bias=args.bias,
+                attention_dropout=args.attention_dropout,
+                activation_dropout=args.activation_dropout,
+                max_seq_len=args.max_positions,
+                use_position_embeddings=True,
+                offset_positions_by_padding=offset_positions_by_padding,
+                encoder_normalize_before=getattr(
+                    args.encoder, "normalize_before", False
+                ),
+                norm_type=args.norm_type,
+                apply_bert_init=getattr(args, "apply_bert_init", False),
+                activation_fn=args.activation_fn,
+                learned_pos_embedding=args.encoder.learned_pos,
+                sen_rep_type=getattr(args, "sen_rep_type", "cls"),
+                xformers_att_config=args.encoder.xformers_att_config,
+            )
+        elif args.layer_type == 'mega':
+            self.encoder = MegaLRAEncoder(
+                padding_idx=padding_idx,
+                vocab_size=vocab_size,
+                num_encoder_layers=args.encoder.layers,
+                embedding_type=embedding_type,
+                embedding_dim=args.encoder.embed_dim,
+                hidden_dim=args.encoder.hidden_dim,
+                ffn_hidden_dim=args.encoder.ffn_embed_dim,
+                z_dim=args.z_dim,
+                n_dim=args.n_dim,
+                activation=args.activation_fn,
+                attention_activation=args.attention_activation_fn,
+                dropout=args.dropout,
+                attention_dropout=args.attention_dropout,
+                hidden_dropout=args.activation_dropout,
+                norm_type=args.norm_type,
+                normalize_before=args.encoder.normalize_before,
+                normalize_embedding=args.normalize_embedding,
+                feature_dropout=args.feature_dropout,
+                chunk_size=getattr(args, 'chunk_size', -1),
+                truncation=getattr(args, 'truncation_length', 1024),
+                rel_pos_bias=args.rel_pos_bias,
+                max_seq_len=args.max_positions,
+                sen_rep_type=getattr(args, 'sen_rep_type', 'mp')
+            )
+
+    def forward(self, src_tokens, src_lengths=None, **kwargs):
+        return self.encoder(src_tokens, src_lengths, last_state_only=True)
+
+@register_model_architecture("lra", "lra_base")
+def base_architecture(args):
+    args.dropout = safe_getattr(args, "dropout", 0.1)
+    args.attention_dropout = safe_getattr(args, "attention_dropout", 0.1)
+    args.activation_dropout = safe_getattr(args, "activation_dropout", 0.0)
+    args.feature_dropout = safe_getattr(args, "feature_dropout", False)
+    args.encoder.ffn_embed_dim = safe_getattr(args.encoder, "ffn_embed_dim", 2048)
+    args.z_dim = safe_getattr(args, "z_dim", 128)
+    args.n_dim = safe_getattr(args, "n_dim", 2)
+    args.encoder.layers = safe_getattr(args.encoder, "layers", 6)
+    args.encoder.attention_heads = safe_getattr(args.encoder, "attention_heads", 8)
+    args.encoder.embed_dim = safe_getattr(args.encoder, "embed_dim", 512)
+    args.share_encoder_input_output_embed = safe_getattr(
+        args, "share_encoder_input_output_embed", False
+    )
+    args.encoder.learned_pos = safe_getattr(args.encoder, "learned_pos", False)
+    args.no_token_positional_embeddings = safe_getattr(
+        args, "no_token_positional_embeddings", False
+    )
+    args.classifier_out_dim = safe_getattr(args, "classifier_out_dim", 2048)
+    args.sentence_class_num = safe_getattr(args, "sentence_class_num", 2)
+    args.sent_loss = safe_getattr(args, "sent_loss", True)
+    args.apply_bert_init = safe_getattr(args, "apply_bert_init", True)
+    args.activation_fn = safe_getattr(args, "activation_fn", "gelu")
+    args.attention_activation_fn = safe_getattr(args, "attention_activation_fn", "relu2")
+    args.classifier_layers = safe_getattr(args, "classifier_layers", 1)
+    args.classifier_activation_fn = safe_getattr(args, "classifier_activation_fn", "gelu")
+    args.encoder.normalize_before = safe_getattr(args.encoder, "normalize_before", False)
+    args.normalize_embedding = safe_getattr(args, "normalize_embedding", False)
+    args.layer_type = safe_getattr(args, "layer_type", "transformer")
+    args.adaptive_input = safe_getattr(args, "adaptive_input", False)
+    args.classifier_in_dim = safe_getattr(
+        args,
+        "classifier_in_dim",
+        args.encoder.embed_dim,
+    )
+
+@register_model_architecture("lra", "transformer_lra_listops")
+def transformer_lra_listop(args):
+    args.sentence_class_num = safe_getattr(args, "sentence_class_num", 10)
+    args.max_positions = safe_getattr(args, "max_positions", 2002)
+    args.encoder.normalize_before = safe_getattr(args.encoder, "normalize_before", True)
+    args.tie_layer_weights = safe_getattr(args, "tie_layer_weights", True)
+    base_architecture(args)
+ 
+@register_model_architecture("lra", "transformer_lra_imdb")
+def transformer_lra_imdb(args):
+    args.max_positions = safe_getattr(args, "max_positions", 4002)
+    args.encoder.normalize_before = safe_getattr(args.encoder, "normalize_before", True)
+    args.encoder.ffn_embed_dim = safe_getattr(args.encoder, "ffn_embed_dim", 1024)
+    args.encoder.layers = safe_getattr(args.encoder, "layers", 4)
+    args.encoder.embed_dim = safe_getattr(args.encoder, "embed_dim", 256)
+    args.encoder.attention_heads = safe_getattr(args.encoder, "attention_heads", 4)
+    args.classifier_layers = safe_getattr(args, "classifier_layers", 1)
+    args.classifier_out_dim = safe_getattr(args, "classifier_out_dim", 1024)
+    base_architecture(args)
+
+@register_model_architecture("lra", "transformer_lra_aan")
+def transformer_lra_aan(args):
+    args.apply_bert_init = safe_getattr(args, "apply_bert_init", False)
+    args.max_positions = safe_getattr(args, "max_positions", 4002)
+    args.encoder.normalize_before = safe_getattr(args.encoder, "normalize_before", True)
+    args.encoder.ffn_embed_dim = safe_getattr(args.encoder, "ffn_embed_dim", 512)
+    args.encoder.layers = safe_getattr(args.encoder, "layers", 4)
+    args.encoder.embed_dim = safe_getattr(args.encoder, "embed_dim", 128)
+    args.encoder.attention_heads = safe_getattr(args.encoder, "attention_heads", 4)
+    args.classifier_layers = safe_getattr(args, "classifier_layers", 1)
+    args.classifier_out_dim = safe_getattr(args, "classifier_out_dim", 512)
+    args.classifier_in_dim = safe_getattr(
+        args, "classifier_in_dim", args.encoder.embed_dim * 2
+    )
+    base_architecture(args)
+
+@register_model_architecture("lra", "transformer_lra_cifar10")
+def transformer_lra_cifar10(args):
+    args.apply_bert_init = safe_getattr(args, "apply_bert_init", False)
+    args.encoder.ffn_embed_dim = safe_getattr(args.encoder, "ffn_embed_dim", 128)
+    args.encoder.layers = safe_getattr(args.encoder, "layers", 1)
+    args.encoder.embed_dim = safe_getattr(args.encoder, "embed_dim", 64)
+    args.encoder.attention_heads = safe_getattr(args.encoder, "attention_heads", 8)
+    args.classifier_layers = safe_getattr(args, "classifier_layers", 1)
+    args.classifier_out_dim = safe_getattr(args, "classifier_out_dim", 128)
+    args.sentence_class_num = safe_getattr(args, "sentence_class_num", 10)
+    args.max_positions = safe_getattr(args, "max_positions", 1024)
+    args.encoder.normalize_before = safe_getattr(args.encoder, "normalize_before", True)
+    base_architecture(args)
+
+@register_model_architecture("lra", "transformer_lra_pf32")
+def transformer_lra_pf32(args):
+    args.apply_bert_init = safe_getattr(args, "apply_bert_init", False)
+    args.layer_type = safe_getattr(args, 'layer_type', 'transformer')
+    args.encoder.ffn_embed_dim = safe_getattr(args.encoder, "ffn_embed_dim", 256)
+    args.encoder.layers = safe_getattr(args.encoder, "layers", 1)
+    args.encoder.embed_dim = safe_getattr(args.encoder, "embed_dim", 128)
+    args.encoder.attention_heads = safe_getattr(args.encoder, "attention_heads", 4)
+    args.classifier_layers = safe_getattr(args, "classifier_layers", 1)
+    args.classifier_out_dim = safe_getattr(args, "classifier_out_dim", 256)
+    args.sentence_class_num = safe_getattr(args, "sentence_class_num", 2)
+    args.max_positions = safe_getattr(args, "max_positions", 1024)
+    args.encoder.normalize_before = safe_getattr(args.encoder, "normalize_before", True)
+    args.sen_rep_type = safe_getattr(args, "sen_rep_type", "mp")
+    args.norm_type = safe_getattr(args, 'norm_type', 'batchnorm')
+    args.classifier_activation_fn = safe_getattr(args, "classifier_activation_fn", "relu")
+    base_architecture(args)
+
+@register_model_architecture("lra", "analog_transformer_lra_listops")
+def analog_transformer_lra_listop(args):
+    assign_default_rpu_params(args)
+    args.analog = safe_getattr(args, "analog", True)
+    transformer_lra_listop(args)
+
+@register_model_architecture("lra", "analog_transformer_lra_imdb")
+def analog_transformer_lra_imdb(args):
+    assign_default_rpu_params(args)
+    args.analog = safe_getattr(args, "analog", True)
+    transformer_lra_imdb(args)
+
+@register_model_architecture("lra", "analog_transformer_lra_aan")
+def analog_transformer_lra_aan(args):
+    assign_default_rpu_params(args)
+    args.analog = safe_getattr(args, "analog", True)
+    transformer_lra_aan(args)
+
+@register_model_architecture("lra", "analog_transformer_lra_cifar10")
+def analog_transformer_lra_cifar10(args):
+    assign_default_rpu_params(args)
+    args.analog = safe_getattr(args, "analog", True)
+    transformer_lra_cifar10(args)
+
+@register_model_architecture("lra", "analog_transformer_lra_pf32")
+def analog_transformer_lra_pf32(args):
+    assign_default_rpu_params(args)
+    args.analog = safe_getattr(args, "analog", True)
+    transformer_lra_pf32(args)
+
+# MEGA models.
+@register_model_architecture('lra', 'mega_lra_pf32')
+def mega_lra_pf32(args):
+    args.apply_bert_init = safe_getattr(args, 'apply_bert_init', False)
+    args.layer_type = safe_getattr(args, 'layer_type', 'mega')
+    args.encoder.hidden_dim = safe_getattr(args.encoder, 'hidden_dim', 256)
+    args.z_dim = safe_getattr(args, 'z_dim', 64)
+    args.n_dim = safe_getattr(args, 'n_dim', 16)
+    args.encoder.layers = safe_getattr(args.encoder, 'layers', 6)
+    args.activation_fn = safe_getattr(args, 'activation_fn', 'silu')
+    args.encoder.embed_dim = safe_getattr(args.encoder, 'embed_dim', 128)
+    args.norm_type = safe_getattr(args, 'norm_type', 'batchnorm')
+    args.classifier_layers = safe_getattr(args, 'classifier_layers', 1)
+    args.classifier_out_dim = safe_getattr(args, 'classifier_out_dim', 256)
+    args.sentence_class_num = safe_getattr(args, 'sentence_class_num', 2)
+    args.chunk_size = safe_getattr(args, 'chunk_size', 1024)
+    args.truncation_length = safe_getattr(args, 'truncation_length', 1024)
+    args.max_positions = safe_getattr(args, 'max_positions', 1024)
+    args.sen_rep_type = safe_getattr(args, 'sen_rep_type', 'mp')
+    base_architecture(args)
+
+@register_model_architecture('lra', 'mega_lra_cifar10')
+def mega_lra_cifar10(args):
+    args.apply_bert_init = safe_getattr(args, 'apply_bert_init', False)
+    args.layer_type = safe_getattr(args, 'layer_type', 'mega')
+    args.encoder.hidden_dim = safe_getattr(args.encoder, 'hidden_dim', 320)
+    args.z_dim = safe_getattr(args, 'z_dim', 96)
+    args.n_dim = safe_getattr(args, 'n_dim', 16)
+    args.encoder.layers = safe_getattr(args.encoder, 'layers', 8)
+    args.activation_fn = safe_getattr(args, 'activation_fn', 'silu')
+    args.encoder.embed_dim = safe_getattr(args.encoder, 'embed_dim', 160)
+    args.norm_type = safe_getattr(args, 'norm_type', 'batchnorm')
+    args.classifier_layers = safe_getattr(args, 'classifier_layers', 1)
+    args.classifier_out_dim = safe_getattr(args, 'classifier_out_dim', 320)
+    args.sentence_class_num = safe_getattr(args, 'sentence_class_num', 10)
+    args.chunk_size = safe_getattr(args, 'chunk_size', 1024)
+    args.truncation_length = safe_getattr(args, 'truncation_length', 1024)
+    args.max_positions = safe_getattr(args, 'max_positions', 1024)
+    args.sen_rep_type = safe_getattr(args, 'sen_rep_type', 'mp')
+    base_architecture(args)
+
+@register_model_architecture('lra', 'mega_lra_aan')
+def mega_lra_aan(args):
+    args.apply_bert_init = safe_getattr(args, 'apply_bert_init', False)
+    args.layer_type = safe_getattr(args, 'layer_type', 'mega')
+    args.encoder.hidden_dim = safe_getattr(args.encoder, 'hidden_dim', 256)
+    args.z_dim = safe_getattr(args, 'z_dim', 64)
+    args.n_dim = safe_getattr(args, 'n_dim', 16)
+    args.encoder.layers = safe_getattr(args.encoder, 'layers', 6)
+    args.activation_fn = safe_getattr(args, 'activation_fn', 'silu')
+    args.encoder.embed_dim = safe_getattr(args.encoder, 'embed_dim', 128)
+    args.classifier_layers = safe_getattr(args, 'classifier_layers', 1)
+    args.classifier_out_dim = safe_getattr(args, 'classifier_out_dim', 256)
+    args.classifier_in_dim = safe_getattr(args, 'classifier_in_dim', args.encoder.embed_dim * 2)
+    args.chunk_size = safe_getattr(args, 'chunk_size', -1)
+    args.truncation_length = safe_getattr(args, 'truncation_length', 1024)
+    args.max_positions = safe_getattr(args, 'max_positions', 4002)
+    args.sen_rep_type = safe_getattr(args, 'sen_rep_type', 'mp')
+    base_architecture(args)
+
+@register_model_architecture('lra', 'mega_lra_imdb')
+def mega_lra_imdb(args):
+    args.apply_bert_init = safe_getattr(args, 'apply_bert_init', False)
+    args.layer_type = safe_getattr(args, 'layer_type', 'mega')
+    args.encoder.hidden_dim = safe_getattr(args.encoder, 'hidden_dim', 256)
+    args.z_dim = safe_getattr(args, 'z_dim', 64)
+    args.n_dim = safe_getattr(args, 'n_dim', 16)
+    args.encoder.layers = safe_getattr(args.encoder, 'layers', 4)
+    args.activation_fn = safe_getattr(args, 'activation_fn', 'silu')
+    args.encoder.embed_dim = safe_getattr(args.encoder, 'embed_dim', 128)
+    args.classifier_layers = safe_getattr(args, 'classifier_layers', 1)
+    args.classifier_out_dim = safe_getattr(args, 'classifier_out_dim', 256)
+    args.chunk_size = safe_getattr(args, 'chunk_size', -1)
+    args.truncation_length = safe_getattr(args, 'truncation_length', 1024)
+    args.max_positions = safe_getattr(args, 'max_positions', 4002)
+    args.norm_type = safe_getattr(args, 'norm_type', 'scalenorm')
+    args.sen_rep_type = safe_getattr(args, 'sen_rep_type', 'mp')
+    base_architecture(args)
+
+@register_model_architecture('lra', 'mega_lra_listop')
+def mega_lra_listop(args):
+    args.apply_bert_init = safe_getattr(args, 'apply_bert_init', False)
+    args.layer_type = safe_getattr(args, 'layer_type', 'mega')
+    args.encoder.hidden_dim = safe_getattr(args.encoder, 'hidden_dim', 160)
+    args.z_dim = safe_getattr(args, 'z_dim', 64)
+    args.n_dim = safe_getattr(args, 'n_dim', 16)
+    args.encoder.layers = safe_getattr(args.encoder, 'layers', 6)
+    args.activation_fn = safe_getattr(args, 'activation_fn', 'silu')
+    args.encoder.embed_dim = safe_getattr(args.encoder, 'embed_dim', 80)
+    args.classifier_layers = safe_getattr(args, 'classifier_layers', 1)
+    args.classifier_out_dim = safe_getattr(args, 'classifier_out_dim', 160)
+    args.chunk_size = safe_getattr(args, 'chunk_size', -1)
+    args.truncation_length = safe_getattr(args, 'truncation_length', 1024)
+    args.max_positions = safe_getattr(args, 'max_positions', 2002)
+    args.norm_type = safe_getattr(args, 'norm_type', 'scalenorm')
+    args.sentence_class_num = safe_getattr(args, 'sentence_class_num', 10)
+    args.sen_rep_type = safe_getattr(args, 'sen_rep_type', 'mp')
+    base_architecture(args)
\ No newline at end of file
diff --git a/fairseq/models/lra/transformer_lra_encoder.py b/fairseq/models/lra/transformer_lra_encoder.py
new file mode 100644
index 00000000..0201afbd
--- /dev/null
+++ b/fairseq/models/lra/transformer_lra_encoder.py
@@ -0,0 +1,304 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional, Tuple, Union, List
+import math
+
+import torch
+import torch.nn as nn
+from fairseq.modules import (
+    FairseqDropout,
+    LayerDropModuleList,
+    LayerNorm,
+    MultiheadAttention,
+    PositionalEmbedding,
+    RealNumberEmbedding,
+    TransformerSentenceEncoderLayer,
+)
+from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_
+
+
+def init_bert_params(module):
+    """
+    Initialize the weights specific to the BERT Model.
+    This overrides the default initializations depending on the specified arguments.
+        1. If normal_init_linear_weights is set then weights of linear
+           layer will be initialized using the normal distribution and
+           bais will be set to the specified value.
+        2. If normal_init_embed_weights is set then weights of embedding
+           layer will be initialized using the normal distribution.
+        3. If normal_init_proj_weights is set then weights of
+           in_project_weight for MultiHeadAttention initialized using
+           the normal distribution (to be validated).
+    """
+
+    if isinstance(module, nn.Linear):
+        module.weight.data.normal_(mean=0.0, std=0.02)
+        if module.bias is not None:
+            module.bias.data.zero_()
+    if isinstance(module, nn.Embedding):
+        module.weight.data.normal_(mean=0.0, std=0.02)
+        if module.padding_idx is not None:
+            module.weight.data[module.padding_idx].zero_()
+    if isinstance(module, MultiheadAttention):
+        module.q_proj.weight.data.normal_(mean=0.0, std=0.02)
+        module.k_proj.weight.data.normal_(mean=0.0, std=0.02)
+        module.v_proj.weight.data.normal_(mean=0.0, std=0.02)
+
+
+class TransformerLRAEncoder(nn.Module):
+    """
+    Implementation for a Bi-directional Transformer based Sentence Encoder used
+    in BERT/XLM style pre-trained models.
+
+    This first computes the token embedding using the token embedding matrix,
+    position embeddings (if specified) and segment embeddings
+    (if specified). After applying the specified number of
+    TransformerEncoderLayers, it outputs all the internal states of the
+    encoder as well as the final representation associated with the first
+    token (usually CLS token).
+
+    Input:
+        - tokens: B x T matrix representing sentences
+        - segment_labels: B x T matrix representing segment label for tokens
+
+    Output:
+        - a tuple of the following:
+            - a list of internal model states used to compute the
+              predictions where each tensor has shape T x B x C
+            - sentence representation associated with first input token
+              in format B x C.
+    """
+
+    def __init__(
+        self,
+        padding_idx: int,
+        vocab_size: int,
+        num_encoder_layers: int = 6,
+        embedding_type: str = "sparse",
+        embedding_dim: int = 768,
+        ffn_embedding_dim: int = 3072,
+        num_attention_heads: int = 8,
+        dropout: float = 0.1,
+        bias: bool = True,
+        attention_dropout: float = 0.1,
+        activation_dropout: float = 0.1,
+        layerdrop: float = 0.0,
+        max_seq_len: int = 256,
+        use_position_embeddings: bool = True,
+        offset_positions_by_padding: bool = True,
+        encoder_normalize_before: bool = False,
+        apply_bert_init: bool = False,
+        activation_fn: str = "relu",
+        norm_type: str = "layernorm",
+        learned_pos_embedding: bool = True,
+        embed_scale: float = None,
+        export: bool = False,
+        traceable: bool = False,
+        tie_layer_weights: bool = False,
+        q_noise: float = 0.0,
+        qn_block_size: int = 8,
+        sen_rep_type: str = 'cls',
+        xformers_att_config: Optional[str] = None
+    ) -> None:
+
+        super().__init__()
+        self.sen_rep_type = sen_rep_type
+        self.padding_idx = padding_idx
+        self.vocab_size = vocab_size
+        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
+        self.layerdrop = layerdrop
+        self.max_seq_len = max_seq_len
+        self.embedding_type = embedding_type
+        self.embedding_dim = embedding_dim
+        self.use_position_embeddings = use_position_embeddings
+        self.apply_bert_init = apply_bert_init
+        self.learned_pos_embedding = learned_pos_embedding
+        self.traceable = traceable
+        self.tpu = False  # whether we're on TPU
+        self.tie_layer_weights = tie_layer_weights
+        self.xformers_att_config = xformers_att_config
+
+        assert embedding_type in ['sparse', 'linear']
+        self.embed_tokens = self.build_embedding(self.embedding_type, self.vocab_size, self.embedding_dim, self.padding_idx)
+        self.embed_scale = embed_scale
+
+        if q_noise > 0:
+            self.quant_noise = apply_quant_noise_(
+                nn.Linear(self.embedding_dim, self.embedding_dim, bias=False),
+                q_noise,
+                qn_block_size,
+            )
+        else:
+            self.quant_noise = None
+
+        self.embed_positions = (
+            PositionalEmbedding(
+                self.max_seq_len,
+                self.embedding_dim,
+                padding_idx=(self.padding_idx if offset_positions_by_padding else 0),
+                learned=self.learned_pos_embedding,
+            )
+            if self.use_position_embeddings
+            else None
+        )
+        if self.use_position_embeddings and not self.learned_pos_embedding:
+            if self.embed_scale is None:
+                self.embed_scale = math.sqrt(self.embedding_dim)
+
+        if self.layerdrop > 0.0:
+            self.layers = LayerDropModuleList(p=self.layerdrop)
+        else:
+            self.layers = nn.ModuleList([])
+        self.num_layers = num_encoder_layers
+        if self.tie_layer_weights:
+            real_num_layers = 1
+        else:
+            real_num_layers = num_encoder_layers
+        self.layers.extend([
+            self.build_transformer_sentence_encoder_layer(
+                embedding_dim=self.embedding_dim,
+                ffn_embedding_dim=ffn_embedding_dim,
+                num_attention_heads=num_attention_heads,
+                dropout=self.dropout_module.p,
+                bias=bias,
+                attention_dropout=attention_dropout,
+                activation_dropout=activation_dropout,
+                activation_fn=activation_fn,
+                norm_type=norm_type,
+                export=export,
+                q_noise=q_noise,
+                qn_block_size=qn_block_size,
+                traceable=self.traceable
+            )
+            for _ in range(real_num_layers)
+        ])
+
+        if encoder_normalize_before:
+            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)
+        else:
+            self.emb_layer_norm = None
+
+        # Apply initialization of model params after building the model
+        if self.apply_bert_init:
+            self.apply(init_bert_params)
+
+    def build_embedding(self, embedding_type, vocab_size, embedding_dim, padding_idx):
+        if embedding_type == 'sparse':
+            embed_tokens = nn.Embedding(vocab_size, embedding_dim, padding_idx)
+            nn.init.normal_(embed_tokens.weight, mean=0, std=embedding_dim ** -0.5)
+            return embed_tokens
+        else:
+            embed_tokens = RealNumberEmbedding(embedding_dim)
+            return embed_tokens
+
+    def build_transformer_sentence_encoder_layer(
+        self,
+        embedding_dim,
+        ffn_embedding_dim,
+        num_attention_heads,
+        dropout,
+        bias,
+        attention_dropout,
+        activation_dropout,
+        activation_fn,
+        norm_type,
+        export,
+        q_noise,
+        qn_block_size,
+        traceable,
+    ):
+        return TransformerSentenceEncoderLayer(
+            embedding_dim=embedding_dim,
+            ffn_embedding_dim=ffn_embedding_dim,
+            num_attention_heads=num_attention_heads,
+            dropout=dropout,
+            bias=bias,
+            attention_dropout=attention_dropout,
+            activation_dropout=activation_dropout,
+            activation_fn=activation_fn,
+            export=export,
+            norm_type=norm_type,
+            q_noise=q_noise,
+            qn_block_size=qn_block_size,
+            xformers_att_config=self.xformers_att_config,
+            traceable=traceable
+        )
+
+    def prepare_for_tpu_(self, **kwargs):
+        self.tpu = True
+
+    def forward(
+        self,
+        tokens: torch.Tensor,
+        src_lengths: torch.Tensor,
+        last_state_only: bool = False,
+        positions: Optional[torch.Tensor] = None,
+    ) -> Tuple[Union[torch.Tensor, List[torch.Tensor]], torch.Tensor]:
+
+        # compute padding mask. This is needed for multi-head attention
+        if self.embedding_type == 'sparse':
+            padding_mask = tokens.eq(self.padding_idx)
+            if not self.traceable and not self.tpu and not padding_mask.any():
+                padding_mask = None
+            # B x T -> B x T x D
+            x = self.embed_tokens(tokens)
+        else:
+            padding_mask = None
+            # B x T -> B x T x 1 -> B x T x D
+            x = self.embed_tokens(tokens)
+
+        if self.embed_scale is not None:
+            x *= self.embed_scale
+
+        if self.embed_positions is not None:
+            x += self.embed_positions(tokens, positions=positions)
+
+        if self.quant_noise is not None:
+            x = self.quant_noise(x)
+            
+        # assert self.emb_layer_norm is None
+        if self.emb_layer_norm is not None:
+            x = self.emb_layer_norm(x)
+
+        x = self.dropout_module(x)
+
+        # account for padding while computing the representation
+        if padding_mask is not None:
+            x = x.masked_fill(padding_mask.unsqueeze(-1), 0.0)
+
+        # B x T x C -> T x B x C
+        x = x.transpose(0, 1)
+
+        inner_states = []
+        if not last_state_only:
+            inner_states.append(x)
+
+        
+        for i in range(self.num_layers):
+            if self.tie_layer_weights:
+                j = 0
+            else:
+                j = i
+            x, _ = self.layers[j](x, self_attn_padding_mask=padding_mask)
+            if not last_state_only:
+                inner_states.append(x)
+
+        if padding_mask is not None:
+            x = x.masked_fill(padding_mask.transpose(0, 1).unsqueeze(-1), 0.0)
+
+        if self.sen_rep_type == 'mp':
+            sentence_rep = x.sum(dim=0) / src_lengths.unsqueeze(1)
+        else:
+            sentence_rep = x[0, :, :]
+
+        if last_state_only:
+            inner_states = [x]
+
+        if self.traceable:
+            return torch.stack(inner_states), sentence_rep
+        else:
+            return inner_states, sentence_rep
diff --git a/fairseq/models/roberta/enc_dec.py b/fairseq/models/roberta/enc_dec.py
index e538dee0..d753382f 100644
--- a/fairseq/models/roberta/enc_dec.py
+++ b/fairseq/models/roberta/enc_dec.py
@@ -70,7 +70,8 @@ class RobertaEncDecModel(FairseqEncoderDecoderModel):
                 args.encoder_normalize_before = False
             roberta_enc = roberta.RobertaModel.build_model(args, task)
 
-        return cls.from_roberta(roberta_enc, args, task.source_dictionary)
+        model = cls.from_roberta(roberta_enc, args, task.source_dictionary)
+        return model
 
     @staticmethod
     def from_roberta(roberta_enc: roberta.RobertaModel, args, dictionary):
diff --git a/fairseq/models/roberta/hub_interface.py b/fairseq/models/roberta/hub_interface.py
index ba298d63..1bd67695 100644
--- a/fairseq/models/roberta/hub_interface.py
+++ b/fairseq/models/roberta/hub_interface.py
@@ -9,6 +9,7 @@ import torch.nn as nn
 import torch.nn.functional as F
 from fairseq import utils
 from fairseq.data import encoders
+from fairseq.utils import analog2model
 
 
 class RobertaHubInterface(nn.Module):
@@ -84,10 +85,10 @@ class RobertaHubInterface(nn.Module):
     ) -> torch.Tensor:
         if tokens.dim() == 1:
             tokens = tokens.unsqueeze(0)
-        if tokens.size(-1) > self.model.max_positions():
+        if tokens.size(-1) > analog2model(self.model).max_positions():
             raise ValueError(
                 "tokens exceeds maximum length: {} > {}".format(
-                    tokens.size(-1), self.model.max_positions()
+                    tokens.size(-1), analog2model(self.model).max_positions()
                 )
             )
         features, extra = self.model(
@@ -105,13 +106,13 @@ class RobertaHubInterface(nn.Module):
     def register_classification_head(
         self, name: str, num_classes: int = None, embedding_size: int = None, **kwargs
     ):
-        self.model.register_classification_head(
+        analog2model(self.model).register_classification_head(
             name, num_classes=num_classes, embedding_size=embedding_size, **kwargs
         )
 
     def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool = False):
         features = self.extract_features(tokens.to(device=self.device))
-        logits = self.model.classification_heads[head](features)
+        logits = analog2model(self.model).classification_heads[head](features)
         if return_logits:
             return logits
         return F.log_softmax(logits, dim=-1)
diff --git a/fairseq/models/roberta/model.py b/fairseq/models/roberta/model.py
index d7ced919..76ee9c6d 100644
--- a/fairseq/models/roberta/model.py
+++ b/fairseq/models/roberta/model.py
@@ -23,10 +23,12 @@ from fairseq.models.transformer import DEFAULT_MIN_PARAMS_TO_WRAP, TransformerEn
 from fairseq.modules import LayerNorm
 from fairseq.modules.quant_noise import quant_noise as apply_quant_noise_
 from fairseq.modules.transformer_sentence_encoder import init_bert_params
-from fairseq.utils import safe_getattr, safe_hasattr
+from fairseq.utils import safe_getattr, safe_hasattr, assign_default_rpu_params, create_rpu_config
 
 from .hub_interface import RobertaHubInterface
 
+from aihwkit.nn.conversion import convert_to_analog
+
 logger = logging.getLogger(__name__)
 
 
@@ -239,7 +241,10 @@ class RobertaModel(FairseqEncoderModel):
         if OmegaConf.is_config(args):
             OmegaConf.set_struct(args, True)
 
-        return cls(args, encoder)
+        model = cls(args, encoder)
+        if getattr(args, "analog", False):
+            model = convert_to_analog(model, create_rpu_config(args))
+        return model
 
     def forward(
         self,
@@ -379,6 +384,29 @@ class RobertaModel(FairseqEncoderModel):
         logger.info(x["args"])
         return RobertaHubInterface(x["args"], x["task"], x["models"][0])
 
+    @classmethod
+    def from_pretrained_verbose(
+        cls,
+        model_name_or_path,
+        checkpoint_file="model.pt",
+        data_name_or_path=".",
+        bpe="gpt2",
+        **kwargs,
+    ):
+        from fairseq import hub_utils
+
+        x = hub_utils.from_pretrained(
+            model_name_or_path,
+            checkpoint_file,
+            data_name_or_path,
+            archive_map=cls.hub_models(),
+            bpe=bpe,
+            load_checkpoint_heads=True,
+            **kwargs,
+        )
+        return x["args"], x["task"], x["models"][0]
+
+
     def upgrade_state_dict_named(self, state_dict, name):
         prefix = name + "." if name != "" else ""
 
@@ -552,6 +580,8 @@ class RobertaEncoder(FairseqEncoder):
 
         self.sentence_encoder = self.build_encoder(args, dictionary, embed_tokens)
 
+        #TODO Custom head for distillation (need final layer dim of teacher)
+
         self.lm_head = self.build_lm_head(
             embed_dim=args.encoder_embed_dim,
             output_dim=len(dictionary),
@@ -624,6 +654,12 @@ class RobertaEncoder(FairseqEncoder):
         return self.args.max_positions
 
 
+@register_model_architecture("roberta", "analog_roberta")
+def analog_base_architecture(args):
+    base_architecture(args)
+    assign_default_rpu_params(args)
+    args.analog = safe_getattr(args, "analog", True)
+
 @register_model_architecture("roberta", "roberta")
 def base_architecture(args):
     args.encoder_layers = safe_getattr(args, "encoder_layers", 12)
@@ -669,6 +705,10 @@ def base_architecture(args):
         args, "spectral_norm_classification_head", False
     )
 
+    # Non-aihwkit HW-aware training
+    args.modifier_std_dev = safe_getattr(args, "modifier_std_dev", 0)
+    args.modifier_type = safe_getattr(args, "modifier_type", "none")
+
 
 @register_model_architecture("roberta", "roberta_prenorm")
 def roberta_prenorm_architecture(args):
@@ -682,6 +722,15 @@ def roberta_base_architecture(args):
     base_architecture(args)
 
 
+@register_model_architecture("roberta", "roberta_tiny")
+def roberta_large_architecture(args):
+    args.encoder_layers = safe_getattr(args, "encoder_layers", 8)
+    args.encoder_embed_dim = safe_getattr(args, "encoder_embed_dim", 252)
+    args.encoder_ffn_embed_dim = safe_getattr(args, "encoder_ffn_embed_dim", 510)
+    args.encoder_attention_heads = safe_getattr(args, "encoder_attention_heads", 7)
+    base_architecture(args)
+
+
 @register_model_architecture("roberta", "roberta_large")
 def roberta_large_architecture(args):
     args.encoder_layers = safe_getattr(args, "encoder_layers", 24)
diff --git a/fairseq/models/rxn/__init__.py b/fairseq/models/rxn/__init__.py
new file mode 100644
index 00000000..fd0cfe7d
--- /dev/null
+++ b/fairseq/models/rxn/__init__.py
@@ -0,0 +1,6 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from .model import RXNModel
\ No newline at end of file
diff --git a/fairseq/models/rxn/collation.py b/fairseq/models/rxn/collation.py
new file mode 100644
index 00000000..74b37827
--- /dev/null
+++ b/fairseq/models/rxn/collation.py
@@ -0,0 +1,32 @@
+import torch
+from torch.nn.utils.rnn import pad_sequence
+from typing import List
+
+# helper function to club together sequential operations
+def sequential_transforms(*transforms):
+    def func(txt_input):
+        for transform in transforms:
+            txt_input = transform(txt_input)
+        return txt_input
+    return func
+
+# function to add BOS/EOS and create tensor for input sequence indices
+def tensor_transform(token_ids: List[int]):
+    return torch.cat((torch.tensor([0]), 
+                      torch.tensor(token_ids), 
+                      torch.tensor([2])))
+
+def get_tokens(tokensied_smiles):
+    return tokensied_smiles.strip().split()
+
+
+# function to collate data samples into batch tesors
+def collate_fn(batch, text_transform: dict):
+    src_batch, tgt_batch = [], []
+    for src_sample, tgt_sample in batch:
+        src_batch.append(text_transform['source'](src_sample.rstrip("\n")))
+        tgt_batch.append(text_transform['target'](tgt_sample.rstrip("\n")))
+
+    src_batch = pad_sequence(src_batch, padding_value=1)
+    tgt_batch = pad_sequence(tgt_batch, padding_value=1)
+    return src_batch, tgt_batch
\ No newline at end of file
diff --git a/fairseq/models/rxn/model.py b/fairseq/models/rxn/model.py
new file mode 100644
index 00000000..26422909
--- /dev/null
+++ b/fairseq/models/rxn/model.py
@@ -0,0 +1,121 @@
+import torch
+import logging
+import math
+from fairseq.models import (
+    register_model,
+    register_model_architecture,
+    BaseFairseqModel,
+)
+from fairseq.models.rxn.rxn_config import RXNConfig
+from fairseq.utils import safe_getattr, create_rpu_config, convert2analog
+from aihwkit.nn.conversion import convert_to_analog
+from fairseq.models.transformer import (
+    TransformerDecoder,
+    TransformerEncoder
+)
+from fairseq.tasks.rxn_task import build_vocab
+
+# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.
+class PositionalEncoding(torch.nn.Module):
+    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):
+        super(PositionalEncoding, self).__init__()
+        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)
+        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
+        pos_embedding = torch.zeros((maxlen, emb_size))
+        pos_embedding[:, 0::2] = torch.sin(pos * den)
+        pos_embedding[:, 1::2] = torch.cos(pos * den)
+        pos_embedding = pos_embedding.unsqueeze(-2)
+
+        self.dropout = torch.nn.Dropout(dropout)
+        self.register_buffer("pos_embedding", pos_embedding)
+
+    def forward(self, token_embedding: torch.Tensor):
+        return self.dropout(
+            token_embedding + self.pos_embedding[: token_embedding.size(0), :]
+        )
+
+
+# helper Module to convert tensor of input indices into corresponding tensor of token embeddings
+class TokenEmbedding(torch.nn.Module):
+    def __init__(self, vocab_size: int, emb_size):
+        super(TokenEmbedding, self).__init__()
+        self.embedding = torch.nn.Embedding(vocab_size, emb_size)
+        self.emb_size = emb_size
+
+    def forward(self, tokens: torch.Tensor):
+        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)
+
+
+@register_model("rxn", dataclass=RXNConfig)
+class RXNModel(BaseFairseqModel):
+    def __init__(self, args):
+        super(RXNModel, self).__init__()
+
+        self.vocab = build_vocab(args.vocab_path)
+        self.generator = torch.nn.Linear(args.embed_dim, len(self.vocab))
+        self.encoder = TransformerEncoder(
+            args,
+            self.vocab,
+            torch.nn.Embedding(len(self.vocab), args.embed_dim, padding_idx=self.vocab.pad_index)
+        )
+        self.decoder = TransformerDecoder(
+            args,
+            self.vocab,
+            torch.nn.Embedding(len(self.vocab), args.embed_dim, padding_idx=self.vocab.pad_index),
+            output_projection=self.generator
+        )
+        # - Initialization
+        for n, p in self.named_parameters():
+            if p.dim() > 1:
+                torch.nn.init.xavier_uniform_(p)
+            if "embed_tokens.weight" in n:
+                logging.info(f"Setting pad index vec of {n} to 0.")
+                p.data[self.vocab.pad_index] *= 0.
+
+    def forward(
+        self,
+        src: torch.Tensor,
+        trg: torch.Tensor
+    ):
+        tgt_input = trg[:-1, :]
+        memory = self.encoder(src_tokens=src.transpose(0, 1), src_lengths=(~(src == self.vocab.pad_index)).long().sum(dim=0)) # [T (src), B, C]
+        new_outs = self.decoder(prev_output_tokens=tgt_input.transpose(0, 1), encoder_out=memory)[0].transpose(0, 1)
+        return new_outs
+
+    def encode(self, src: torch.Tensor, max_len: int = None):
+        return self.encoder(
+            src_tokens=src.transpose(0, 1),
+            src_lengths=(~(src == self.vocab.pad_index)).long().sum(dim=0),
+            max_len=max_len
+        )
+
+    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, max_in_len=None, max_out_len=None):
+        x, extra  = self.decoder(prev_output_tokens=tgt.transpose(0, 1), encoder_out=memory, max_in_len=max_in_len, max_out_len=max_out_len)
+        return x.transpose(0, 1), extra
+
+    @classmethod
+    def build_model(cls, args, task):
+        """Build a new model instance."""
+        model = cls(args)
+        if getattr(args, "analog", False):
+            model = convert_to_analog(model, create_rpu_config(args))
+        else:
+            model = convert2analog(model, args.modifier_std_dev, args.modifier_type)
+        return model
+
+
+@register_model_architecture("rxn", "rxn_base")
+def base_architecture(args):
+    args.dropout = safe_getattr(args, "dropout", 0.1)
+    args.ffn_embed_dim = safe_getattr(args, "ffn_embed_dim", 510)
+    args.attention_heads = safe_getattr(args, "attention_heads", 7)
+    args.embed_dim = safe_getattr(args, "embed_dim", 252)
+    args.hidden_dim = safe_getattr(args, "hidden_dim", 252)
+    args.num_encoder_layers = safe_getattr(args, "num_encoder_layers", 3)
+    args.num_decoder_layers = safe_getattr(args, "num_decoder_layers", 3)
+
+@register_model_architecture("rxn", "analog_rxn_base")
+def analog_base_architecture(args):
+    base_architecture(args)
+    args.analog = safe_getattr(args, "analog", True)
+    
\ No newline at end of file
diff --git a/fairseq/models/rxn/rxn_config.py b/fairseq/models/rxn/rxn_config.py
new file mode 100644
index 00000000..faa41e05
--- /dev/null
+++ b/fairseq/models/rxn/rxn_config.py
@@ -0,0 +1,50 @@
+from dataclasses import dataclass, field
+from typing import Optional
+
+from omegaconf import MISSING
+from fairseq.models.transformer.transformer_config import EncDecBaseConfig
+
+
+@dataclass
+class RXNConfig(EncDecBaseConfig):
+    dropout: Optional[float] = field(default=0, metadata={"help": "Dropout probability"})
+    vocab_path: Optional[str] = field(default=MISSING, metadata={"help": "path to vocabulary <the_path>/.vocabulary.pt"})
+
+    input_dim: Optional[int] = field(default=252)
+    output_dim: Optional[int] = field(default=252)
+    tile_type: Optional[str] = field(default="torch")
+    noise_per_sample: Optional[bool] = field(default=False)
+    modifier_std_dev: Optional[float] = field(default=0.0)
+    modifier_type: Optional[str] = field(default="none")
+    forward_out_noise: Optional[float] = field(default=0.0)
+    mapping_weight_scaling_omega: Optional[float] = field(default=1.0)
+    mapping_learn_out_scaling: Optional[bool] = field(default=True)
+    mapping_weight_scaling_columnwise: Optional[bool] = field(default=False)
+    mapping_out_scaling_columnwise: Optional[bool] = field(default=False)
+    remap_type: Optional[str] = field(default="layer")
+    forward_is_perfect: Optional[bool] = field(default=False)
+    clip_type: Optional[str] = field(default="none")
+    clip_sigma: Optional[float] = field(default=2.5)
+    clip_fixed_value: Optional[float] = field(default=1.0)
+    forward_inp_res: Optional[float] = field(default=1 / (2**8 - 2))
+    forward_out_res: Optional[float] = field(default=1 / (2**8 - 2))
+    forward_out_bound: Optional[float] = field(default=10.0)
+    forward_inp_bound: Optional[float] = field(default=1.0)
+    forward_bound_management: Optional[str] = field(default="none")
+    forward_noise_management: Optional[str] = field(default="abs_max")
+    forward_w_noise_type: Optional[str] = field(default="none")
+    forward_w_noise: Optional[float] = field(default=0.0)
+    input_range_enable: Optional[bool] = field(default=False)
+    input_range_manage_output_clipping: Optional[bool] = field(default=False)
+    input_range_decay: Optional[float] = field(default=0.01)
+    input_range_input_min_percentage: Optional[float] = field(default=0.95)
+    input_range_output_min_percentage: Optional[float] = field(default=0.95)
+    input_range_init_value: Optional[float] = field(default=3.0)
+    input_range_init_std_alpha: Optional[float] = field(default=3.0)
+    input_range_init_from_data: Optional[int] = field(default=0)
+    static_input_clipping: Optional[bool] = field(default=False)
+    static_input_clipping_quantile: Optional[float] = field(default=0.995)
+    split_layers: Optional[bool] = field(default=True)
+    max_input_size: Optional[int] = field(default=0)
+    max_output_size: Optional[int] = field(default=0)
+    eta_eval: Optional[float] = field(default=0.0)
diff --git a/fairseq/models/transformer/transformer_config.py b/fairseq/models/transformer/transformer_config.py
index 4650de2e..9262f23d 100644
--- a/fairseq/models/transformer/transformer_config.py
+++ b/fairseq/models/transformer/transformer_config.py
@@ -24,37 +24,44 @@ _NAME_PARSER = r"(decoder|encoder|quant_noise)_(.*)"
 
 @dataclass
 class EncDecBaseConfig(FairseqDataclass):
+
+    # TODO generalize this
+    # input_dim: Optional[int] = field(default=II("model.embed_dim"), metadata={"help": "input_size to decoder"})
+    # output_dim: Optional[int] = field(default=II("model.hidden_dim"), metadata={"help": "output size of decoder"})
+
     embed_path: Optional[str] = field(
         default=None, metadata={"help": "path to pre-trained embedding"}
     )
     embed_dim: Optional[int] = field(
         default=512, metadata={"help": "embedding dimension"}
     )
-    ffn_embed_dim: int = field(
+    ffn_embed_dim: Optional[int] = field(
         default=2048, metadata={"help": "embedding dimension for FFN"}
     )
-    layers: int = field(default=6, metadata={"help": "number of layers"})
-    attention_heads: int = field(
+    layers: Optional[int] = field(default=6, metadata={"help": "number of layers"})
+    attention_heads: Optional[int] = field(
         default=8, metadata={"help": "number of attention heads"}
     )
-    normalize_before: bool = field(
+    normalize_before: Optional[bool] = field(
         default=False, metadata={"help": "apply layernorm before each block"}
     )
-    learned_pos: bool = field(
+    learned_pos: Optional[bool] = field(
         default=False, metadata={"help": "use learned positional embeddings"}
     )
     # args for "Reducing Transformer Depth on Demand with Structured Dropout" (Fan et al., 2019)
-    layerdrop: float = field(default=0, metadata={"help": "LayerDrop probability"})
+    layerdrop: Optional[float] = field(default=0, metadata={"help": "LayerDrop probability"})
     layers_to_keep: Optional[List[int]] = field(
         default=None, metadata={"help": "which layers to *keep* when pruning"}
     )
-
     xformers_att_config: Optional[str] = field(
         default=None,
         metadata={
             "help": "config for xFormers attention, defined in xformers.components.attention.AttentionConfig"
         },
     )
+    hidden_dim: Optional[int] = field(
+        default=None
+    )
 
 
 @dataclass
diff --git a/fairseq/models/transformer/transformer_decoder.py b/fairseq/models/transformer/transformer_decoder.py
index c22e5625..3a23cc26 100644
--- a/fairseq/models/transformer/transformer_decoder.py
+++ b/fairseq/models/transformer/transformer_decoder.py
@@ -194,6 +194,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         alignment_heads: Optional[int] = None,
         src_lengths: Optional[Any] = None,
         return_all_hiddens: bool = False,
+        max_in_len: int = None,
+        max_out_len: int = None,
+        bsz: int = None
     ):
         """
         Args:
@@ -221,6 +224,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
             full_context_alignment=full_context_alignment,
             alignment_layer=alignment_layer,
             alignment_heads=alignment_heads,
+            max_in_len=max_in_len,
+            max_out_len=max_out_len,
+            bsz=bsz
         )
 
         if not features_only:
@@ -235,6 +241,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         full_context_alignment: bool = False,
         alignment_layer: Optional[int] = None,
         alignment_heads: Optional[int] = None,
+        max_in_len: int = None,
+        max_out_len: int = None,
+        bsz: int = None
     ):
         return self.extract_features_scriptable(
             prev_output_tokens,
@@ -243,6 +252,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
             full_context_alignment,
             alignment_layer,
             alignment_heads,
+            max_in_len,
+            max_out_len,
+            bsz=bsz
         )
 
     """
@@ -259,6 +271,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         full_context_alignment: bool = False,
         alignment_layer: Optional[int] = None,
         alignment_heads: Optional[int] = None,
+        max_in_len: int = None,
+        max_out_len: int = None,
+        bsz: int = None
     ):
         """
         Similar to *forward* but only return features.
@@ -285,9 +300,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
 
         enc: Optional[Tensor] = None
         padding_mask: Optional[Tensor] = None
-        if encoder_out is not None and len(encoder_out["encoder_out"]) > 0:
+        if encoder_out is not None and (self.traceable or len(encoder_out["encoder_out"]) > 0):
             enc = encoder_out["encoder_out"][0]
-        if encoder_out is not None and len(encoder_out["encoder_padding_mask"]) > 0:
+        if encoder_out is not None and (self.traceable or len(encoder_out["encoder_padding_mask"]) > 0):
             padding_mask = encoder_out["encoder_padding_mask"][0]
 
         # embed positions
@@ -325,7 +340,7 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         x = x.transpose(0, 1)
 
         self_attn_padding_mask: Optional[Tensor] = None
-        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
+        if self.traceable or (self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any()):
             self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)
 
         # decoder layers
@@ -333,7 +348,7 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         inner_states: List[Optional[Tensor]] = [x]
         for idx, layer in enumerate(self.layers):
             if incremental_state is None and not full_context_alignment:
-                self_attn_mask = self.buffered_future_mask(x)
+                self_attn_mask = self.buffered_future_mask(x, max_out_len)
             else:
                 self_attn_mask = None
 
@@ -346,6 +361,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
                 self_attn_padding_mask=self_attn_padding_mask,
                 need_attn=bool((idx == alignment_layer)),
                 need_head_weights=bool((idx == alignment_layer)),
+                max_in_len=max_in_len,
+                max_out_len=max_out_len,
+                bsz=bsz
             )
             inner_states.append(x)
             if layer_attn is not None and idx == alignment_layer:
@@ -383,10 +401,14 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
             return self.max_target_positions
         return min(self.max_target_positions, self.embed_positions.max_positions)
 
-    def buffered_future_mask(self, tensor):
-        dim = tensor.size(0)
+    def buffered_future_mask(self, tensor, max_len):
+        if self.traceable:
+            dim = max_len
+        else:
+            dim = tensor.size(0)
         # self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.
         if (
+            self.traceable or
             self._future_mask.size(0) == 0
             or (not self._future_mask.device == tensor.device)
             or self._future_mask.size(0) < dim
diff --git a/fairseq/models/transformer/transformer_encoder.py b/fairseq/models/transformer/transformer_encoder.py
index 17369ec8..ec1e3a7f 100644
--- a/fairseq/models/transformer/transformer_encoder.py
+++ b/fairseq/models/transformer/transformer_encoder.py
@@ -138,6 +138,8 @@ class TransformerEncoderBase(FairseqEncoder):
         src_lengths: Optional[torch.Tensor] = None,
         return_all_hiddens: bool = False,
         token_embeddings: Optional[torch.Tensor] = None,
+        max_len: int = None,
+        bsz: int = None,
     ):
         """
         Args:
@@ -163,7 +165,7 @@ class TransformerEncoderBase(FairseqEncoder):
                   Only populated if *return_all_hiddens* is True.
         """
         return self.forward_scriptable(
-            src_tokens, src_lengths, return_all_hiddens, token_embeddings
+            src_tokens, src_lengths, return_all_hiddens, token_embeddings, max_len, bsz
         )
 
     # TorchScript doesn't support super() method so that the scriptable Subclass
@@ -176,6 +178,8 @@ class TransformerEncoderBase(FairseqEncoder):
         src_lengths: Optional[torch.Tensor] = None,
         return_all_hiddens: bool = False,
         token_embeddings: Optional[torch.Tensor] = None,
+        max_len: int = None,
+        bsz: int = None
     ):
         """
         Args:
@@ -202,9 +206,13 @@ class TransformerEncoderBase(FairseqEncoder):
         """
         # compute padding mask
         encoder_padding_mask = src_tokens.eq(self.padding_idx)
-        has_pads = (
-            torch.tensor(src_tokens.device.type == "xla") or encoder_padding_mask.any()
-        )
+        if self.traceable:
+            # has_pads = encoder_padding_mask.any()
+            has_pads = torch.tensor(True)
+        else:
+            has_pads = (
+                torch.tensor(src_tokens.device.type == "xla") or encoder_padding_mask.any()
+            )
         # Torchscript doesn't handle bool Tensor correctly, so we need to work around.
         if torch.jit.is_scripting():
             has_pads = torch.tensor(1) if has_pads else torch.tensor(0)
@@ -228,7 +236,7 @@ class TransformerEncoderBase(FairseqEncoder):
         # encoder layers
         for layer in self.layers:
             lr = layer(
-                x, encoder_padding_mask=encoder_padding_mask if has_pads else None
+                x, encoder_padding_mask=encoder_padding_mask if has_pads else None, max_len=max_len, bsz=bsz
             )
 
             if isinstance(lr, tuple) and len(lr) == 2:
diff --git a/fairseq/modules/__init__.py b/fairseq/modules/__init__.py
index dcfda9b8..af9bae78 100644
--- a/fairseq/modules/__init__.py
+++ b/fairseq/modules/__init__.py
@@ -30,10 +30,13 @@ from .lightweight_convolution import LightweightConv, LightweightConv1dTBC
 from .linearized_convolution import LinearizedConvolution
 from .location_attention import LocationAttention
 from .lstm_cell_with_zoneout import LSTMCellWithZoneOut
+from .mega_sentence_encoder_layer import MegaSentenceEncoderLayer
 from .multihead_attention import MultiheadAttention
+from .real_number_embedding import RealNumberEmbedding
 from .positional_embedding import PositionalEmbedding
 from .same_pad import SamePad, SamePad2d
 from .scalar_bias import ScalarBias
+from .sequence_norm import SequenceNorm
 from .sinusoidal_positional_embedding import SinusoidalPositionalEmbedding
 from .transformer_sentence_encoder_layer import TransformerSentenceEncoderLayer
 from .transformer_sentence_encoder import TransformerSentenceEncoder
@@ -86,6 +89,7 @@ __all__ = [
     "LSTMCellWithZoneOut",
     "MultiheadAttention",
     "PositionalEmbedding",
+    "RealNumberEmbedding",
     "SamePad",
     "SamePad2d",
     "ScalarBias",
diff --git a/fairseq/modules/exponential_moving_average.py b/fairseq/modules/exponential_moving_average.py
new file mode 100644
index 00000000..76467b5d
--- /dev/null
+++ b/fairseq/modules/exponential_moving_average.py
@@ -0,0 +1,244 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import Dict, Optional, Tuple
+
+import torch
+import torch.nn.functional as F
+from torch import Tensor, nn
+
+from fairseq.incremental_decoding_utils import with_incremental_state
+
+
+@with_incremental_state
+class MultiHeadEMA(nn.Module):
+    """Exponential Moving Average Layer.
+
+    See "https://arxiv.org/abs/2209.10655" for more details.
+    """
+
+    def __init__(
+        self,
+        embed_dim,
+        ndim=2,
+        bidirectional=False,
+        truncation=None,
+    ):
+        super().__init__()
+
+        self.embed_dim = embed_dim
+        self.ndim = ndim
+        self.bidirectional = bidirectional
+        self.truncation = truncation
+        self.scale = math.sqrt(1.0 / self.ndim)
+
+        kernel_dim = 2 * embed_dim if self.bidirectional else embed_dim
+        self.delta = nn.Parameter(torch.Tensor(kernel_dim, ndim, 1))
+        self.alpha = nn.Parameter(torch.Tensor(kernel_dim, ndim, 1))
+        self.beta = nn.Parameter(torch.Tensor(kernel_dim, ndim, 1))
+        self.gamma = nn.Parameter(torch.Tensor(kernel_dim, ndim))
+        self.omega = nn.Parameter(torch.Tensor(embed_dim))
+        self._kernel = None
+        self._coeffs = None
+
+        self.reset_parameters()
+
+        self.onnx_trace = False
+        self.tpu = False
+
+    def prepare_for_onnx_export_(self):
+        self.onnx_trace = True
+
+    def prepare_for_tpu_(self, **kwargs):
+        self.tpu = True
+
+    def reset_parameters(self):
+        with torch.no_grad():
+            # delta & alpha
+            nn.init.normal_(self.delta, mean=0.0, std=0.2)
+            nn.init.normal_(self.alpha, mean=0.0, std=0.2)
+            # beta [1, -1, 1, -1, ...] seems more stable.
+            val = torch.ones(self.ndim, 1)
+            if self.ndim > 1:
+                idx = torch.tensor(list(range(1, self.ndim, 2)))
+                val.index_fill_(0, idx, -1.0)
+            self.beta.normal_(mean=0.0, std=0.02).add_(val)
+            # gamma & omega
+            nn.init.normal_(self.gamma, mean=0.0, std=1.0)
+            nn.init.normal_(self.omega, mean=0.0, std=1.0)
+
+    def _calc_coeffs(self):
+        self._coeffs = None
+        # D x N x 1
+        p = torch.sigmoid(self.delta)
+        alpha = torch.sigmoid(self.alpha)
+        q = 1.0 - p * alpha
+        return p, q
+
+    def _compute_kernel(self, length: int):
+        self._kernel = None
+        # D x N x 1
+        p, q = self._calc_coeffs()
+        # D x N x L
+        vander = torch.arange(length).to(p).view(1, 1, length) * torch.log(q)
+        kernel = (p * self.beta) * torch.exp(vander)
+        # D x L
+        return torch.einsum('dnl,dn->dl', kernel, self.gamma * self.scale)
+
+    def coeffs(self):
+        if self.training:
+            return self._calc_coeffs()
+        else:
+            if self._coeffs is None:
+                self._coeffs = self._calc_coeffs()
+            return self._coeffs
+
+    def kernel(self, length: int):
+        kernel_size = length if self.truncation is None else min(self.truncation, length)
+        if self.training:
+            return self._compute_kernel(kernel_size)
+        else:
+            if self._kernel is None or self._kernel.size(-1) < kernel_size:
+                self._kernel = self._compute_kernel(kernel_size)
+            return self._kernel[..., :kernel_size]
+
+    def step(self, x, length, hx=None):
+        if length == 1:
+            return self.one_step(x, hx=hx)
+
+        # D x N x 1
+        p, q = self.coeffs()
+        # D x N x L+1
+        vander = torch.arange(length + 1).to(p).view(1, 1, length + 1) * torch.log(q)
+        vander = torch.exp(vander)
+        if hx is not None:
+            # D x N x L * D x N x 1 -> D x N x L
+            k = vander[:, :, 1:] * (self.gamma * self.scale).unsqueeze(-1)
+            ox = torch.einsum('bdn,dnl->bdl', hx, k)
+            # D x N * B x D x N -> B x D x N
+            hh = vander[:, :, -1] * hx
+        else:
+            ox = None
+            hh = None
+
+        # D x N x L
+        vander = vander[:, :, :-1]
+        kernel = (p * self.beta) * vander
+        k = torch.einsum('dnl,dn->dl', kernel, self.gamma * self.scale)
+
+        k_f = torch.fft.rfft(k.float(), n=2 * length)
+        x_f = torch.fft.rfft(x.float(), n=2 * length)
+        # B x D x L
+        out = torch.fft.irfft(x_f * k_f, n=2 * length)[..., 0:length]
+        out = out.type_as(x)
+        if ox is not None:
+            out = out + ox
+
+        h = torch.einsum('bdl,dnl->bdn', x, torch.flip(kernel, dims=[2]))
+        if hh is not None:
+            h = h + hh
+        # L x B x D, B x D x N
+        return out.permute(2, 0, 1), h
+
+    def one_step(self, x, hx=None):
+        p, q = self.coeffs()
+        # (D x N) x (B x D x 1) -> B x D x N
+        h = (p * self.beta).squeeze(-1) * x
+        if hx is not None:
+            h = h + q.squeeze(-1) * hx
+        # B x D
+        out = torch.einsum('bdn,dn->bd', h, self.gamma * self.scale)
+        # 1 x B x D, B x D x N
+        return out.unsqueeze(0), h
+
+    def forward(
+        self,
+        x,
+        padding_mask: Optional[Tensor] = None,
+        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
+    ) -> Tensor:
+        """Input shape: Time x Batch x Channel
+        Args:
+            padding_mask (ByteTensor, optional): mask to exclude
+                keys that are pads, of shape `(batch, src_len)`, where
+                padding elements are indicated by 1s.
+        """
+
+        seq_len, bsz, embed_dim = x.size()
+        assert embed_dim == self.embed_dim
+
+        # L x B x D
+        residual = x * self.omega
+
+        # L x B x D -> B x D x L
+        x = x.permute(1, 2, 0)
+        if padding_mask is not None:
+            x = x * (1.0 - padding_mask.unsqueeze(1).type_as(x))
+
+        assert not self.bidirectional or incremental_state is None, 'Bidirectional EMA does not support incremental state'
+        if incremental_state is not None:
+            saved_state = self._get_input_buffer(incremental_state)
+            if 'prev_state' in saved_state:
+                h = saved_state['prev_state']
+            else:
+                h = None
+            out, h = self.step(x, seq_len, hx=h)
+            saved_state['prev_state'] = h
+            self._set_input_buffer(incremental_state, saved_state)
+            # B x D -> 1 x B x D
+            out = F.silu(out + residual)
+        else:
+            # D x L
+            k = self.kernel(seq_len)
+            fft_len = seq_len
+            s = 0
+            kernel_size = k.size(1)
+            if self.bidirectional:
+                k1, k2 = torch.split(k, [self.embed_dim, self.embed_dim], dim=0)
+                # D x 2*L-1
+                k = F.pad(k1, (kernel_size - 1, 0)) + F.pad(k2.flip(-1), (0, kernel_size - 1))
+                x = F.pad(x, (kernel_size - 1, 0))
+                fft_len = fft_len + kernel_size - 1
+                s = 2 * kernel_size - 2
+
+            k_f = torch.fft.rfft(k.float(), n=2 * fft_len)
+            x_f = torch.fft.rfft(x.float(), n=2 * fft_len)
+            # B x D x L
+            out = torch.fft.irfft(x_f * k_f, n=2 * fft_len)[..., s:s + seq_len]
+            out = out.type_as(x)
+            # B x D x L -> L x B x D
+            out = F.silu(out.permute(2, 0, 1) + residual)
+
+        return out
+
+    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]) -> Dict[str, Optional[Tensor]]:
+        result = self.get_incremental_state(incremental_state, "ema_state")
+        if result is not None:
+            return result
+        else:
+            empty_result: Dict[str, Optional[Tensor]] = {}
+            return empty_result
+
+    def _set_input_buffer(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], buffer: Dict[str, Optional[Tensor]]):
+        return self.set_incremental_state(incremental_state, "ema_state", buffer)
+
+    @torch.jit.export
+    def reorder_incremental_state(
+            self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order: Tensor
+    ):
+        """Reorder buffered internal state (for incremental generation)."""
+        input_buffer = self._get_input_buffer(incremental_state)
+        if input_buffer is not None:
+            for k in input_buffer.keys():
+                input_buffer_k = input_buffer[k]
+                if input_buffer_k is not None:
+                    input_buffer[k] = input_buffer_k.index_select(0, new_order)
+            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
+        return incremental_state
+
+    def extra_repr(self) -> str:
+        return 'edim={}, ndim={}, bidirectional={}, trunction={}'.format(self.embed_dim, self.ndim, self.bidirectional, self.truncation)
\ No newline at end of file
diff --git a/fairseq/modules/fairseq_dropout.py b/fairseq/modules/fairseq_dropout.py
index 3cddca77..1d5ea150 100644
--- a/fairseq/modules/fairseq_dropout.py
+++ b/fairseq/modules/fairseq_dropout.py
@@ -20,7 +20,7 @@ class FairseqDropout(nn.Module):
         self.module_name = module_name
         self.apply_during_inference = False
 
-    def forward(self, x, inplace: bool = False):
+    def forward(self, x, batch_first: bool = False, inplace: bool = False):
         if self.p > 0 and (self.training or self.apply_during_inference):
             return F.dropout(x, p=self.p, training=True, inplace=inplace)
         else:
@@ -49,3 +49,50 @@ class FairseqDropout(nn.Module):
                 self.apply_during_inference = True
             else:
                 logger.info("Disabling dropout for module: {}".format(name))
+
+class FairseqFeatureDropout(nn.Module):
+
+    def __init__(self, p, module_name=None):
+        super().__init__()
+        self.p = p
+        self.module_name = module_name
+        self.apply_during_inference = False
+
+    def forward(self, x, batch_first: bool = False, inplace: bool = False):
+        if self.training or self.apply_during_inference:
+            if batch_first:
+                # B x L x D -> B x D x L -> B x L x D
+                return F.dropout2d(x.transpose(-1, -2), p=self.p, training=True, inplace=inplace).transpose(-1, -2)
+            else:
+                assert x.dim() == 3
+                # L x B x D -> B x D x L -> L x B x D
+                return F.dropout2d(x.permute(1, 2, 0), p=self.p, training=True, inplace=inplace).permute(2, 0, 1)
+        else:
+            return x
+
+    def make_generation_fast_(
+        self,
+        name: str,
+        retain_dropout: bool = False,
+        retain_dropout_modules: Optional[List[str]] = None,
+        **kwargs
+    ):
+        if retain_dropout:
+            if retain_dropout_modules is not None and self.module_name is None:
+                logger.warning(
+                    'Cannot enable dropout during inference for module {} '
+                    'because module_name was not set'.format(name)
+                )
+            elif (
+                retain_dropout_modules is None  # if None, apply to all modules
+                or self.module_name in retain_dropout_modules
+            ):
+                logger.info(
+                    'Enabling dropout during inference for module: {}'.format(name)
+                )
+                self.apply_during_inference = True
+            else:
+                logger.info('Disabling dropout for module: {}'.format(name))
+
+    def extra_repr(self) -> str:
+        return 'p={}'.format(self.p)
\ No newline at end of file
diff --git a/fairseq/modules/layer_norm.py b/fairseq/modules/layer_norm.py
index 0b276ce0..cf31fd4b 100644
--- a/fairseq/modules/layer_norm.py
+++ b/fairseq/modules/layer_norm.py
@@ -7,32 +7,33 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-try:
-    from apex.normalization import FusedLayerNorm as _FusedLayerNorm
+# try:
+#     from apex.normalization import FusedLayerNorm as _FusedLayerNorm
 
-    has_fused_layernorm = True
+#     has_fused_layernorm = True
 
-    class FusedLayerNorm(_FusedLayerNorm):
-        @torch.jit.unused
-        def forward(self, x):
-            if not x.is_cuda:
-                return super().forward(x)
-            else:
-                with torch.cuda.device(x.device):
-                    return super().forward(x)
+#     class FusedLayerNorm(_FusedLayerNorm):
+#         @torch.jit.unused
+#         def forward(self, x):
+#             return super().forward(x)
+#             # !!! only works on cuda !!!
+#             # if not x.is_cuda:
+#             #     return super().forward(x)
+#             # else:
+#             #     with torch.cuda.device(x.device):
+#             #         return super().forward(x)
 
-except ImportError:
-    has_fused_layernorm = False
+# except ImportError:
+#     has_fused_layernorm = False
 
 
 def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):
-    if torch.jit.is_scripting() or torch.jit.is_tracing():
-        export = True
-    if not export and torch.cuda.is_available() and has_fused_layernorm:
-        return FusedLayerNorm(normalized_shape, eps, elementwise_affine)
+    # if torch.jit.is_scripting() or torch.jit.is_tracing():
+    #     export = True
+    # if not export and torch.cuda.is_available() and has_fused_layernorm:
+    #     return FusedLayerNorm(normalized_shape, eps, elementwise_affine)
     return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)
 
-
 class Fp32LayerNorm(nn.LayerNorm):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
diff --git a/fairseq/modules/mega_sentence_encoder_layer.py b/fairseq/modules/mega_sentence_encoder_layer.py
new file mode 100644
index 00000000..f678a836
--- /dev/null
+++ b/fairseq/modules/mega_sentence_encoder_layer.py
@@ -0,0 +1,115 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+from typing import Optional
+import math
+
+import torch
+import torch.nn as nn
+
+from fairseq.modules.moving_average_gated_attention import MovingAverageGatedAttention
+from fairseq.modules.normalized_feedforward_network import NormalizedFeedForwardNetwork
+
+
+class MegaSentenceEncoderLayer(nn.Module):
+    """
+        Implements a Flash-Quad encoder layer.
+    """
+
+    def __init__(
+        self,
+        embedding_dim: int = 512,
+        hidden_dim: int = 1024,
+        ffn_hidden_dim: int = 1024,
+        z_dim: int = 128,
+        n_dim: int = 16,
+        dropout: float = 0.0,
+        attention_dropout: float = 0.0,
+        hidden_dropout: float = 0.0,
+        chunk_size: int = -1,
+        truncation: int = None,
+        rel_pos_bias = 'simple',
+        max_positions: int = 1024,
+        activation='silu',
+        attention_activation: str = 'softmax',
+        norm_type: str = 'layernorm',
+        prenorm: bool = True,
+        feature_dropout: bool = False,
+        export: bool = False,
+    ) -> None:
+        super().__init__()
+
+        self.embedding_dim = embedding_dim
+        self.chunk_size = chunk_size
+        self.mega_layer = self.build_mega_layer(embedding_dim, hidden_dim, z_dim, n_dim,
+                                           dropout, attention_dropout, hidden_dropout,
+                                           activation, attention_activation,
+                                           chunk_size, truncation, rel_pos_bias, max_positions,
+                                           norm_type, prenorm, feature_dropout, export)
+
+        if ffn_hidden_dim is not None and ffn_hidden_dim > 0:
+            self.nffn = self.build_nffn_layer(embedding_dim, ffn_hidden_dim,
+                                              dropout, hidden_dropout, activation,
+                                              norm_type, prenorm, feature_dropout, export)
+        else:
+            self.nffn = None
+
+    def build_mega_layer(self, embedding_dim, hidden_dim, z_dim, n_dim,
+                         dropout, attention_dropout, hidden_dropout,
+                         activation, attention_activation,
+                         chunk_size, truncation, rel_pos_bias, max_positions,
+                         norm_type, prenorm, feature_dropout, export):
+        return MovingAverageGatedAttention(
+            embed_dim=embedding_dim,
+            zdim=z_dim,
+            hdim=hidden_dim,
+            ndim=n_dim,
+            dropout=dropout,
+            attention_dropout=attention_dropout,
+            hidden_dropout=hidden_dropout,
+            chunk_size=chunk_size,
+            truncation=truncation,
+            rel_pos_bias=rel_pos_bias,
+            max_positions=max_positions,
+            activation=activation,
+            attention_activation=attention_activation,
+            bidirectional=True,
+            norm_type=norm_type,
+            prenorm=prenorm,
+            feature_dropout=feature_dropout,
+            export=export
+        )
+
+    def build_nffn_layer(self, embedding_dim, ffn_hidden_dim,
+                         dropout, hidden_dropout, activation,
+                         norm_type, prenorm, feature_dropout, export):
+        return NormalizedFeedForwardNetwork(
+            embed_dim=embedding_dim,
+            ffn_hidden_dim=ffn_hidden_dim,
+            dropout=dropout,
+            hidden_dropout=hidden_dropout,
+            activation=activation,
+            norm_type=norm_type,
+            prenorm=prenorm,
+            feature_dropout=feature_dropout,
+            export=export
+        )
+
+    def forward(
+        self,
+        x: torch.Tensor,
+        x_padding_mask: Optional[torch.Tensor] = None,
+    ):
+
+        seq_len = x.size(0)
+        if self.chunk_size > 0:
+            assert seq_len % self.chunk_size == 0, 'the input sequence length {} cannot be divided by chunk size {}'.format(seq_len, self.chunk_size)
+        x, attn = self.mega_layer(x, x_padding_mask)
+
+        if self.nffn is not None:
+            x = self.nffn(x)
+
+        return x, attn
\ No newline at end of file
diff --git a/fairseq/modules/moving_average_gated_attention.py b/fairseq/modules/moving_average_gated_attention.py
new file mode 100644
index 00000000..52bee7ba
--- /dev/null
+++ b/fairseq/modules/moving_average_gated_attention.py
@@ -0,0 +1,403 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import math
+from typing import Dict, Optional, Tuple
+
+import torch
+import torch.nn.functional as F
+from torch import Tensor, nn
+from torch.nn import Parameter
+
+from fairseq import utils
+from fairseq.incremental_decoding_utils import with_incremental_state
+from fairseq.modules.fairseq_dropout import FairseqDropout, FairseqFeatureDropout
+from fairseq.modules.relative_positional_bias import SimpleRelativePositionalBias, RotaryRelativePositionalBias
+from fairseq.modules.sequence_norm import SequenceNorm
+from fairseq.modules.exponential_moving_average import MultiHeadEMA
+
+
+@with_incremental_state
+class MovingAverageGatedAttention(nn.Module):
+    """Exponential Moving Average Gated Attention.
+
+    See "" for more details.
+    """
+
+    def __init__(
+        self,
+        embed_dim,
+        zdim,
+        hdim,
+        ndim,
+        dropout=0.0,
+        attention_dropout=0.0,
+        hidden_dropout=0.0,
+        activation='silu',
+        attention_activation='softmax',
+        bidirectional=False,
+        chunk_size=-1,
+        truncation=None,
+        norm_type='layernorm',
+        prenorm=True,
+        norm_affine=True,
+        feature_dropout=False,
+        rel_pos_bias='simple',
+        max_positions=1024,
+        export=False,
+    ):
+        super().__init__()
+
+        self.embed_dim = embed_dim
+        self.hdim = hdim
+        self.zdim = zdim
+        self.ndim = ndim
+        self.activation = utils.get_activation_fn(activation=activation)
+        self.attention_activation = attention_activation
+        self.scaling = self.zdim ** -0.5 if attention_activation == 'softmax' else None
+
+        dropout_module = FairseqFeatureDropout if feature_dropout else FairseqDropout
+        self.dropout = dropout_module(dropout, module_name=self.__class__.__name__)
+        self.hidden_dropout = dropout_module(hidden_dropout, module_name=self.__class__.__name__)
+        # Attention dropout is standard dropout
+        self.attention_dropout = FairseqDropout(attention_dropout, module_name=self.__class__.__name__)
+
+        self.chunk_size = chunk_size
+        self.prenorm = prenorm
+        self.norm = SequenceNorm(norm_type, embed_dim, affine=norm_affine, export=export)
+
+        self.move = MultiHeadEMA(embed_dim, ndim=ndim, bidirectional=bidirectional, truncation=truncation)
+
+        self.v_proj = nn.Linear(embed_dim, hdim)
+        self.mx_proj = nn.Linear(embed_dim, zdim + hdim + 2 * embed_dim)
+        self.h_proj = nn.Linear(hdim, embed_dim)
+
+        self.gamma = Parameter(torch.Tensor(2, zdim))
+        self.beta = Parameter(torch.Tensor(2, zdim))
+
+        self.max_positions = max_positions
+        max_positions = max_positions if chunk_size < 0 else chunk_size
+        if rel_pos_bias == 'simple':
+            self.rel_pos_bias = SimpleRelativePositionalBias(max_positions)
+        elif rel_pos_bias == 'rotary':
+            self.rel_pos_bias = RotaryRelativePositionalBias(zdim, max_positions)
+        else:
+            raise ValueError('unknown relative position bias: {}'.format(rel_pos_bias))
+
+        self.reset_parameters()
+
+        self.onnx_trace = False
+        self.tpu = False
+
+    def prepare_for_onnx_export_(self):
+        self.onnx_trace = True
+
+    def prepare_for_tpu_(self, **kwargs):
+        self.tpu = True
+
+    def reset_parameters(self):
+        std = 0.02
+        nn.init.normal_(self.v_proj.weight, mean=0.0, std=std)
+        nn.init.constant_(self.v_proj.bias, 0.0)
+
+        nn.init.normal_(self.mx_proj.weight, mean=0.0, std=std)
+        nn.init.constant_(self.mx_proj.bias, 0.0)
+
+        nn.init.normal_(self.h_proj.weight, mean=0.0, std=std)
+        nn.init.constant_(self.h_proj.bias, 0.0)
+
+        nn.init.normal_(self.gamma, mean=0.0, std=std)
+        nn.init.constant_(self.beta, 0.0)
+
+    def element_attention(self, q, k, padding_mask, attn_mask, before_attn_fn):
+        slen = k.size(2)
+        if padding_mask is not None:
+            # B x K x C
+            inverse_mask = 1.0 - padding_mask.type_as(q)
+            # B x K x 1
+            lengths = inverse_mask.sum(dim=-1, keepdim=True)
+            # B x K x 1 x 1
+            lengths = lengths.clamp(min=1.0).unsqueeze(-1)
+        else:
+            lengths = slen
+            inverse_mask = None
+
+        if attn_mask is not None:
+            # C x 1
+            lengths = attn_mask.sum(dim=-1, keepdim=True)
+
+        # C x C
+        bias = self.rel_pos_bias(slen)
+        if slen != q.size(2):
+            assert q.size(2) == 1
+            # 1 x C
+            bias = bias[-1:]
+
+        # B x K x C x C
+        qk = torch.matmul(q, k.transpose(2, 3)) / lengths + bias
+
+        if before_attn_fn:
+            return qk
+
+        if self.attention_activation == 'relu2':
+            attn_weights = utils.relu_squared(qk).type_as(qk)
+        elif self.attention_activation == 'laplace':
+            attn_weights = utils.laplace(qk).type_as(qk)
+        else:
+            raise ValueError('Unknown attention activation function: {}'.format(self.attention_activation))
+
+        if inverse_mask is not None:
+            attn_weights = attn_weights * inverse_mask.unsqueeze(2)
+
+        if attn_mask is not None:
+            attn_weights = attn_weights * attn_mask
+
+        return attn_weights
+
+    def softmax_attention(self, q, k, padding_mask, attn_mask, before_attn_fn):
+        slen = k.size(2)
+        # C x C
+        bias = self.rel_pos_bias(slen)
+        if slen != q.size(2):
+            assert q.size(2) == 1
+            # 1 x C
+            bias = bias[-1:]
+
+        # scaled attention
+        q = q * self.scaling
+        # B x K x C x C
+        qk = torch.matmul(q, k.transpose(2, 3)) + bias
+
+        if attn_mask is not None:
+            qk = qk + attn_mask
+
+        if padding_mask is not None:
+            padding_mask_all = padding_mask.all(dim=-1, keepdim=True)
+            padding_mask = torch.logical_and(padding_mask, ~padding_mask_all)
+            qk = qk.masked_fill(padding_mask.unsqueeze(2).to(torch.bool), float('-inf'))
+
+        if before_attn_fn:
+            return qk
+
+        attn_weights = utils.softmax(qk, dim=-1, onnx_trace=self.onnx_trace).type_as(qk)
+        return attn_weights
+
+    def forward(
+        self,
+        x,
+        padding_mask: Optional[Tensor] = None,
+        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
+        need_weights: bool = False,
+        attn_mask: Optional[Tensor] = None,
+        before_attn_fn: bool = False,
+    ) -> Tuple[Tensor, Optional[Tensor]]:
+        """Input shape: Time x Batch x Channel
+
+        Args:
+            padding_mask (ByteTensor, optional): mask to exclude
+                keys that are pads, of shape `(batch, src_len)`, where
+                padding elements are indicated by 1s.
+            need_weights (bool, optional): return the attention weights,
+                averaged over heads (default: False).
+            attn_mask (ByteTensor, optional): typically used to
+                implement causal attention, where the mask prevents the
+                attention from looking forward in time (default: None).
+            before_attn_fn (bool, optional): return the raw attention
+                weights and values before the attention softmax.
+        """
+
+        seq_len, bsz, embed_dim = x.size()
+        assert embed_dim == self.embed_dim
+
+        if incremental_state is not None:
+            saved_state = self._get_input_buffer(incremental_state)
+        else:
+            saved_state = None
+
+        residual = x
+        if self.prenorm:
+            x = self.norm(x)
+
+        # L x B x E
+        v = self.activation(self.v_proj(x))
+
+        # L x B x D
+        mx = self.move(x, padding_mask, incremental_state)
+        mx = self.dropout(mx)
+
+        # L x B x D -> L x B x (2*D+S+E)
+        base = self.mx_proj(mx)
+        u, zr, hx = torch.split(base, [self.embed_dim, self.zdim + self.hdim, self.embed_dim], dim=-1)
+        # L x B x D
+        u = torch.sigmoid(u)
+        # L x B x (E+S)
+        z, r = torch.split(F.silu(zr), [self.zdim, self.hdim], dim=-1)
+        # L x B x S -> L x B x 1 x S -> L x B x 2 x S
+        z = z.unsqueeze(2) * self.gamma + self.beta
+        # L x B x 2 x S -> L x B x S
+        q, k = torch.unbind(z, dim=2)
+
+        # L x B x D -> B x L x D
+        q = q.transpose(0, 1)
+        k = k.transpose(0, 1)
+        v = v.transpose(0, 1)
+
+        if saved_state is not None:
+            # assert self.chunk_size < 0 or q.size(1) <= self.chunk_size
+            # saved states are stored with shape (bsz, seq_len, dim)
+            if "prev_key" in saved_state:
+                prev_key = saved_state["prev_key"]
+                assert prev_key is not None
+                assert k is not None
+                k = torch.cat([prev_key, k], dim=1)
+            if "prev_value" in saved_state:
+                prev_value = saved_state["prev_value"]
+                assert prev_value is not None
+                assert v is not None
+                v = torch.cat([prev_value, v], dim=1)
+            prev_padding_mask: Optional[Tensor] = None
+            if "prev_padding_mask" in saved_state:
+                prev_padding_mask = saved_state["prev_padding_mask"]
+            padding_mask = MovingAverageGatedAttention._append_prev_padding_mask(
+                padding_mask=padding_mask,
+                prev_padding_mask=prev_padding_mask,
+                batch_size=bsz,
+                seq_len=k.size(1),
+            )
+
+            if self.chunk_size < 0:
+                saved_state["prev_key"] = k
+                saved_state["prev_value"] = v
+                saved_state["prev_key_padding_mask"] = padding_mask
+            else:
+                curr_len = k.size(1) % self.chunk_size
+                if curr_len == 0:
+                    if "prev_key" in saved_state:
+                        del saved_state["prev_key"]
+                        del saved_state["prev_value"]
+                        del saved_state["prev_key_padding_mask"]
+                else:
+                    saved_state["prev_key"] = k
+                    saved_state["prev_value"] = v
+                    saved_state["prev_key_padding_mask"] = padding_mask
+            # In this branch incremental_state is never None
+            assert incremental_state is not None
+            self._set_input_buffer(incremental_state, saved_state)
+
+        ctx_len = k.size(1)
+        if self.chunk_size < 0:
+            # B x L x S -> B x 1 x L x S
+            q = q.unsqueeze(1)
+            k = k.unsqueeze(1)
+            v = v.unsqueeze(1)
+            if padding_mask is not None:
+                # B x L -> B x 1 x L
+                padding_mask = padding_mask.unsqueeze(1)
+        else:
+            if seq_len < self.chunk_size:
+                q = q.unsqueeze(1)
+            else:
+                # B x L x S -> B x K x C x S
+                nc = seq_len // self.chunk_size
+                q = q.reshape(bsz, nc, self.chunk_size, self.zdim)
+
+            if ctx_len < self.chunk_size:
+                k = k.unsqueeze(1)
+                v = v.unsqueeze(1)
+                if padding_mask is not None:
+                    padding_mask = padding_mask.unsqueeze(1)
+            else:
+                # B x L x S -> B x K x C x S
+                nc = ctx_len // self.chunk_size
+                k = k.reshape(bsz, nc, self.chunk_size, self.zdim)
+                v = v.reshape(bsz, nc, self.chunk_size, self.hdim)
+                if padding_mask is not None:
+                    # B x L -> B x K x C
+                    padding_mask = padding_mask.view(bsz, nc, self.chunk_size)
+
+        # This is part of a workaround to get around fork/join parallelism
+        # not supporting Optional types.
+        if padding_mask is not None and padding_mask.dim() == 0:
+            padding_mask = None
+
+        if self.attention_activation == 'softmax':
+            attn_weights = self.softmax_attention(q, k, padding_mask, attn_mask, before_attn_fn)
+        else:
+            attn_weights = self.element_attention(q, k, padding_mask, attn_mask, before_attn_fn)
+
+        if before_attn_fn:
+            return attn_weights, v
+
+        v = self.hidden_dropout(v, batch_first=True)
+        kernel = self.attention_dropout(attn_weights)
+        # B x K x C x E -> B x L x E -> L x B x E
+        h = torch.matmul(kernel, v).view(bsz, seq_len, self.hdim).transpose(0, 1)
+        # L x B x E -> L x B x D
+        h = self.activation(hx + self.h_proj(h * r))
+        h = self.dropout(h)
+        # L x B x D
+        out = torch.addcmul(residual, u, h - residual)
+
+        if not self.prenorm:
+            out = self.norm(out)
+
+        if need_weights:
+            return out, attn_weights
+        else:
+            return out, None
+
+    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]) -> Dict[str, Optional[Tensor]]:
+        result = self.get_incremental_state(incremental_state, "attn_state")
+        if result is not None:
+            return result
+        else:
+            empty_result: Dict[str, Optional[Tensor]] = {}
+            return empty_result
+
+    def _set_input_buffer(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], buffer: Dict[str, Optional[Tensor]]):
+        return self.set_incremental_state(incremental_state, "attn_state", buffer)
+
+    @torch.jit.export
+    def reorder_incremental_state(
+            self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order: Tensor
+    ):
+        """Reorder buffered internal state (for incremental generation)."""
+        input_buffer = self._get_input_buffer(incremental_state)
+        if input_buffer is not None:
+            for k in input_buffer.keys():
+                input_buffer_k = input_buffer[k]
+                if input_buffer_k is not None:
+                    input_buffer[k] = input_buffer_k.index_select(0, new_order)
+            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
+        return incremental_state
+
+    @staticmethod
+    def _append_prev_padding_mask(
+        padding_mask: Optional[Tensor],
+        prev_padding_mask: Optional[Tensor],
+        batch_size: int,
+        seq_len: int,
+    ) -> Optional[Tensor]:
+        # saved key padding masks have shape (bsz, seq_len)
+        if prev_padding_mask is not None and padding_mask is not None:
+            new_padding_mask = torch.cat([prev_padding_mask, padding_mask], dim=1)
+        # During incremental decoding, as the padding token enters and
+        # leaves the frame, there will be a time when prev or current
+        # is None
+        elif prev_padding_mask is not None:
+            filler = torch.zeros((batch_size, seq_len - prev_padding_mask.size(1)), device=prev_padding_mask.device)
+            new_padding_mask = torch.cat([prev_padding_mask, filler.bool()], dim=1)
+        elif padding_mask is not None:
+            filler = torch.zeros((batch_size, seq_len - padding_mask.size(1)), device=padding_mask.device)
+            new_padding_mask = torch.cat([filler.bool(), padding_mask], dim=1)
+        else:
+            new_padding_mask = prev_padding_mask
+        return new_padding_mask
+
+    def extra_repr(self) -> str:
+        return 'edim={}, zdim={}, hdim={}, ndim={}, chunk={}, attn_act={}, prenorm={}'.format(self.embed_dim, self.zdim,
+                                                                                  self.hdim, self.ndim, self.chunk_size,
+                                                                                  self.attention_activation, self.prenorm)
\ No newline at end of file
diff --git a/fairseq/modules/multihead_attention.py b/fairseq/modules/multihead_attention.py
index 262132df..dfcb0a6b 100644
--- a/fairseq/modules/multihead_attention.py
+++ b/fairseq/modules/multihead_attention.py
@@ -11,6 +11,8 @@ import torch.nn.functional as F
 from torch import Tensor, nn
 from torch.nn import Parameter
 
+from aihwkit.nn import AnalogLinear, AnalogLinearMapped
+
 try:
     from xformers.components.attention import build_attention
     from xformers.components.attention.utils import maybe_merge_masks
@@ -38,15 +40,18 @@ def _mask_for_xformers(mask: Tensor, to_dtype: Optional[torch.dtype] = None):
     """
     float_types = [torch.float, torch.float16]
     # If an input mask is a float it is an additive mask. Otherwise it is either uint8 or bool.
-    additive = mask.dtype in float_types
+
+    # !!! ASSUME BOOL MASK for static tracing
+
+    # additive = mask.dtype in float_types
     # If to_dype is not specified, keep same dtype as mask.
     to_dtype = mask.dtype if to_dtype is None else to_dtype
     to_additive = to_dtype in float_types
 
-    if additive:
-        if to_additive:
-            return mask.to(to_dtype)
-        mask = mask < 0
+    # if additive:
+    #     if to_additive:
+    #         return mask.to(to_dtype)
+    #     mask = mask < 0
 
     if to_additive:
         # return additive mask
@@ -90,6 +95,7 @@ class MultiheadAttention(FairseqIncrementalDecoder):
         xformers_blocksparse_blocksize: Optional[
             int
         ] = 16,  # This should be part of the config
+        traceable=False,
     ):
         super().__init__(dictionary)
 
@@ -101,6 +107,7 @@ class MultiheadAttention(FairseqIncrementalDecoder):
         self.kdim = kdim if kdim is not None else embed_dim
         self.vdim = vdim if vdim is not None else embed_dim
         self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim
+        self.traceable = traceable
 
         self.num_heads = num_heads
         self.dropout_module = FairseqDropout(
@@ -121,15 +128,16 @@ class MultiheadAttention(FairseqIncrementalDecoder):
         )
 
         self.k_proj = quant_noise(
-            nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size
+            nn.Linear(self.kdim, embed_dim, bias=False), q_noise, qn_block_size
         )
         self.v_proj = quant_noise(
-            nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size
+            nn.Linear(self.vdim, embed_dim, bias=False), q_noise, qn_block_size
         )
         self.q_proj = quant_noise(
-            nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size
+            nn.Linear(embed_dim, embed_dim, bias=False), q_noise, qn_block_size
         )
 
+        # out_proj can have a bias
         self.out_proj = quant_noise(
             nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size
         )
@@ -380,7 +388,7 @@ class MultiheadAttention(FairseqIncrementalDecoder):
 
         tgt_len, bsz, embed_dim = query.size()
 
-        if key_padding_mask is not None:
+        if not self.traceable and key_padding_mask is not None:
             assert key_padding_mask.size(0) == bsz
             assert key_padding_mask.size(1) == tgt_len
 
@@ -457,7 +465,7 @@ class MultiheadAttention(FairseqIncrementalDecoder):
             .flatten(start_dim=2, end_dim=3)
             .transpose(0, 1)
         )
-        assert list(y.size()) == [tgt_len, bsz, embed_dim]
+        # assert list(y.size()) == [tgt_len, bsz, embed_dim]
 
         # Dropout not needed because already applied in attention.
         # It is applied to the attention weights before matmul with v.
@@ -466,6 +474,213 @@ class MultiheadAttention(FairseqIncrementalDecoder):
         # TODO: support returning attention weights if needed.
         return y, None
 
+    def analog_multi_head_attention_forward(
+        self,
+        query: Tensor,
+        key: Tensor,
+        value: Tensor,
+        embed_dim_to_check: int,
+        num_heads: int,
+        bias_k: Optional[Tensor],
+        bias_v: Optional[Tensor],
+        add_zero_attn: bool,
+        dropout_p: float,
+        training: bool = True,
+        key_padding_mask: Optional[Tensor] = None,
+        need_weights: Optional[bool] = True,
+        attn_mask: Optional[Tensor] = None,
+        is_causal: Optional[bool] = False,
+        max_len: int = None,
+        max_encoder_len: int = None,
+        bsz: int = None
+    ) -> Tuple[Tensor, Optional[Tensor]]:
+        
+        is_batched = self.traceable or F._mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)
+
+        # For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input
+        # is batched, run the computation and before returning squeeze the
+        # batch dimension so that the output doesn't carry this temporary batch dimension.
+        if not is_batched:
+            # unsqueeze if the input is unbatched
+            query = query.unsqueeze(1)
+            key = key.unsqueeze(1)
+            value = value.unsqueeze(1)
+            if key_padding_mask is not None:
+                key_padding_mask = key_padding_mask.unsqueeze(0)
+
+        if self.traceable:
+            tgt_len = src_len = max_len
+            # bsz = ... we use the value passed to the function
+            embed_dim = 252
+        else:
+            # set up shape vars
+            tgt_len, bsz, embed_dim = query.shape
+            src_len, _, _ = key.shape
+
+        if self.traceable:
+            key_padding_mask = (
+                torch.zeros_like(key_padding_mask, dtype=torch.float32)
+                .masked_fill_(key_padding_mask, float("-inf"))
+            )
+        else:
+            key_padding_mask = F._canonical_mask(
+                mask=key_padding_mask,
+                mask_name="key_padding_mask",
+                other_type=F._none_or_dtype(attn_mask),
+                other_name="attn_mask",
+                target_type=query.dtype
+            )
+
+
+        if is_causal:
+            attn_mask = None
+
+        assert self.traceable or embed_dim == embed_dim_to_check, \
+            f"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}"
+        if isinstance(embed_dim, torch.Tensor):
+            # embed_dim can be a tensor when JIT tracing
+            head_dim = embed_dim.div(num_heads, rounding_mode='trunc')
+        else:
+            head_dim = embed_dim // num_heads
+        assert self.traceable or head_dim * num_heads == embed_dim, f"embed_dim {embed_dim} not divisible by num_heads {num_heads}"
+        assert self.traceable or key.shape == value.shape, f"key shape {key.shape} does not match value shape {value.shape}"
+
+        #
+        # compute in-projection
+        #
+        q = self.q_proj(query)
+        k = self.k_proj(key)
+        v = self.v_proj(value)
+
+        # prep attention mask
+
+        if not self.traceable:
+            attn_mask = F._canonical_mask(
+                mask=attn_mask,
+                mask_name="attn_mask",
+                other_type=F._none_or_dtype(key_padding_mask),
+                other_name="key_padding_mask",
+                target_type=q.dtype,
+                check_other=False,
+            )
+
+        if not self.traceable and attn_mask is not None:
+            # ensure attn_mask's dim is 3
+            if attn_mask.dim() == 2:
+                correct_2d_size = (tgt_len, src_len)
+                if attn_mask.shape != correct_2d_size:
+                    raise RuntimeError(f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.")
+                attn_mask = attn_mask.unsqueeze(0)
+            elif attn_mask.dim() == 3:
+                correct_3d_size = (bsz * num_heads, tgt_len, src_len)
+                if attn_mask.shape != correct_3d_size:
+                    raise RuntimeError(f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.")
+            else:
+                raise RuntimeError(f"attn_mask's dimension {attn_mask.dim()} is not supported")
+
+        # add bias along batch dimension (currently second)
+        if bias_k is not None and bias_v is not None:
+            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
+            v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
+            if attn_mask is not None:
+                attn_mask = F.pad(attn_mask, (0, 1))
+            if key_padding_mask is not None:
+                key_padding_mask = F.pad(key_padding_mask, (0, 1))
+        else:
+            assert bias_k is None
+            assert bias_v is None
+
+        #
+        # reshape q, k, v for multihead attention and make em batch first
+        #
+        q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
+        k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
+        v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
+
+        # add zero attention along batch dimension (now first)
+        if add_zero_attn:
+            zero_attn_shape = (bsz * num_heads, 1, head_dim)
+            k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)
+            v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)
+            if attn_mask is not None:
+                attn_mask = F.pad(attn_mask, (0, 1))
+            if key_padding_mask is not None:
+                key_padding_mask = F.pad(key_padding_mask, (0, 1))
+
+        # update source sequence length after adjustments
+        if self.traceable:
+            src_len = max_encoder_len
+        else:
+            src_len = k.size(1)
+
+        # merge key padding and attention masks
+        if key_padding_mask is not None:
+            assert self.traceable or key_padding_mask.shape == (bsz, src_len), \
+                f"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}"
+            key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \
+                expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
+            if attn_mask is None:
+                attn_mask = key_padding_mask
+            else:
+                attn_mask = attn_mask + key_padding_mask
+
+        # adjust dropout probability
+        if not training:
+            dropout_p = 0.0
+
+        #
+        # (deep breath) calculate attention and out projection
+        #
+
+        if need_weights:
+            B, Nt, E = q.shape
+            q_scaled = q / math.sqrt(E)
+            if attn_mask is not None:
+                attn_output_weights = torch.baddbmm(attn_mask, q_scaled, k.transpose(-2, -1))
+            else:
+                attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))
+            attn_output_weights = F.softmax(attn_output_weights, dim=-1)
+            if dropout_p > 0.0:
+                attn_output_weights = F.dropout(attn_output_weights, p=dropout_p)
+
+            attn_output = torch.bmm(attn_output_weights, v)
+
+            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)
+            attn_output = self.out_proj(attn_output)
+            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
+
+            # optionally average attention weights over heads
+            attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)
+
+            if not is_batched:
+                # squeeze the output if input was unbatched
+                attn_output = attn_output.squeeze(1)
+                attn_output_weights = attn_output_weights.squeeze(0)
+            return attn_output, attn_output_weights
+        else:
+            # attn_mask can be either (L,S) or (N*num_heads, L, S)
+            # if attn_mask's shape is (1, L, S) we need to unsqueeze to (1, 1, L, S)
+            # in order to match the input for SDPA of (N, num_heads, L, S)
+            if attn_mask is not None:
+                if not self.traceable and attn_mask.size(0) == 1 and attn_mask.dim() == 3:
+                    attn_mask = attn_mask.unsqueeze(0)
+                else:
+                    attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
+
+            q = q.view(bsz, num_heads, tgt_len, head_dim)
+            k = k.view(bsz, num_heads, src_len, head_dim)
+            v = v.view(bsz, num_heads, src_len, head_dim)
+
+            attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
+            attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(bsz * tgt_len, embed_dim)
+
+            attn_output = self.out_proj(attn_output)
+            attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))
+            if not is_batched:
+                # squeeze the output if input was unbatched
+                attn_output = attn_output.squeeze(1)
+            return attn_output, None
+
     def forward(
         self,
         query: Tensor,
@@ -478,6 +693,9 @@ class MultiheadAttention(FairseqIncrementalDecoder):
         attn_mask: Optional[Tensor] = None,
         before_softmax: bool = False,
         need_head_weights: bool = False,
+        max_len: int = None,
+        max_encoder_len: int = None,
+        bsz: int = None
     ) -> Tuple[Tensor, Optional[Tensor]]:
         """Input shape: Time x Batch x Channel
 
@@ -505,17 +723,18 @@ class MultiheadAttention(FairseqIncrementalDecoder):
         src_len = tgt_len
         if not self.skip_embed_dim_check:
             assert (
-                embed_dim == self.embed_dim
+                self.traceable or embed_dim == self.embed_dim
             ), f"query dim {embed_dim} != {self.embed_dim}"
-        assert list(query.size()) == [tgt_len, bsz, embed_dim]
+        assert self.traceable or list(query.size()) == [tgt_len, bsz, embed_dim]
         if key is not None:
             src_len, key_bsz, _ = key.size()
             if not torch.jit.is_scripting():
                 assert value is not None
-                assert src_len, key_bsz == value.shape[:2]
+                assert self.traceable or src_len, key_bsz == value.shape[:2]
 
         if (
-            not self.onnx_trace
+            self.traceable or
+            (not self.onnx_trace
             and not is_tpu  # don't use PyTorch version on TPUs
             and incremental_state is None
             and not static_kv
@@ -526,7 +745,7 @@ class MultiheadAttention(FairseqIncrementalDecoder):
             # for input embedding dimention and K,Q,V projection dimension.
             # Since pruning will break the dimension check and it is not easy to modify the pytorch API,
             # it is preferred to bypass the pytorch MHA when we need to skip embed_dim_check
-            and not self.skip_embed_dim_check
+            and not self.skip_embed_dim_check)
         ):
             assert key is not None and value is not None
 
@@ -536,29 +755,49 @@ class MultiheadAttention(FairseqIncrementalDecoder):
                 )
 
             else:
-                return F.multi_head_attention_forward(
-                    query,
-                    key,
-                    value,
-                    self.embed_dim,
-                    self.num_heads,
-                    torch.empty([0]),
-                    torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),
-                    self.bias_k,
-                    self.bias_v,
-                    self.add_zero_attn,
-                    self.dropout_module.p,
-                    self.out_proj.weight,
-                    self.out_proj.bias,
-                    self.training or self.dropout_module.apply_during_inference,
-                    key_padding_mask.bool() if key_padding_mask is not None else None,
-                    need_weights,
-                    attn_mask,
-                    use_separate_proj_weight=True,
-                    q_proj_weight=self.q_proj.weight,
-                    k_proj_weight=self.k_proj.weight,
-                    v_proj_weight=self.v_proj.weight,
-                )
+                if True or self.traceable:
+                    return self.analog_multi_head_attention_forward(
+                        query=query,
+                        key=key,
+                        value=value,
+                        embed_dim_to_check=self.embed_dim,
+                        num_heads=self.num_heads,
+                        bias_k=self.bias_k,
+                        bias_v=self.bias_v,
+                        add_zero_attn=self.add_zero_attn,
+                        dropout_p=self.dropout_module.p,
+                        training=self.training or self.dropout_module.apply_during_inference,
+                        key_padding_mask=key_padding_mask.bool() if key_padding_mask is not None else None,
+                        need_weights=need_weights,
+                        attn_mask=attn_mask,
+                        max_len=max_len,
+                        max_encoder_len=max_encoder_len,
+                        bsz=bsz
+                    )
+                else:
+                    return F.multi_head_attention_forward(
+                        query,
+                        key,
+                        value,
+                        self.embed_dim,
+                        self.num_heads,
+                        torch.empty([0]),
+                        torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),
+                        self.bias_k,
+                        self.bias_v,
+                        self.add_zero_attn,
+                        self.dropout_module.p,
+                        self.out_proj.weight,
+                        self.out_proj.bias,
+                        self.training or self.dropout_module.apply_during_inference,
+                        key_padding_mask.bool() if key_padding_mask is not None else None,
+                        need_weights,
+                        attn_mask,
+                        use_separate_proj_weight=True,
+                        q_proj_weight=self.q_proj.weight,
+                        k_proj_weight=self.k_proj.weight,
+                        v_proj_weight=self.v_proj.weight,
+                    )
 
         if incremental_state is not None:
             saved_state = self._get_input_buffer(incremental_state)
diff --git a/fairseq/modules/normalized_feedforward_network.py b/fairseq/modules/normalized_feedforward_network.py
new file mode 100644
index 00000000..19164876
--- /dev/null
+++ b/fairseq/modules/normalized_feedforward_network.py
@@ -0,0 +1,75 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn.functional as F
+from torch import nn
+
+from fairseq import utils
+from fairseq.modules.fairseq_dropout import FairseqDropout, FairseqFeatureDropout
+from fairseq.modules.sequence_norm import SequenceNorm
+
+
+class NormalizedFeedForwardNetwork(nn.Module):
+    def __init__(
+        self,
+        embed_dim,
+        ffn_hidden_dim,
+        dropout=0.0,
+        hidden_dropout=0.0,
+        activation='silu',
+        norm_type='layernorm',
+        prenorm=True,
+        norm_affine=True,
+        feature_dropout=False,
+        export=False,
+    ):
+        super().__init__()
+
+        self.embedding_dim = embed_dim
+        self.hidden_dim = ffn_hidden_dim
+        self.act_fn = activation
+        self.activation = utils.get_activation_fn(activation)
+
+        dropout_module = FairseqFeatureDropout if feature_dropout else FairseqDropout
+        self.dropout = dropout_module(dropout, module_name=self.__class__.__name__)
+        self.hidden_dropout = dropout_module(hidden_dropout, module_name=self.__class__.__name__)
+
+        self.prenorm = prenorm
+        self.norm = SequenceNorm(norm_type, embed_dim, affine=norm_affine, export=export)
+
+        self.fc1 = nn.Linear(embed_dim, ffn_hidden_dim)
+        self.fc2 = nn.Linear(ffn_hidden_dim, embed_dim)
+
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        std = 0.02
+        nn.init.normal_(self.fc1.weight, mean=0.0, std=std)
+        nn.init.constant_(self.fc1.bias, 0.0)
+
+        nn.init.normal_(self.fc2.weight, mean=0.0, std=std)
+        nn.init.constant_(self.fc2.bias, 0.0)
+
+    def forward(self, x):
+        residual = x
+
+        if self.prenorm:
+            x = self.norm(x)
+
+        x = self.activation(self.fc1(x))
+        x = self.hidden_dropout(x)
+        x = self.fc2(x)
+        x = self.dropout(x)
+        x = x + residual
+
+        if not self.prenorm:
+            x = self.norm(x)
+
+        return x
+
+    def extra_repr(self) -> str:
+        return 'edim={}, hdim={}, act={}, prenorm={}'.format(self.embedding_dim, self.hidden_dim, self.act_fn, self.prenorm)
\ No newline at end of file
diff --git a/fairseq/modules/real_number_embedding.py b/fairseq/modules/real_number_embedding.py
new file mode 100644
index 00000000..74f3a82c
--- /dev/null
+++ b/fairseq/modules/real_number_embedding.py
@@ -0,0 +1,27 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from torch import nn
+from torch.nn import Parameter
+
+
+class RealNumberEmbedding(nn.Module):
+    def __init__(self, embedding_dim):
+        super().__init__()
+        self.embedding_dim = embedding_dim
+        self.weight = Parameter(torch.Tensor(embedding_dim))
+        self.bias = Parameter(torch.Tensor(embedding_dim))
+
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        nn.init.normal_(self.weight, mean=0.0, std=self.embedding_dim ** -0.5)
+        nn.init.normal_(self.bias, mean=0.0, std=0.1)
+
+    def forward(self, x):
+        emb = x.unsqueeze(-1) * self.weight + self.bias
+        return emb
\ No newline at end of file
diff --git a/fairseq/modules/relative_positional_bias.py b/fairseq/modules/relative_positional_bias.py
new file mode 100644
index 00000000..c8c9d205
--- /dev/null
+++ b/fairseq/modules/relative_positional_bias.py
@@ -0,0 +1,94 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+
+import math
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class SimpleRelativePositionalBias(nn.Module):
+
+    def __init__(self, max_positions):
+        super().__init__()
+        self.max_positions = max_positions
+        self.rel_pos_bias = nn.Parameter(torch.Tensor(2 * max_positions - 1))
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        std = 0.02
+        nn.init.normal_(self.rel_pos_bias, mean=0.0, std=std)
+
+    def forward(self, seq_len):
+        if seq_len > self.max_positions:
+            raise ValueError('Sequence length {} going beyond max length {}'.format(seq_len, self.max_positions))
+
+        # seq_len * 2 -1
+        b = self.rel_pos_bias[(self.max_positions - seq_len):(self.max_positions + seq_len - 1)]
+        # seq_len * 3 - 1
+        t = F.pad(b, (0, seq_len))
+        # (seq_len * 3 - 1) * seq_len
+        t = torch.tile(t, (seq_len,))
+        t = t[:-seq_len]
+        # seq_len x (3 * seq_len - 2)
+        t = t.view(seq_len, 3 * seq_len - 2)
+        r = (2 * seq_len - 1) // 2
+        start = r
+        end = t.size(1) - r
+        t = t[:, start:end]
+        return t
+
+    def extra_repr(self) -> str:
+        return 'max positions={}'.format(self.max_positions)
+
+
+class RotaryRelativePositionalBias(nn.Module):
+    def __init__(self, embed_dim, max_positions):
+        super().__init__()
+        assert embed_dim % 2 == 0
+        self.embed_dim = embed_dim
+        self.max_positions = max_positions
+        self.sine, self.cosine = RotaryRelativePositionalBias.get_sinusoid_embeddings(max_positions, embed_dim)
+        self.alpha = nn.Parameter(torch.Tensor(1, embed_dim))
+        self.beta = nn.Parameter(torch.Tensor(1, embed_dim))
+        self.register_buffer("_float_tensor", torch.FloatTensor(1))
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        std = 0.02
+        nn.init.normal_(self.alpha, mean=0.0, std=std)
+        nn.init.normal_(self.beta, mean=0.0, std=std)
+
+    @staticmethod
+    def get_sinusoid_embeddings(max_positions: int, embedding_dim: int):
+        half_dim = embedding_dim // 2
+        emb = math.log(10000) / half_dim
+        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
+        emb = torch.arange(max_positions, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
+        return torch.sin(emb), torch.cos(emb)
+
+    def rotary(self, x):
+        n, d = x.size()
+        x1, x2 = torch.chunk(x, 2, dim=-1)
+        if self.sine is None or n > self.sine.size(0):
+            self.sine, self.cosine = RotaryRelativePositionalBias.get_sinusoid_embeddings(n, d)
+            self.max_positions = n
+        self.sine = self.sine.to(self._float_tensor)
+        self.cosine = self.cosine.to(self._float_tensor)
+
+        sin = self.sine[:n]
+        cos = self.cosine[:n]
+        return torch.cat([x1 * cos - x2 * sin, x2 * cos + x1 * sin], dim=1)
+
+    def forward(self, seq_len):
+        a = self.rotary(self.alpha.expand(seq_len, self.embed_dim))
+        b = self.rotary(self.beta.expand(seq_len, self.embed_dim))
+        t = torch.einsum('mk,nk->mn', a, b)
+        return t
+
+    def extra_repr(self) -> str:
+        return 'dim={}, max positions={}'.format(self.embed_dim, self.max_positions)
\ No newline at end of file
diff --git a/fairseq/modules/root_mean_square_norm.py b/fairseq/modules/root_mean_square_norm.py
new file mode 100644
index 00000000..e6ee0b7c
--- /dev/null
+++ b/fairseq/modules/root_mean_square_norm.py
@@ -0,0 +1,37 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn as nn
+
+
+class RMSNorm(nn.Module):
+    def __init__(self, number_features, eps=1e-6, affine=True):
+        super().__init__()
+        self.num_features = number_features
+        self.eps = eps
+        self.affine = affine
+        if affine:
+            self.weight = nn.Parameter(torch.Tensor(self.num_features))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        if self.affine:
+            nn.init.constant_(self.weight, 1.0)
+
+    def forward(self, x):
+        mean_square = torch.mean(torch.square(x), dim=-1, keepdim=True)
+        if self.weight is not None:
+            x = x * self.weight
+
+        x = x * torch.rsqrt(mean_square + self.eps)
+        return x
+
+    def extra_repr(self) -> str:
+        return '{num_features}, eps={eps}, affine={affine}'.format(**self.__dict__)
\ No newline at end of file
diff --git a/fairseq/modules/scale_norm.py b/fairseq/modules/scale_norm.py
new file mode 100644
index 00000000..1444e697
--- /dev/null
+++ b/fairseq/modules/scale_norm.py
@@ -0,0 +1,37 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+import torch.nn as nn
+
+
+class ScaleNorm(nn.Module):
+    def __init__(self, dim, eps=1e-6, affine=True):
+        super().__init__()
+        self.dim = dim
+        self.eps = eps
+        self.affine = affine
+        if affine:
+            self.scalar = nn.Parameter(torch.Tensor(1))
+        else:
+            self.register_parameter('scalar', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self):
+        if self.affine:
+            nn.init.constant_(self.scalar, 1.0)
+
+    def forward(self, x):
+        mean_square = torch.mean(torch.square(x), dim=self.dim, keepdim=True)
+        if self.scalar is not None:
+            x = self.scalar * x
+
+        x = x * torch.rsqrt(mean_square + self.eps)
+        return x
+
+    def extra_repr(self) -> str:
+        return 'dim={dim}, eps={eps}, affine={affine}'.format(**self.__dict__)
\ No newline at end of file
diff --git a/fairseq/modules/sequence_norm.py b/fairseq/modules/sequence_norm.py
new file mode 100644
index 00000000..3a7cf523
--- /dev/null
+++ b/fairseq/modules/sequence_norm.py
@@ -0,0 +1,40 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch.nn as nn
+
+from fairseq.modules.layer_norm import LayerNorm
+from fairseq.modules.scale_norm import ScaleNorm
+from fairseq.modules.root_mean_square_norm import RMSNorm
+
+
+class SequenceNorm(nn.Module):
+    def __init__(self, norm_type, embedding_dim, eps=1e-5, affine=True, export=False):
+        super().__init__()
+        if norm_type == 'layernorm':
+            self.norm = LayerNorm(embedding_dim, eps=eps, elementwise_affine=affine, export=export)
+        elif norm_type == 'scalenorm':
+            self.norm = ScaleNorm(dim=-1, eps=eps, affine=affine)
+        elif norm_type == 'rmsnorm':
+            self.norm = RMSNorm(embedding_dim, eps=eps, affine=affine)
+        elif norm_type == 'batchnorm':
+            self.norm = nn.BatchNorm1d(embedding_dim, eps=eps, affine=affine)
+        elif norm_type == 'syncbatchnorm':
+            self.norm = nn.SyncBatchNorm(embedding_dim, eps=eps, affine=affine)
+        else:
+            raise ValueError('Unknown norm type: {}'.format(norm_type))
+
+    def normalize(self, x):
+        if isinstance(self.norm, nn.modules.batchnorm._BatchNorm):
+            assert x.dim() == 3
+            x = x.permute(1, 2, 0)
+            x = self.norm(x)
+            return x.permute(2, 0, 1)
+        else:
+            return self.norm(x)
+
+    def forward(self, x):
+        return self.normalize(x)
\ No newline at end of file
diff --git a/fairseq/modules/sinusoidal_positional_embedding.py b/fairseq/modules/sinusoidal_positional_embedding.py
index 4793ecfb..687dde01 100644
--- a/fairseq/modules/sinusoidal_positional_embedding.py
+++ b/fairseq/modules/sinusoidal_positional_embedding.py
@@ -18,7 +18,7 @@ class SinusoidalPositionalEmbedding(nn.Module):
     Padding symbols are ignored.
     """
 
-    def __init__(self, embedding_dim, padding_idx, init_size=1024):
+    def __init__(self, embedding_dim, padding_idx, init_size=1024, traceable: bool = False):
         super().__init__()
         self.embedding_dim = embedding_dim
         self.padding_idx = padding_idx if padding_idx is not None else 0
@@ -28,6 +28,7 @@ class SinusoidalPositionalEmbedding(nn.Module):
         self.onnx_trace = False
         self.register_buffer("_float_tensor", torch.FloatTensor(1))
         self.max_positions = int(1e5)
+        self.traceable = traceable
 
     def prepare_for_onnx_export_(self):
         self.onnx_trace = True
@@ -68,7 +69,7 @@ class SinusoidalPositionalEmbedding(nn.Module):
         bspair = torch.onnx.operators.shape_as_tensor(input)
         bsz, seq_len = bspair[0], bspair[1]
         max_pos = self.padding_idx + 1 + seq_len
-        if self.weights is None or max_pos > self.weights.size(0):
+        if not self.traceable and (self.weights is None or max_pos > self.weights.size(0)):
             # recompute/expand embeddings if needed
             self.weights = SinusoidalPositionalEmbedding.get_embedding(
                 max_pos, self.embedding_dim, self.padding_idx
diff --git a/fairseq/modules/transformer_layer.py b/fairseq/modules/transformer_layer.py
index 19e035de..e1456f0e 100644
--- a/fairseq/modules/transformer_layer.py
+++ b/fairseq/modules/transformer_layer.py
@@ -165,6 +165,8 @@ class TransformerEncoderLayerBase(nn.Module):
         x,
         encoder_padding_mask: Optional[Tensor],
         attn_mask: Optional[Tensor] = None,
+        max_len: int = None,
+        bsz: int = None
     ):
         """
         Args:
@@ -201,6 +203,9 @@ class TransformerEncoderLayerBase(nn.Module):
             key_padding_mask=encoder_padding_mask,
             need_weights=False,
             attn_mask=attn_mask,
+            max_len=max_len,
+            max_encoder_len=max_len,
+            bsz=bsz
         )
         x = self.dropout_module(x)
         x = self.residual_connection(x, residual)
@@ -393,6 +398,9 @@ class TransformerDecoderLayerBase(nn.Module):
         self_attn_padding_mask: Optional[torch.Tensor] = None,
         need_attn: bool = False,
         need_head_weights: bool = False,
+        max_in_len: int = None,
+        max_out_len: int = None,
+        bsz: int = None
     ):
         """
         Args:
@@ -456,6 +464,9 @@ class TransformerDecoderLayerBase(nn.Module):
             incremental_state=incremental_state,
             need_weights=False,
             attn_mask=self_attn_mask,
+            max_len=max_out_len,
+            max_encoder_len=max_out_len,
+            bsz=bsz
         )
         if self.c_attn is not None:
             tgt_len, bsz = x.size(0), x.size(1)
@@ -493,6 +504,9 @@ class TransformerDecoderLayerBase(nn.Module):
                 static_kv=True,
                 need_weights=need_attn or (not self.training and self.need_attn),
                 need_head_weights=need_head_weights,
+                max_len=max_out_len,
+                max_encoder_len=max_in_len,
+                bsz=bsz
             )
             x = self.dropout_module(x)
             x = self.residual_connection(x, residual)
diff --git a/fairseq/modules/transformer_sentence_encoder_layer.py b/fairseq/modules/transformer_sentence_encoder_layer.py
index f869c4b2..df5646d5 100644
--- a/fairseq/modules/transformer_sentence_encoder_layer.py
+++ b/fairseq/modules/transformer_sentence_encoder_layer.py
@@ -25,13 +25,17 @@ class TransformerSentenceEncoderLayer(nn.Module):
         ffn_embedding_dim: int = 3072,
         num_attention_heads: int = 8,
         dropout: float = 0.1,
+        bias: bool = True,
         attention_dropout: float = 0.1,
         activation_dropout: float = 0.1,
         activation_fn: str = "relu",
+        norm_type: str = "layernorm",
         export: bool = False,
         q_noise: float = 0.0,
         qn_block_size: int = 8,
         init_fn: Callable = None,
+        xformers_att_config: Optional[str] = None,
+        traceable: Optional[bool] = False
     ) -> None:
         super().__init__()
 
@@ -51,6 +55,7 @@ class TransformerSentenceEncoderLayer(nn.Module):
         self.activation_dropout_module = FairseqDropout(
             activation_dropout, module_name=self.__class__.__name__
         )
+        self.traceable = traceable
 
         # Initialize blocks
         self.activation_fn = utils.get_activation_fn(activation_fn)
@@ -58,9 +63,12 @@ class TransformerSentenceEncoderLayer(nn.Module):
             self.embedding_dim,
             num_attention_heads,
             dropout=attention_dropout,
+            bias=bias,
             self_attention=True,
             q_noise=q_noise,
             qn_block_size=qn_block_size,
+            xformers_att_config=xformers_att_config,
+            traceable=self.traceable
         )
 
         # layer norm associated with the self attention layer
@@ -71,39 +79,47 @@ class TransformerSentenceEncoderLayer(nn.Module):
             ffn_embedding_dim,
             q_noise=q_noise,
             qn_block_size=qn_block_size,
+            bias=True
         )
         self.fc2 = self.build_fc2(
             ffn_embedding_dim,
             self.embedding_dim,
             q_noise=q_noise,
             qn_block_size=qn_block_size,
+            bias=True,
         )
 
         # layer norm associated with the position wise feed-forward NN
         self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)
 
-    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
-        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)
+    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size, bias):
+        return quant_noise(nn.Linear(input_dim, output_dim, bias=bias), q_noise, qn_block_size)
 
-    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
-        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)
+    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size, bias):
+        return quant_noise(nn.Linear(input_dim, output_dim, bias=bias), q_noise, qn_block_size)
 
     def build_self_attention(
         self,
         embed_dim,
         num_attention_heads,
         dropout,
+        bias,
         self_attention,
         q_noise,
         qn_block_size,
+        xformers_att_config,
+        traceable
     ):
         return MultiheadAttention(
             embed_dim,
             num_attention_heads,
             dropout=dropout,
+            bias=bias,
             self_attention=True,
             q_noise=q_noise,
             qn_block_size=qn_block_size,
+            xformers_att_config=xformers_att_config,
+            traceable=traceable
         )
 
     def forward(
@@ -116,6 +132,7 @@ class TransformerSentenceEncoderLayer(nn.Module):
         LayerNorm is applied either before or after the self-attention/ffn
         modules similar to the original Transformer implementation.
         """
+        mask = (~(x == 0.).all(-1)).unsqueeze(-1)
         residual = x
         x, attn = self.self_attn(
             query=x,
@@ -130,10 +147,10 @@ class TransformerSentenceEncoderLayer(nn.Module):
         x = self.self_attn_layer_norm(x)
 
         residual = x
-        x = self.activation_fn(self.fc1(x))
+        x = self.activation_fn(self.fc1(x * mask))
         x = self.activation_dropout_module(x)
-        x = self.fc2(x)
+        x = self.fc2(x * mask)
         x = self.dropout_module(x)
         x = residual + x
         x = self.final_layer_norm(x)
-        return x, attn
+        return x * mask, attn
diff --git a/fairseq/optim/adam.py b/fairseq/optim/adam.py
index 678ec7c6..3fcfe2c9 100644
--- a/fairseq/optim/adam.py
+++ b/fairseq/optim/adam.py
@@ -17,6 +17,7 @@ from fairseq.optim import FairseqOptimizer, register_optimizer
 from fairseq.optim.fused_adam import get_fused_adam_class
 from omegaconf import II, OmegaConf
 
+from aihwkit.optim.analog_optimizer import AnalogOptimizerMixin
 
 logger = logging.getLogger(__name__)
 
@@ -63,7 +64,7 @@ class FairseqAdam(FairseqOptimizer):
                 raise NotImplementedError("--fp16-adam-stats is only supported on GPU")
             # on TPUs we use the Adam defined here, since it
             # automatically casts gradients to FP32
-            self._optimizer = Adam(params, **self.optimizer_config)
+            self._optimizer = AnalogAdam(params, **self.optimizer_config)
         elif use_fused_adam:
             logger.info("using FusedAdam")
             self._optimizer = fused_adam_cls(
@@ -74,7 +75,7 @@ class FairseqAdam(FairseqOptimizer):
                 raise NotImplementedError(
                     "--fp16-adam-stats is only supported with FusedAdamV1"
                 )
-            self._optimizer = Adam(params, **self.optimizer_config)
+            self._optimizer = AnalogAdam(params, **self.optimizer_config)
 
     @property
     def optimizer_config(self):
@@ -237,3 +238,6 @@ class Adam(torch.optim.Optimizer):
                     p.data.copy_(p_data_fp32)
 
         return loss
+
+class AnalogAdam(AnalogOptimizerMixin, Adam):
+    """ Analog wrapper for Adam """
\ No newline at end of file
diff --git a/fairseq/optim/fused_adam.py b/fairseq/optim/fused_adam.py
index 39a2a836..bda37dbc 100644
--- a/fairseq/optim/fused_adam.py
+++ b/fairseq/optim/fused_adam.py
@@ -7,6 +7,7 @@ import types
 
 import torch
 
+from aihwkit.optim.analog_optimizer import AnalogOptimizerMixin
 
 def get_fused_adam_class():
     """
@@ -23,7 +24,7 @@ def get_fused_adam_class():
         import importlib
 
         fused_adam_cuda = importlib.import_module("fused_adam_cuda")
-        return FusedAdamV1
+        return AnalogFusedAdamV1
     except ImportError:
         try:
             # fallback to the newer interface
@@ -31,7 +32,7 @@ def get_fused_adam_class():
             from apex.optimizers import FusedAdam as _FusedAdam  # noqa
 
             if multi_tensor_applier.available:
-                return FusedAdamV2
+                return AnalogFusedAdamV2
         except ImportError:
             pass
     return None
@@ -254,6 +255,10 @@ class FusedAdamV1(torch.optim.Optimizer):
         return loss
 
 
+class AnalogFusedAdamV1(AnalogOptimizerMixin, FusedAdamV1):
+    """ Analog wrapper for FusedAdamV1 """
+
+
 try:
     from apex.multi_tensor_apply import multi_tensor_applier
     from apex.optimizers import FusedAdam
@@ -384,6 +389,9 @@ try:
                         )
 
             return loss
+    
+    class AnalogFusedAdamV2(AnalogOptimizerMixin, FusedAdamV2):
+        """ Analog wrapper for FusedAdamV2 """
 
 except ImportError:
     pass
diff --git a/fairseq/optim/lr_scheduler/linear_decay_schedule.py b/fairseq/optim/lr_scheduler/linear_decay_schedule.py
new file mode 100644
index 00000000..c3409d99
--- /dev/null
+++ b/fairseq/optim/lr_scheduler/linear_decay_schedule.py
@@ -0,0 +1,104 @@
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+# All rights reserved.
+
+# This source code is licensed under the license found in the
+# LICENSE file in the root directory of this source tree.
+
+from fairseq.dataclass.configs import FairseqDataclass
+from . import FairseqLRScheduler, register_lr_scheduler
+from dataclasses import dataclass, field
+from typing import List, Optional
+
+from omegaconf import II
+
+
+@dataclass
+class LinearDecayLRScheduleConfig(FairseqDataclass):
+    
+    warmup_updates: int = field(
+        default=0,
+        metadata={"help": "warmup the learning rate linearly for the first N updates"},
+    )
+    warmup_init_lr: float = field(
+        default=-1,
+        metadata={
+            "help": "initial learning rate during warmup phase; default is cfg.lr"
+        },
+    )
+    warmup_power: Optional[float] = field(
+        default=1,
+    )
+    end_learning_rate: Optional[float] = field(
+        default=0.0,
+    )
+    total_num_update: Optional[int] = field(
+        default=1000000
+    )
+    lr: List[float] = II("optimization.lr")
+
+
+@register_lr_scheduler('linear_decay', dataclass=LinearDecayLRScheduleConfig)
+class LinearDecaySchedule(FairseqLRScheduler):
+    """Decay the LR on a linear schedule.
+    """
+
+    def __init__(self, args, optimizer):
+        super().__init__(args, optimizer)
+        self.args = args
+        if len(args.lr) > 1:
+            raise ValueError(
+                'Cannot use a fixed learning rate schedule with step.'
+                ' Consider --lr-scheduler=fixed instead.'
+            )
+        warmup_end_lr = args.lr[0]
+        if args.warmup_updates < 0:
+            raise ValueError('warm up steps cannot be negative.')
+        elif args.warmup_updates == 0:
+            assert args.warmup_init_lr < 0
+            args.warmup_init_lr = warmup_end_lr
+        else:
+            assert args.warmup_init_lr < warmup_end_lr
+            if args.warmup_init_lr < 0:
+                args.warmup_init_lr = 0
+
+        # linearly warmup for the first args.warmup_updates
+        if args.warmup_updates > 0:
+            self.warmup_power = args.warmup_power
+            self.warmup_factor = (warmup_end_lr - args.warmup_init_lr) / (args.warmup_updates ** args.warmup_power)
+        else:
+            self.warmup_power = 1
+            self.warmup_factor = 0
+
+        self.end_learning_rate = args.end_learning_rate
+        self.total_num_update = args.total_num_update
+        self.lr_factor = (warmup_end_lr - self.end_learning_rate) / (self.total_num_update - args.warmup_updates)
+
+        # initial learning rate
+        self.lr = args.warmup_init_lr
+        self.optimizer.set_lr(self.lr)
+
+
+    def state_dict(self):
+        return {'lr': self.lr}
+
+    def load_state_dict(self, state_dict):
+        if 'lr' in state_dict:
+            self.lr = state_dict['lr']
+
+    def step(self, epoch, val_loss=None):
+        """Update the learning rate at the end of the given epoch."""
+        super().step(epoch, val_loss)
+        # we don't change the learning rate at epoch boundaries
+        return self.optimizer.get_lr()
+
+    def step_update(self, num_updates):
+        """Update the learning rate after each update."""
+        if num_updates <= self.args.warmup_updates:
+            self.lr = self.args.warmup_init_lr + (num_updates ** self.warmup_power) * self.warmup_factor
+        elif num_updates >= self.total_num_update:
+            self.lr = self.end_learning_rate
+        else:
+            self.lr = self.lr_factor * (self.total_num_update - num_updates) + self.end_learning_rate
+
+        self.optimizer.set_lr(self.lr)
+        return self.lr
\ No newline at end of file
diff --git a/fairseq/sequence_generator.py b/fairseq/sequence_generator.py
index 78db504e..ed95de05 100644
--- a/fairseq/sequence_generator.py
+++ b/fairseq/sequence_generator.py
@@ -277,7 +277,7 @@ class SequenceGenerator(nn.Module):
         ), "min_len cannot be larger than max_len, please adjust these!"
         # compute the encoder output for each beam
         with torch.autograd.profiler.record_function("EnsembleModel: forward_encoder"):
-            encoder_outs = self.model.forward_encoder(net_input)
+            encoder_outs = self.model.forward_encoder({"src_tokens": src_tokens, "src_lengths": src_lengths})
 
         # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores
         new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)
diff --git a/fairseq/tasks/__init__.py b/fairseq/tasks/__init__.py
index 6da1f001..75b319db 100644
--- a/fairseq/tasks/__init__.py
+++ b/fairseq/tasks/__init__.py
@@ -44,6 +44,8 @@ def setup_task(cfg: FairseqDataclass, **kwargs):
         task is not None
     ), f"Could not infer task type from {cfg}. Available argparse tasks: {TASK_REGISTRY.keys()}. Available hydra tasks: {TASK_DATACLASS_REGISTRY.keys()}"
 
+    if "new_data_dir" in kwargs and kwargs["new_data_dir"] is not None:
+        cfg.data = kwargs["new_data_dir"]
     return task.setup_task(cfg, **kwargs)
 
 
diff --git a/fairseq/tasks/fairseq_task.py b/fairseq/tasks/fairseq_task.py
index e39d1d68..e1303503 100644
--- a/fairseq/tasks/fairseq_task.py
+++ b/fairseq/tasks/fairseq_task.py
@@ -16,6 +16,7 @@ from fairseq.data import Dictionary, FairseqDataset, data_utils, encoders, itera
 from fairseq.dataclass import FairseqDataclass
 from fairseq.dataclass.utils import gen_parser_from_dataclass
 from fairseq.optim.amp_optimizer import AMPOptimizer
+from fairseq.utils import analog2model, convert2analog
 from omegaconf import DictConfig
 
 
@@ -354,6 +355,10 @@ class FairseqTask(object):
 
         model = models.build_model(cfg, self, from_checkpoint)
         model = quantization_utils.quantize_model_scalar(model, cfg)
+
+        # - Exchange linear layers with layers that inject noise based on the args
+        model = convert2analog(model=model, modifier_std_dev=cfg.modifier_std_dev, modifier_type=cfg.modifier_type)
+
         return model
 
     def build_criterion(self, cfg: DictConfig, from_checkpoint=False):
@@ -526,7 +531,7 @@ class FairseqTask(object):
                 - logging outputs to display while training
         """
         model.train()
-        model.set_num_updates(update_num)
+        analog2model(model).set_num_updates(update_num)
         with torch.autograd.profiler.record_function("forward"):
             with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
                 loss, sample_size, logging_output = criterion(model, sample)
diff --git a/fairseq/tasks/long_range_arena.py b/fairseq/tasks/long_range_arena.py
new file mode 100644
index 00000000..6365358a
--- /dev/null
+++ b/fairseq/tasks/long_range_arena.py
@@ -0,0 +1,353 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+import os
+from typing import Optional, Tuple
+
+import numpy as np
+from dataclasses import dataclass, field
+from omegaconf import II, MISSING, OmegaConf
+
+from fairseq import utils
+from fairseq.data import (
+    ConcatSentencesDataset,
+    data_utils,
+    Dictionary,
+    IdDataset,
+    NestedDictionaryDataset,
+    NumSamplesDataset,
+    NumelDataset,
+    OffsetTokensDataset,
+    PixelSequenceDataset,
+    PrependTokenDataset,
+    RawLabelDataset,
+    RightPadDataset,
+    RollDataset,
+    SortDataset,
+    StripTokenDataset,
+    TruncateDataset
+)
+from fairseq.data.shorten_dataset import maybe_shorten_dataset
+from fairseq.dataclass import FairseqDataclass
+from fairseq.tasks import FairseqTask, register_task
+
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class LRATextConfig(FairseqDataclass):
+    data: str = field(
+        default=MISSING,
+        metadata={
+            "help": "colon separated path to data directories list, "
+            "will be iterated upon during epochs in round-robin manner"
+        }
+    )
+    num_classes: int = field(
+        default=-1,
+        metadata={"help": "number of classes or regression targets"}
+    )
+    regression_target: bool = field(
+        default=False
+    )
+    no_shuffle: bool = field(
+        default=False
+    )
+    sen_rep_type: str = II("model.sen_rep_type")
+    max_positions: int = II("model.max_positions")
+    data: str = II("task.data")
+    dataset_impl: Optional[str] = II("dataset.dataset_impl")
+    seed: int = II("common.seed")
+
+@dataclass
+class LRAImageConfig(FairseqDataclass):
+    data: str = field(
+        default=MISSING,
+        metadata={
+            "help": "colon separated path to data directories list, "
+            "will be iterated upon during epochs in round-robin manner"
+        }
+    )
+    num_classes: int = field(
+        default=-1,
+        metadata={"help": "number of classes or regression targets"}
+    )
+    regression_target: bool = field(
+        default=False
+    )
+    no_shuffle: bool = field(
+        default=False
+    )
+    max_positions: int = II("model.max_positions")
+    pixel_normalization: Optional[str] = field(
+        default=None
+    )
+    sen_rep_type: str = II("model.sen_rep_type")
+    max_positions: int = II("model.max_positions")
+    data: str = II("task.data")
+    dataset_impl: Optional[str] = II("dataset.dataset_impl")
+    seed: int = II("common.seed")
+
+
+
+@register_task('lra-text', dataclass=LRATextConfig)
+class LRATextTask(FairseqTask):
+    """
+    Sentence (or sentence pair) prediction (classification or regression) task.
+
+    Args:
+        dictionary (Dictionary): the dictionary for the input of the task
+    """
+
+    cfg: LRATextConfig
+
+    def __init__(self, cfg: LRATextConfig, data_dictionary, label_dictionary, cls_idx):
+        super().__init__(cfg)
+        self.cls_idx = cls_idx
+        self.dictionary = data_dictionary
+        self._label_dictionary = label_dictionary
+        self.prepend_cls = cfg.sen_rep_type == 'cls'
+        if not hasattr(cfg, 'max_positions'):
+            self._max_positions = (
+                cfg.max_source_positions,
+                cfg.max_target_positions,
+            )
+        else:
+            self._max_positions = cfg.max_positions
+        # cfg.tokens_per_sample = self._max_positions
+        self.cfg = cfg
+
+    @classmethod
+    def load_dictionary(cls, filename):
+        """Load the dictionary from the filename
+
+        Args:
+            filename (str): the filename
+        """
+        dictionary = Dictionary.load(filename)
+        return dictionary
+
+    @classmethod
+    def setup_task(cls, cfg, **kwargs):
+        # load data dictionary
+        data_dict = cls.load_dictionary(os.path.join(cfg.data, 'src-bin', 'dict.txt'))
+        cls_idx = data_dict.add_symbol('<CLS>')
+        logger.info('[input] dictionary: {} types'.format(len(data_dict)))
+
+        label_dict = None
+        if not cfg.regression_target:
+            # load label dictionary
+            label_dict = cls.load_dictionary(os.path.join(cfg.data, 'label-bin', 'dict.txt'))
+            logger.info('[label] dictionary: {} types'.format(len(label_dict)))
+        else:
+            label_dict = data_dict
+        return LRATextTask(cfg, data_dict, label_dict, cls_idx)
+
+    def load_dataset(self, split, combine=False, **kwargs):
+        """Load a given dataset split (e.g., train, valid, test)."""
+        def get_path(type, split):
+            return os.path.join(self.cfg.data, type, split)
+
+        def make_dataset(type, dictionary):
+            split_path = get_path(type, split)
+
+            dataset = data_utils.load_indexed_dataset(
+                split_path,
+                dictionary,
+                self.cfg.dataset_impl,
+                combine=combine,
+            )
+            return dataset
+
+        src_ds = make_dataset('src-bin', self.source_dictionary)
+        if self.prepend_cls:
+            src_ds = PrependTokenDataset(src_ds, self.cls_idx)
+        src1_ds = make_dataset('src1-bin', self.source_dictionary)
+        if src1_ds is not None:
+            if self.prepend_cls:
+                src1_ds = PrependTokenDataset(src1_ds, self.cls_idx)
+            src1_tokens = TruncateDataset(src1_ds, self._max_positions)
+        with data_utils.numpy_seed(self.cfg.seed):
+            shuffle = np.random.permutation(len(src_ds))
+
+        src_tokens = TruncateDataset(src_ds, self._max_positions)
+        dataset = {
+            'id': IdDataset(),
+            'net_input': {
+                'src_tokens': RightPadDataset(
+                    src_tokens,
+                    pad_idx=self.source_dictionary.pad(),
+                    pad_length=self._max_positions
+                ),
+                'src_lengths': NumelDataset(src_tokens, reduce=False),
+            },
+            'nsentences': NumSamplesDataset(),
+            'ntokens': NumelDataset(src_tokens, reduce=True),
+        }
+        if src1_ds is not None:
+            dataset.update(
+                net_input1={
+                    'src_tokens': RightPadDataset(
+                        src1_tokens,
+                        pad_idx=self.source_dictionary.pad(),
+                        pad_length=self._max_positions
+                    ),
+                    'src_lengths': NumelDataset(src1_tokens, reduce=False),
+                },
+            )
+
+        label_dataset = make_dataset('label-bin', self.label_dictionary)
+        if label_dataset is not None:
+            dataset.update(
+                target=OffsetTokensDataset(
+                    StripTokenDataset(
+                        label_dataset,
+                        id_to_strip=self.label_dictionary.eos(),
+                    ),
+                    offset=-self.label_dictionary.nspecial,
+                )
+            )
+
+        nested_dataset = NestedDictionaryDataset(
+            dataset,
+            sizes=[src_tokens.sizes],
+        )
+        if self.cfg.no_shuffle:
+            dataset = nested_dataset
+        else:
+            dataset = SortDataset(
+                nested_dataset,
+                # shuffle
+                sort_order=[shuffle],
+            )
+
+        logger.info("Loaded {0} with #samples: {1}".format(split, len(dataset)))
+
+        self.datasets[split] = dataset
+        return self.datasets[split]
+
+    def build_model(self, args):
+        from fairseq import models
+        model = models.build_model(args, self)
+        return model
+
+    def max_positions(self):
+        return self._max_positions
+
+    @property
+    def source_dictionary(self):
+        return self.dictionary
+
+    @property
+    def target_dictionary(self):
+        return self.dictionary
+
+    @property
+    def label_dictionary(self):
+        return self._label_dictionary
+
+
+@register_task('lra-image', dataclass=LRAImageConfig)
+class LRAImageTask(FairseqTask):
+    """
+    Sentence (or sentence pair) prediction (classification or regression) task.
+
+    Args:
+        dictionary (Dictionary): the dictionary for the input of the task
+    """
+
+    cfg: LRATextConfig
+
+    def __init__(self, cfg):
+        super().__init__(cfg)
+        if not hasattr(cfg, 'max_positions'):
+            self._max_positions = (
+                cfg.max_source_positions,
+                cfg.max_target_positions,
+            )
+        else:
+            self._max_positions = cfg.max_positions
+        # args.tokens_per_sample = self._max_positions
+        self.normalization = (0.5, 0.5) if cfg.pixel_normalization is None else utils.eval_str_list(cfg.pixel_normalization)
+        assert len(self.normalization) == 2
+        self.cfg = cfg
+
+    @classmethod
+    def load_dictionary(cls, filename):
+        """Load the dictionary from the filename
+
+        Args:
+            filename (str): the filename
+        """
+        raise NotImplementedError
+
+    @classmethod
+    def setup_task(cls, cfg, **kwargs):
+        return LRAImageTask(cfg)
+
+    def load_dataset(self, split, combine=False, **kwargs):
+        if "new_data_dir" in kwargs and kwargs["new_data_dir"] is not None:
+            self.cfg.data = kwargs["new_data_dir"]
+
+        """Load a given dataset split (e.g., train, valid, test)."""
+        def get_path(type, split):
+            return os.path.join(self.cfg.data, type, split)
+
+        def make_dataset(type):
+            split_path = get_path(type, split)
+            dataset = PixelSequenceDataset(split_path + '.src', self.normalization)
+            return dataset
+
+        src_ds = make_dataset('input')
+        with data_utils.numpy_seed(self.cfg.seed):
+            shuffle = np.random.permutation(len(src_ds))
+
+        src_tokens = TruncateDataset(src_ds, self._max_positions)
+        dataset = {
+            'id': IdDataset(),
+            'net_input': {
+                'src_tokens': src_tokens,
+                'src_lengths': NumelDataset(src_tokens, reduce=False),
+            },
+            'nsentences': NumSamplesDataset(),
+            'ntokens': NumelDataset(src_tokens, reduce=True),
+        }
+
+        label_path = get_path('label', split) + '.label'
+        if os.path.exists(label_path):
+            label_dataset = RawLabelDataset([int(line.strip()) for i, line in enumerate(open(label_path).readlines())])
+            dataset.update(target=label_dataset)
+
+        nested_dataset = NestedDictionaryDataset(
+            dataset,
+            sizes=[src_tokens.sizes],
+        )
+        if self.cfg.no_shuffle:
+            dataset = nested_dataset
+        else:
+            dataset = SortDataset(
+                nested_dataset,
+                # shuffle
+                sort_order=[shuffle],
+            )
+
+        logger.info("Loaded {0} with #samples: {1}".format(split, len(dataset)))
+
+        self.datasets[split] = dataset
+        return self.datasets[split]
+
+    def build_model(self, cfg):
+        from fairseq import models
+        model = models.build_model(cfg, self)
+        return model
+
+    def max_positions(self):
+        return self._max_positions
+
+    @property
+    def target_dictionary(self):
+        return None
\ No newline at end of file
diff --git a/fairseq/tasks/masked_lm.py b/fairseq/tasks/masked_lm.py
index b064907a..6ca31535 100644
--- a/fairseq/tasks/masked_lm.py
+++ b/fairseq/tasks/masked_lm.py
@@ -321,7 +321,7 @@ class MaskedLMTask(FairseqTask):
         return self.dictionary
 
     def begin_epoch(self, epoch, model):
-        model.set_epoch(epoch)
+        utils.analog2model(model).set_epoch(epoch)
 
     def max_positions(self):
         return self.cfg.tokens_per_sample
diff --git a/fairseq/tasks/rxn_task.py b/fairseq/tasks/rxn_task.py
new file mode 100644
index 00000000..1458f7cc
--- /dev/null
+++ b/fairseq/tasks/rxn_task.py
@@ -0,0 +1,166 @@
+# Copyright (c) Facebook, Inc. and its affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import logging
+import os
+
+import pandas as pd
+from functools import partial, lru_cache
+
+import torch
+import numpy as np
+from dataclasses import dataclass
+from omegaconf import II
+from fairseq.models.rxn.collation import get_tokens, collate_fn
+
+from fairseq.data import FairseqDataset
+from fairseq.dataclass import FairseqDataclass
+from fairseq.tasks import FairseqTask, register_task
+from fairseq.data import Dictionary
+from tqdm import tqdm
+
+logger = logging.getLogger(__name__)
+
+def sequential_transforms(*transforms):
+    def func(txt_input):
+        for transform in transforms:
+            txt_input = transform(txt_input)
+        return txt_input
+    return func
+
+def tensor_transform(token_ids, bos_idx):
+    return torch.cat((torch.tensor([bos_idx]), 
+                      torch.tensor(token_ids)))
+
+class ReactionDataset(FairseqDataset):
+    def __init__(self, source_file, target_file, vocab):
+        self.source_data = pd.read_csv(source_file, names=['source'])
+        self.target_data = pd.read_csv(target_file, names=['target'])
+        self.vocab = vocab
+
+        self.source_num_tokens = np.array(self.source_data.apply(lambda x : len(get_tokens(x[0])), axis=1))
+        self.target_num_tokens = np.array(self.target_data.apply(lambda x : len(get_tokens(x[0])), axis=1))
+        
+        bos_prepend = partial(tensor_transform, bos_idx=vocab.bos_index)
+        text_transform = {
+            key: sequential_transforms(partial(self.vocab.encode_line, add_if_not_exist=False), bos_prepend)
+            for key in ["source", "target"]
+        }
+
+        self.collator = partial(collate_fn, text_transform=text_transform)
+    
+    def __len__(self):
+        return len(self.source_data)
+    
+    @lru_cache(maxsize=8)
+    def __getitem__(self, idx):
+        return self.source_data.iloc[idx][0], self.target_data.iloc[idx][0]
+    
+    def collater(self, samples):
+        # from fairseq.data.data_utils import collate_tokens
+        return self.collator(samples)
+
+    def num_tokens(self, index):
+        """Return the number of tokens in a sample. This value is used to
+        enforce ``--max-tokens`` during batching."""
+        return max(
+            self.source_num_tokens[index],
+            self.target_num_tokens[index] if self.target_num_tokens is not None else 0
+        )
+
+    def num_tokens_vec(self, indices):
+        """Return the number of tokens for a set of positions defined by indices.
+        This value is used to enforce ``--max-tokens`` during batching."""
+        sizes = self.source_num_tokens[indices]
+        if self.target_num_tokens is not None:
+            sizes = np.maximum(sizes, self.target_num_tokens[indices])
+        return sizes
+    
+    def size(self, index):
+        """Return an example's size as a float or tuple. This value is used when
+        filtering a dataset with ``--max-positions``."""
+        return (
+            self.source_num_tokens[index],
+            self.target_num_tokens[index] if self.target_num_tokens is not None else 0
+        )
+
+
+def build_vocab(data_path: str):
+    vocab_path = os.path.join(os.path.expanduser(data_path), "vocab.pt")
+    special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']
+    shared_vocab = Dictionary(
+        unk=special_symbols[0],
+        pad=special_symbols[1],
+        bos=special_symbols[2],
+        eos=special_symbols[3]
+    )
+    
+    if os.path.exists(vocab_path):
+        with open(vocab_path, "r") as f:
+            shared_vocab = shared_vocab.load(f)
+    else:
+        source_data = os.path.join(data_path, "src-train.txt")
+        target_data = os.path.join(data_path, "tgt-train.txt")
+
+        dfs = []
+        for file in [source_data, target_data]:
+            df = pd.read_csv(file, names=['smiles'])
+            dfs.append(df)
+
+        df = pd.concat(dfs)
+
+        with tqdm(total=len(df)) as pbar:
+            for _, row in df.iterrows():
+                shared_vocab.encode_line(row['smiles'].strip())
+                pbar.update(1)
+
+        shared_vocab.finalize()
+
+        with open(vocab_path, "w") as f:
+            shared_vocab.save(f)
+
+    return shared_vocab
+
+
+@dataclass
+class RXNConfig(FairseqDataclass):
+    data: str = II("task.data")
+    seed: int = II("common.seed")
+
+
+
+@register_task('rxn', dataclass=RXNConfig)
+class RXNTask(FairseqTask):
+
+    cfg: RXNConfig
+
+    def __init__(self, cfg: RXNConfig, vocab):
+        super().__init__(cfg)
+        self.vocab = vocab
+
+    @classmethod
+    def setup_task(cls, cfg, **kwargs):
+        # load data dictionary
+        logger.info(f"[input]: Building vocabulary for RXN at {cfg.data}")
+        vocab = build_vocab(cfg.data)
+        return RXNTask(cfg, vocab)
+
+    def load_dataset(self, split, combine=False, **kwargs):
+        """Load a given dataset split (e.g., train, valid, test)."""
+        def make_dataset(split, vocab):
+            dataset = ReactionDataset(
+                os.path.join(self.cfg.data, f"src-{split}.txt"),
+                os.path.join(self.cfg.data, f"tgt-{split}.txt"),
+                vocab
+            )
+            return dataset
+        self.datasets[split] = make_dataset(split, self.vocab)
+        return self.datasets[split]
+
+    def build_model(self, args):
+        from fairseq import models
+        setattr(args, "vocab_path", self.cfg.data)
+        model = models.build_model(args, self)
+        return model
diff --git a/fairseq/tasks/sentence_prediction.py b/fairseq/tasks/sentence_prediction.py
index de80adda..03761251 100644
--- a/fairseq/tasks/sentence_prediction.py
+++ b/fairseq/tasks/sentence_prediction.py
@@ -280,10 +280,16 @@ class SentencePredictionTask(FairseqTask):
 
         model = models.build_model(cfg, self, from_checkpoint)
 
-        model.register_classification_head(
-            self.cfg.classification_head_name,
-            num_classes=self.cfg.num_classes,
-        )
+        if getattr(cfg, "analog", False):
+                model.child_module.register_classification_head(
+                self.cfg.classification_head_name,
+                num_classes=self.cfg.num_classes,
+            )
+        else:
+            model.register_classification_head(
+                self.cfg.classification_head_name,
+                num_classes=self.cfg.num_classes,
+            )
 
         return model
 
diff --git a/fairseq/trainer.py b/fairseq/trainer.py
index 16b1b916..14923a13 100644
--- a/fairseq/trainer.py
+++ b/fairseq/trainer.py
@@ -15,6 +15,7 @@ import time
 from argparse import Namespace
 from itertools import chain
 from typing import Any, Dict, List
+from copy import deepcopy
 
 import torch
 from omegaconf import OmegaConf
@@ -28,7 +29,10 @@ from fairseq.logging import meters, metrics
 from fairseq.models.ema import build_ema
 from fairseq.nan_detector import NanDetector
 from fairseq.optim import lr_scheduler
-from fairseq.utils import safe_hasattr
+from fairseq.utils import safe_getattr, safe_hasattr, analog2model, dp2model, assign2dp
+
+from aihwkit.nn import AnalogWrapper
+from aihwkit.nn.conversion import convert_to_analog, convert_to_digital
 
 logger = logging.getLogger(__name__)
 
@@ -172,6 +176,7 @@ class Trainer(object):
         self._previous_training_time = 0
         self._cumulative_training_time = None
 
+
     def reinitialize(self):
         """Reinitialize the Trainer, typically after model params change."""
         self._lr_scheduler = None
@@ -392,8 +397,8 @@ class Trainer(object):
         self._gathered_optim_state = None
         if hasattr(self.optimizer.optimizer, "consolidate_state_dict"):
             self.optimizer.optimizer.consolidate_state_dict()
-        elif self.is_fsdp and not self.model.use_sharded_state:
-            st = self.model.gather_full_optim_state_dict(
+        elif self.is_fsdp and not analog2model(self.model).use_sharded_state:
+            st = analog2model(self.model).gather_full_optim_state_dict(
                 self.optimizer
             )  # only returns on rank 0
             self._gathered_optim_state = st
@@ -441,7 +446,7 @@ class Trainer(object):
                 state_dict["last_optimizer_state"] = self.optimizer.state_dict()
         if self.is_fsdp:
             # save meta data for recombining checkpoint upon loading
-            state_dict["fsdp_metadata"] = self.model.local_metadata_dict()
+            state_dict["fsdp_metadata"] = analog2model(self.model).local_metadata_dict()
         return state_dict
 
     def save_checkpoint(self, filename, extra_state):
@@ -475,6 +480,11 @@ class Trainer(object):
         rank = 0 will load the checkpoint, and then broadcast it to all
         other ranks.
         """
+        if isinstance(dp2model(self.model), AnalogWrapper):
+            digital_model = convert_to_digital(dp2model(self.model))
+        else:
+            digital_model = self.model
+
         extra_state, self._optim_history, last_optim_state = None, [], None
 
         logger.info(f"Preparing to load checkpoint {filename}")
@@ -528,7 +538,7 @@ class Trainer(object):
                     and len(state["optimizer_history"]) > 0
                     and "num_updates" in state["optimizer_history"][-1]
                 ):
-                    self.model.set_num_updates(
+                    digital_model.set_num_updates(
                         state["optimizer_history"][-1]["num_updates"]
                     )
 
@@ -542,22 +552,22 @@ class Trainer(object):
                 # User will fine tune the the new roberta encoder via the ckpt saved above
                 # To get rid of registering different pruned version of Roberta, I use the argument --mha-heads-to-keep to prune the Roberta model into a pruned version which matches the pruned ckpt.
                 if (
-                    safe_hasattr(self.model, "args")
-                    and safe_hasattr(self.model.args, "mha_heads_to_keep")
-                    and self.model.args.mha_heads_to_keep != -1
+                    safe_hasattr(digital_model, "args")
+                    and safe_hasattr(digital_model.args, "mha_heads_to_keep")
+                    and digital_model.args.mha_heads_to_keep != -1
                 ):
                     logger.info(
-                        f"Prune model: keep {self.model.args.mha_heads_to_keep} heads for each multihead attention module"
+                        f"Prune model: keep {digital_model.args.mha_heads_to_keep} heads for each multihead attention module"
                     )
-                    for layer in self.model.encoder.sentence_encoder.layers:
+                    for layer in digital_model.encoder.sentence_encoder.layers:
                         reserve_head_index = layer.self_attn._get_reserve_head_index(
-                            num_heads_to_keep=self.model.args.mha_heads_to_keep
+                            num_heads_to_keep=digital_model.args.mha_heads_to_keep
                         )
                         layer.self_attn._adaptive_prune_heads(
                             reserve_head_index=reserve_head_index
                         )
                         layer.self_attn._set_skip_embed_dim_check()
-                    logger.info(self.model)
+                    logger.info(digital_model)
                 # this is the code related to AdaPrune
                 # In short, it removes redundant units in feedforward layer in each transformer layer based on importance
                 # For more info, please refer to the paper: https://openreview.net/forum?id=_CMSV7FTzGI
@@ -568,21 +578,21 @@ class Trainer(object):
                 # User will fine tune the the new roberta encoder via the ckpt saved above
                 # To get rid of registering different pruned version of Roberta, I use the argument --ffn-blocks-to-remove to prune the Roberta model into a pruned version which matches the pruned ckpt.
                 if (
-                    safe_hasattr(self.model, "args")
-                    and safe_hasattr(self.model.args, "ffn_blocks_to_remove")
-                    and self.model.args.ffn_blocks_to_remove != -1
+                    safe_hasattr(digital_model, "args")
+                    and safe_hasattr(digital_model.args, "ffn_blocks_to_remove")
+                    and digital_model.args.ffn_blocks_to_remove != -1
                 ):
                     logger.info(
-                        f"Prune model: remove {self.model.args.ffn_blocks_to_remove} ffn blocks for each transformer layer"
+                        f"Prune model: remove {digital_model.args.ffn_blocks_to_remove} ffn blocks for each transformer layer"
                     )
-                    for layer in self.model.encoder.sentence_encoder.layers:
+                    for layer in digital_model.encoder.sentence_encoder.layers:
                         remove_index = layer._get_fc_rank(
-                            remove_num=self.model.args.ffn_blocks_to_remove
+                            remove_num=digital_model.args.ffn_blocks_to_remove
                         )
                         layer._prune_fc_layer(remove_index=remove_index)
-                    logger.info(self.model)
+                    logger.info(digital_model)
 
-                self.model.load_state_dict(
+                digital_model.load_state_dict(
                     state["model"], strict=True, model_cfg=self.cfg.model
                 )
                 # save memory for later steps
@@ -593,6 +603,18 @@ class Trainer(object):
                     )
                     del state["criterion"]
 
+                if isinstance(dp2model(self.model), AnalogWrapper):
+                    # - Need to convert the digital model back to analog
+                    # and replace self.model with it
+                    # - We need to want to split the layers and input range learning must be enabled.
+                    rpu_config = deepcopy(next(dp2model(self.model).analog_tiles()).rpu_config)
+                    self._model = convert_to_analog(digital_model, rpu_config)
+                    # - Free memory
+                    del digital_model
+                    # - Put the new model on the GPU
+                    self._model = self._model.to(self.device)
+                    self._wrapped_model = assign2dp(self._wrapped_model, self._model)
+
             except Exception:
                 raise Exception(
                     "Cannot load model parameters from checkpoint {}; "
@@ -617,9 +639,9 @@ class Trainer(object):
             if not reset_lr_scheduler:
                 self.lr_scheduler.load_state_dict(last_optim["lr_scheduler_state"])
 
-            if self.is_fsdp and not self.model.use_sharded_state:
+            if self.is_fsdp and not analog2model(self.model).use_sharded_state:
                 # if use_sharded_state, the last_optim_state is already sharded, skip this
-                last_optim_state = self.model.get_shard_from_optim_state_dict(
+                last_optim_state = analog2model(self.model).get_shard_from_optim_state_dict(
                     last_optim_state
                 )
             elif not load_on_all_ranks and is_distributed:
@@ -715,7 +737,7 @@ class Trainer(object):
             max_sentences=self.cfg.dataset.batch_size,
             max_positions=utils.resolve_max_positions(
                 self.task.max_positions(),
-                self.model.max_positions(),
+                analog2model(self.model).max_positions(),
                 self.cfg.dataset.max_tokens,
             ),
             ignore_invalid_inputs=True,
@@ -748,7 +770,7 @@ class Trainer(object):
             max_sentences=self.cfg.dataset.batch_size_valid,
             max_positions=utils.resolve_max_positions(
                 self.task.max_positions(),
-                self.model.max_positions(),
+                analog2model(self.model).max_positions(),
             ),
             ignore_invalid_inputs=self.cfg.dataset.skip_invalid_size_inputs_valid_test,
             required_batch_size_multiple=self.cfg.dataset.required_batch_size_multiple,
@@ -974,6 +996,15 @@ class Trainer(object):
                 self.task.optimizer_step(
                     self.optimizer, model=self.model, update_num=self.get_num_updates()
                 )
+                # clamp the weights if clip type is gaussian
+                if not safe_getattr(self.cfg.model, "analog", False) and self.cfg.model.clip_type == "gaussian":
+                    for m in self.model.modules():
+                        if isinstance(m, torch.nn.Linear):
+                            alpha_std = self.cfg.model.clip_sigma * m.weight.std()
+                            m.weight.data = torch.clamp(m.weight, min=-alpha_std, max=alpha_std)
+                else:
+                    assert self.cfg.model.clip_type in ["none","gaussian"], f"Unknown clip type {self.cfg.model.clip_type}. Can be none or gaussian"
+                
                 if self.cfg.common.amp and overflow:
                     if self._amp_retries == self.cfg.common.amp_batch_retries:
                         logger.info("AMP: skipping this batch.")
@@ -1031,7 +1062,7 @@ class Trainer(object):
         # Some distributed wrappers (e.g., SlowMo) need access to the optimizer
         # after the step
         if hasattr(self.model, "perform_slowmo"):
-            self.model.perform_slowmo(
+            analog2model(self.model).perform_slowmo(
                 self.optimizer.optimizer, getattr(self.optimizer, "fp32_params", None)
             )
 
diff --git a/fairseq/utils.py b/fairseq/utils.py
index 4d4b3505..b3009cbc 100644
--- a/fairseq/utils.py
+++ b/fairseq/utils.py
@@ -9,16 +9,26 @@ import contextlib
 import copy
 import importlib
 import logging
+import math
 import os
 import sys
 import warnings
 from itertools import accumulate
-from typing import TYPE_CHECKING, Callable, Dict, List, Optional
+from typing import TYPE_CHECKING, Callable, Dict, List, Optional, Union
 
 import torch
 import torch.nn.functional as F
-from torch import Tensor
-
+from torch import Tensor, device
+
+from aihwkit.nn import AnalogWrapper
+from aihwkit.simulator.configs import TorchInferenceRPUConfig, InferenceRPUConfig
+from aihwkit.simulator.configs.utils import (
+    WeightModifierType, BoundManagementType, WeightClipType, NoiseManagementType, WeightRemapType, WeightNoiseType
+)
+from aihwkit.simulator.presets.utils import IOParameters
+from aihwkit.inference.noise.pcm import PCMLikeNoiseModel
+from aihwkit.inference.compensation.drift import GlobalDriftCompensation
+from fairseq.distributed.module_proxy_wrapper import ModuleProxyWrapper
 if TYPE_CHECKING:
     from fairseq.modules.multihead_attention import MultiheadAttention
 
@@ -40,6 +50,198 @@ logger = logging.getLogger(__name__)
 
 MANIFOLD_PATH_SEP = "|"
 
+class NoisyLinear(torch.nn.Linear):
+    def __init__(self, in_features: int, out_features: int, modifier_std_dev: float,
+                 modifier_type: str, bias: bool = True, device=None, dtype=None) -> None:
+        super().__init__(in_features, out_features, bias, device, dtype)
+        self.modifier_type = modifier_type
+        self.modifier_std_dev = modifier_std_dev
+
+    def forward(self, input: Tensor) -> Tensor:
+        if self.training:
+            if self.modifier_type == "none":
+                noisy_weights = self.weight
+            elif self.modifier_type == "add_gauss":
+                with torch.no_grad():
+                    noise = (
+                        self.modifier_std_dev * self.weight.abs().max() * torch.randn_like(self.weight, device=self.weight.device)
+                    )
+                    noisy_weights = self.weight.clone() + noise
+            else:
+                raise Exception("Unknown modifier type")
+        else:
+            noisy_weights = self.weight
+        return F.linear(input, noisy_weights, self.bias)
+
+def convert2analog(model, modifier_std_dev, modifier_type):
+    for m in model.modules():
+        if isinstance(m, (NoisyLinear, torch.nn.Linear)): continue
+        for n,child in m.named_children():
+            if isinstance(child, torch.nn.Linear):
+                noisy_linear = NoisyLinear(
+                    in_features=child.weight.shape[1], out_features=child.weight.shape[0],
+                    bias=not (child.bias == None), device=child.weight.device,
+                    modifier_std_dev=modifier_std_dev, modifier_type=modifier_type
+                )
+                noisy_linear.bias = child.bias
+                noisy_linear.weight = child.weight
+                m.__setattr__(n, noisy_linear)
+    return model
+
+def assign_default_rpu_params(args):
+    args.tile_type = safe_getattr(args, "tile_type", "torch")
+    args.noise_per_sample = safe_getattr(args, "noise_per_sample", False)
+    args.modifier_std_dev = safe_getattr(args, "modifier_std_dev", 0.0)
+    args.modifier_type = safe_getattr(args, "modifier_type", "none")
+    args.forward_out_noise = safe_getattr(args, "forward_out_noise", 0.0)
+    args.mapping_weight_scaling_omega = safe_getattr(args, "mapping_weight_scaling_omega", 1.0)
+    args.mapping_learn_out_scaling = safe_getattr(args, "mapping_learn_out_scaling", True)
+    args.mapping_weight_scaling_columnwise = safe_getattr(args, "mapping_weight_scaling_columnwise", False)
+    args.mapping_out_scaling_columnwise = safe_getattr(args, "mapping_out_scaling_columnwise", False)
+    args.remap_type = safe_getattr(args, "remap_type", "layer")
+    args.forward_is_perfect = safe_getattr(args, "forward_is_perfect", False)
+    args.clip_type = safe_getattr(args, "clip_type", "none")
+    args.clip_sigma = safe_getattr(args, "clip_sigma", 2.5)
+    args.clip_fixed_value = safe_getattr(args, "clip_fixed_value", 1.0)
+    args.forward_inp_res = safe_getattr(args, "forward_inp_res", 2**8-2)
+    args.forward_out_res = safe_getattr(args, "forward_out_res", 2**8-2)
+    args.forward_out_bound = safe_getattr(args, "forward_out_bound", 10.0)
+    args.forward_inp_bound = safe_getattr(args, "forward_inp_bound", 1.0)
+    args.forward_bound_management = safe_getattr(args, "forward_bound_management", "none")
+    args.forward_noise_management = safe_getattr(args, "forward_noise_management", "abs_max")
+    args.forward_w_noise_type = safe_getattr(args, "forward_w_noise_type", "none")
+    args.forward_w_noise = safe_getattr(args, "forward_w_noise", 0.0)
+    args.input_range_enable = safe_getattr(args, "input_range_enable", False)
+    args.input_range_manage_output_clipping = safe_getattr(args, "input_range_manage_output_clipping", False)
+    args.input_range_decay = safe_getattr(args, "input_range_decay", 0.01)
+    args.input_range_input_min_percentage = safe_getattr(args, "input_range_input_min_percentage", 0.95)
+    args.input_range_output_min_percentage = safe_getattr(args, "input_range_output_min_percentage", 0.95)
+    args.input_range_init_value = safe_getattr(args, "input_range_init_value", 3.0)
+    args.input_range_init_std_alpha = safe_getattr(args, "input_range_init_std_alpha", 3.0)
+    args.input_range_init_from_data = safe_getattr(args, "input_range_init_from_data", 0)
+    args.static_input_clipping = safe_getattr(args, "static_input_clipping", False)
+    args.static_input_clipping_quantile = safe_getattr(args, "static_input_clipping_quantile", 0.995)
+    args.split_layers = safe_getattr(args, "split_layers", True)
+    args.max_input_size = safe_getattr(args, "max_input_size", 256)
+    args.max_output_size = safe_getattr(args, "max_output_size", int(1e6))
+    args.eta_eval = safe_getattr(args, "eta_eval", 0.0)
+
+def create_rpu_config(args):
+    if args.tile_type == "cpp":
+        rpu_config = InferenceRPUConfig()
+    elif args.tile_type == "torch":
+        rpu_config = TorchInferenceRPUConfig()
+    else:
+        raise Exception("Unknown tile type")
+
+    if args.clip_type == "gaussian":
+        clip_type = WeightClipType.LAYER_GAUSSIAN
+    elif args.clip_type == "fixed":
+        clip_type = WeightClipType.FIXED_VALUE
+    elif args.clip_type == "none":
+        clip_type = WeightClipType.NONE
+    else:
+        raise Exception("Clip type not supported")
+        
+    rpu_config.clip.type = clip_type
+    rpu_config.clip.sigma = args.clip_sigma
+    rpu_config.clip.fixed_value = args.clip_fixed_value
+
+    if args.modifier_type == "mult_gauss":
+        modifier_type = WeightModifierType.MULT_NORMAL
+    elif args.modifier_type == "add_gauss":
+        modifier_type = WeightModifierType.ADD_NORMAL
+    elif args.modifier_type == "pcm_noise":
+        modifier_type = WeightModifierType.PCM_NOISE
+    elif args.modifier_type == "none":
+        modifier_type = WeightModifierType.COPY
+    else:
+        raise Exception("Unknown modifier type")
+
+    if args.forward_bound_management == "none":
+        bound_management = BoundManagementType.NONE
+    elif args.forward_bound_management == "iterative":
+        bound_management = BoundManagementType.ITERATIVE
+    else:
+        raise Exception("Unknown BoundManagementType")
+
+    if args.forward_noise_management == "none":
+        noise_management = NoiseManagementType.NONE
+    elif args.forward_noise_management == "abs_max":
+        noise_management = NoiseManagementType.ABS_MAX
+    else:
+        raise Exception("Unknown BoundManagementType")
+
+    if args.forward_w_noise_type == "none":
+        w_noise_type = WeightNoiseType.NONE
+    elif args.forward_w_noise_type == "constant":
+        w_noise_type = WeightNoiseType.ADDITIVE_CONSTANT
+    elif args.forward_w_noise_type == "pcm_read":
+        w_noise_type = WeightNoiseType.PCM_READ
+    else:
+        raise Exception("Unknown WeightNoiseType")
+
+    rpu_config.modifier.type = modifier_type
+    rpu_config.modifier.std_dev = args.modifier_std_dev
+    rpu_config.modifier.rel_to_actual_wmax = True
+    rpu_config.modifier.per_batch_sample = args.noise_per_sample
+
+    if args.remap_type == "none":
+        remap_type = WeightRemapType.NONE
+    elif args.remap_type == "channel":
+        remap_type = WeightRemapType.CHANNELWISE_SYMMETRIC
+    elif args.remap_type == "layer":
+        remap_type = WeightRemapType.LAYERWISE_SYMMETRIC
+    else:
+        raise Exception("Unknown remap type")
+    rpu_config.remap.type = remap_type
+
+    rpu_config.mapping.weight_scaling_omega = args.mapping_weight_scaling_omega
+    rpu_config.mapping.weight_scaling_columnwise = args.mapping_weight_scaling_columnwise
+    rpu_config.mapping.learn_out_scaling = args.mapping_learn_out_scaling
+    rpu_config.mapping.out_scaling_columnwise = args.mapping_out_scaling_columnwise
+    rpu_config.mapping.max_input_size = args.max_input_size
+    rpu_config.mapping.max_output_size = args.max_output_size
+
+    rpu_config.pre_post.input_range.enable = args.input_range_enable
+    rpu_config.pre_post.input_range.manage_output_clipping = args.input_range_manage_output_clipping
+    rpu_config.pre_post.input_range.decay = args.input_range_decay
+    rpu_config.pre_post.input_range.input_min_percentage = args.input_range_input_min_percentage
+    rpu_config.pre_post.input_range.output_min_percentage = args.input_range_output_min_percentage
+    rpu_config.pre_post.input_range.init_value = args.input_range_init_value
+    rpu_config.pre_post.input_range.init_std_alpha = args.input_range_init_std_alpha
+    rpu_config.pre_post.input_range.init_from_data = args.input_range_init_from_data
+
+    rpu_config.forward = IOParameters()
+    rpu_config.forward.w_noise_type = w_noise_type
+    rpu_config.forward.w_noise = args.forward_w_noise
+    rpu_config.forward.inp_res = args.forward_inp_res
+    rpu_config.forward.out_res = args.forward_out_res
+    rpu_config.forward.bound_management = bound_management
+    rpu_config.forward.noise_management = noise_management
+    rpu_config.forward.out_bound = args.forward_out_bound
+    rpu_config.forward.inp_bound = args.forward_inp_bound
+    rpu_config.forward.out_noise = args.forward_out_noise
+    rpu_config.forward.is_perfect = args.forward_is_perfect
+
+    # - Just inference
+    rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.)
+    rpu_config.drift_compensation = GlobalDriftCompensation()
+
+    return rpu_config
+
+def analog2model(model):
+    return dp2model(model).module if isinstance(dp2model(model), AnalogWrapper) else model
+
+def dp2model(model):
+    return model.module.module if isinstance(model, ModuleProxyWrapper) else model
+
+def assign2dp(wrapped_model, model):
+    if isinstance(wrapped_model, ModuleProxyWrapper):
+        wrapped_model.module.module = model
+    else:
+        wrapped_model = model
+    return wrapped_model
 
 class FileContentsAction(argparse.Action):
     def __init__(self, option_strings, dest, nargs=None, **kwargs):
@@ -544,6 +746,12 @@ def deprecation_warning(message, stacklevel=3):
 def relu_squared(x: torch.Tensor):
     return F.relu(x).pow(2)
 
+def laplace(x, mu=0.707107, sigma=0.282095, onnx_trace: bool = False):
+    if onnx_trace:
+        x = x.float()
+    x = (x - mu).div(sigma * math.sqrt(2.0))
+    return 0.5 * (1.0 + torch.erf(x))
+
 
 def get_activation_fn(activation: str) -> Callable:
     """Returns the activation function corresponding to `activation`"""
@@ -562,6 +770,8 @@ def get_activation_fn(activation: str) -> Callable:
         return gelu_accurate
     elif activation == "gelu_accurate":
         return gelu_accurate
+    elif activation == 'silu':
+        return F.silu
     elif activation == "tanh":
         return torch.tanh
     elif activation == "linear":
@@ -580,6 +790,7 @@ def get_available_activation_fns() -> List:
         "gelu_accurate",
         "tanh",
         "linear",
+        "silu"
     ]
 
 
diff --git a/fairseq_cli/train.py b/fairseq_cli/train.py
index f771bff6..3b64f789 100644
--- a/fairseq_cli/train.py
+++ b/fairseq_cli/train.py
@@ -471,60 +471,91 @@ def validate(
     for subset_idx, subset in enumerate(subsets):
         logger.info('begin validation on "{}" subset'.format(subset))
 
-        # Initialize data iterator
-        itr = trainer.get_valid_iterator(subset).next_epoch_itr(
-            shuffle=False, set_dataset_epoch=False  # use a fixed valid set
-        )
-        if cfg.common.tpu:
-            itr = utils.tpu_data_loader(itr)
-        progress = progress_bar.progress_bar(
-            itr,
-            log_format=cfg.common.log_format,
-            log_interval=cfg.common.log_interval,
-            epoch=epoch_itr.epoch,
-            prefix=f"valid on '{subset}' subset",
-            aim_repo=(
-                cfg.common.aim_repo
-                if distributed_utils.is_master(cfg.distributed_training)
-                else None
-            ),
-            aim_run_hash=(
-                cfg.common.aim_run_hash
-                if distributed_utils.is_master(cfg.distributed_training)
-                else None
-            ),
-            aim_param_checkpoint_dir=cfg.checkpoint.save_dir,
-            tensorboard_logdir=(
-                cfg.common.tensorboard_logdir
-                if distributed_utils.is_master(cfg.distributed_training)
-                else None
-            ),
-            default_log_format=("tqdm" if not cfg.common.no_progress_bar else "simple"),
-            wandb_project=(
-                cfg.common.wandb_project
-                if distributed_utils.is_master(cfg.distributed_training)
-                else None
-            ),
-            wandb_run_name=os.environ.get(
-                "WANDB_NAME", os.path.basename(cfg.checkpoint.save_dir)
-            ),
-        )
+        def add_noise(model, eta_eval):
+            for m in model.modules():
+                if isinstance(m, torch.nn.Linear):
+                    noise = eta_eval * m.weight.abs().max() * torch.randn_like(m.weight, device=m.weight.device)
+                    m.weight.data = m.weight + noise
+            return model
+
+        n_repeat = 1 if getattr(cfg.model, "eta_eval", 0.0) == 0.0 else 10
+        stats_l = []
+        if n_repeat > 1:
+            original_state_dict = trainer.model.state_dict()
+        for _ in range(n_repeat):
+
+            # Initialize data iterator
+            itr = trainer.get_valid_iterator(subset).next_epoch_itr(
+                shuffle=False, set_dataset_epoch=False  # use a fixed valid set
+            )
+            if cfg.common.tpu:
+                itr = utils.tpu_data_loader(itr)
+            progress = progress_bar.progress_bar(
+                itr,
+                log_format=cfg.common.log_format,
+                log_interval=cfg.common.log_interval,
+                epoch=epoch_itr.epoch,
+                prefix=f"valid on '{subset}' subset",
+                aim_repo=(
+                    cfg.common.aim_repo
+                    if distributed_utils.is_master(cfg.distributed_training)
+                    else None
+                ),
+                aim_run_hash=(
+                    cfg.common.aim_run_hash
+                    if distributed_utils.is_master(cfg.distributed_training)
+                    else None
+                ),
+                aim_param_checkpoint_dir=cfg.checkpoint.save_dir,
+                tensorboard_logdir=(
+                    cfg.common.tensorboard_logdir
+                    if distributed_utils.is_master(cfg.distributed_training)
+                    else None
+                ),
+                default_log_format=("tqdm" if not cfg.common.no_progress_bar else "simple"),
+                wandb_project=(
+                    cfg.common.wandb_project
+                    if distributed_utils.is_master(cfg.distributed_training)
+                    else None
+                ),
+                wandb_run_name=os.environ.get(
+                    "WANDB_NAME", os.path.basename(cfg.checkpoint.save_dir)
+                ),
+            )
+
+            if n_repeat > 1:
+                # - Add noise in-place
+                add_noise(trainer.model, eta_eval=getattr(cfg.model, "eta_eval", 0.0))
+
+            # create a new root metrics aggregator so validation metrics
+            # don't pollute other aggregators (e.g., train meters)
+            with metrics.aggregate(new_root=True) as agg:
+                for i, sample in enumerate(progress):
+                    if (
+                        cfg.dataset.max_valid_steps is not None
+                        and i > cfg.dataset.max_valid_steps
+                    ):
+                        break
+                    trainer.valid_step(sample)
+
+            # log validation stats
+            # only tracking the best metric on the 1st validation subset
+            
+            stats = get_valid_stats(cfg, trainer, agg.get_smoothed_values(), False)
+            stats_l.append(stats)
+
+            if n_repeat > 1:
+                trainer.model.load_state_dict(original_state_dict)
+
+        stats = stats_l[0]
+        for s in stats_l[1:]:
+            for k,v in s.items():
+                stats[k] += v
+        for k in stats:
+            stats[k] /= n_repeat
 
-        # create a new root metrics aggregator so validation metrics
-        # don't pollute other aggregators (e.g., train meters)
-        with metrics.aggregate(new_root=True) as agg:
-            for i, sample in enumerate(progress):
-                if (
-                    cfg.dataset.max_valid_steps is not None
-                    and i > cfg.dataset.max_valid_steps
-                ):
-                    break
-                trainer.valid_step(sample)
-
-        # log validation stats
-        # only tracking the best metric on the 1st validation subset
         tracking_best = subset_idx == 0
-        stats = get_valid_stats(cfg, trainer, agg.get_smoothed_values(), tracking_best)
+        stats = get_valid_stats(cfg, trainer, stats, tracking_best)
 
         if hasattr(task, "post_validate"):
             task.post_validate(trainer.get_model(), stats, agg)
